<!DOCTYPE html><html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta name="theme-color" content="#123456"><meta name="generator" content="Hexo 4.2.0"><title>标签: Python - Homepage of Jinghua Xu</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="#3273dc"><meta name="application-name" content="Homepage of Jinghua Xu"><meta name="msapplication-TileImage" content="/img/photo.jpg"><meta name="msapplication-TileColor" content="#3273dc"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Homepage of Jinghua Xu"><meta name="apple-mobile-web-app-status-bar-style" content="default"><link rel="apple-touch-icon" sizes="144x144" href="/img/photo.jpg"><meta name="description" content="重庆大学2022级明月科创实验班人工智能专业本科在读"><meta property="og:type" content="blog"><meta property="og:title" content="Homepage of Jinghua Xu"><meta property="og:url" content="http://asgard-tim.github.io/"><meta property="og:site_name" content="Homepage of Jinghua Xu"><meta property="og:description" content="重庆大学2022级明月科创实验班人工智能专业本科在读"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://asgard-tim.github.io/img/og_image.png"><meta property="article:author" content="Tim"><meta property="article:tag" content="Blog"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://asgard-tim.github.io/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://Asgard-Tim.github.io"},"headline":"Homepage of Jinghua Xu","image":["http://asgard-tim.github.io/img/og_image.png"],"author":{"@type":"Person","name":"Tim"},"publisher":{"@type":"Organization","name":"Homepage of Jinghua Xu","logo":{"@type":"ImageObject","url":"http://asgard-tim.github.io/img/title1.png"}},"description":"重庆大学2022级明月科创实验班人工智能专业本科在读"}</script><link rel="icon" href="/img/photo.jpg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/xt256.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/title1.png" alt="Homepage of Jinghua Xu" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">时间轴</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com">GitHub</a><a class="navbar-item" target="_blank" rel="noopener" title="Contect me on GitHub" href="https://github.com/Asgard-Tim"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">标签</a></li><li class="is-active"><a href="#" aria-current="page">Python</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2025-06-09T18:59:03.000Z" title="2025/6/10 02:59:03">2025-06-10</time>发表</span><span class="level-item"><time datetime="2025-07-01T15:22:18.826Z" title="2025/7/1 23:22:18">2025-07-01</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/">课程项目</a><span>&nbsp;/&nbsp;</span><a class="link-muted" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E5%B7%A5%E7%A8%8B%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90/">工程数值分析</a></span><span class="level-item">1 小时读完 (大约9294个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2025/06/10/project3/">常微分方程反演的机器学习方法</a></p><div class="content"><div id="postchat_postcontent"><h2 id="选题背景与意义"><a href="#选题背景与意义" class="headerlink" title="选题背景与意义"></a>选题背景与意义</h2><p><strong>微分方程</strong>是数学中一个重要且广泛应用的领域，涉及到描述变化和相互关系的方程。它是一种包含导数或微分的方程，常用于自然现象建模以及解决科学和工程领域中的问题。微分方程研究是数学中的一个重要分支，涵盖了广泛的领域和应用。通过研究微分方程，我们可以<strong>理解和预测自然现象的行为</strong>，以及<strong>解决科学和工程中的实际问题</strong>。同时，微分方程的研究也促进了数学的发展和数学工具在其他学科中的应用。 </p>
<p>常微分方程被广泛应用于物理、生物、化学、经济学等各个领域。在许多情况下，观测数据是已知的，而其中隐含的微分方程仍然难以捉摸。因此，<strong>数据驱动的常微分方程(或动力系统)发现和反演</strong>是一个重要的研究方向。</p>
<p>微分方程反演问题是相对于微分方程的求解而言的：</p>
<ul>
<li>微分方程的求解通常是一个正向问题，即给定初始条件，求解关于未知函数的方程；</li>
<li>微分方程反演问题是指根据已知的结果，推导出产生这些结果的微分方程，是从结果向方程的逆向推导和反演。</li>
</ul>
<p>在数学上，微分方程的反演问题具有一系列的数学<strong>挑战和困难</strong>：</p>
<ul>
<li>反问题的<strong>病态性</strong>：微小扰动在反问题的输出中会导致较大的误差，因此需要稳定性分析和正则化方法来解决数值计算中的误差；</li>
<li>存在<strong>非唯一解</strong>情况：需要利用先验信息、约束条件和经验知识等来进行问题的约束和规定，以得到合理的解。</li>
</ul>
<p>微分方程反演问题研究在数学理论和实际应用中具有重要意义。通过解决微分方程的反演问题，我们可以从有限的观测数据中重建和推断未知的边界条件、初始条件或未知函数，从而深入理解系统的行为和性质，并提供科学、工程和医学等领域的相关应用。  </p>
<p>随着计算机技术的发展，数值计算和机器学习在微分方程反演问题研究中的应用也呈现出迅猛发展的趋势。传统的微分方程推断方法依赖于领域专家的知识和经验，而<strong>机器学习方法</strong>，如神经网络、深度学习和遗传算法等，<strong>可以自动从大量数据中学习模式和关系，从而更准确地推断出微分方程模型</strong>。如今我们可以获取到<strong>大规模的、高维度的数据集</strong>，这为从数据中推断微分方程提供了更多的信息和挑战。而传统的模型推断方法在处理大数据集时可能受到维度灾难和计算复杂度的限制。机器学习的强大计算能力和自适应建模能力为解决这些问题提供了新的思路。  </p>
<p>利用机器学习从数据中反演微分方程的方法可以分为两个方面：</p>
<ul>
<li>在<strong>模型选择</strong>方面，机器学习可以通过自动搜索和学习合适的微分方程模型，利用测度函数或结构特征来评估不同模型的性能，并选择最适合数据的微分方程模型；</li>
<li>在<strong>参数估计</strong>方面，机器学习可以利用大数据集中的样本来优化微分方程模型的参数，以最小化模型与实际数据之间的误差。</li>
</ul>
<p>利用机器学习从数据中反演微分方程是微分方程和机器学习交叉领域的重要研究方向。它利用机器学习的能力和大数据的优势，可以更准确地建模和预测复杂的动力学系统，推进科学研究和实际应用的发展。</p>
<p>针对常微分方程，目前<strong>常用的反演方法</strong>有如下几种：</p>
<ul>
<li><strong>高斯过程</strong>：将高斯过程置于状态函数之上，然后通过最大化边际对数似然性从数据中推断参数，这种方法适用于解决高维问题，但是对系统的形式有限制，并且仅用于估计系统的参数。</li>
<li><strong>符号回归</strong>：创建并更正与观测数据相对应的符号模型，为控制函数提供更具表现力的函数形式，但缺点是对于大型系统来说计算成本高，并可能容易过拟合。</li>
<li><strong>稀疏回归</strong>：找到候选基函数的稀疏组合来近似控制函数，其系数由最小二乘法或贝叶斯回归确定。这种方法提供系统的明确形式，不需要太多先验知识，但是依靠适当的候选函数；对于没有简单或稀疏表示的复杂动力系统来说可能是低效的。</li>
<li><strong>统计学习</strong>：通过最小化经验误差来学习系统在某个假设空间中的相互作用核，避免维度诅咒，可以发现非常高维度的系统，但是仅适用于具有交互作用的核函数的动力系统。</li>
<li><strong>物理信息神经网络（PINN）</strong>：一种新的深度学习方法，用于解决非线性偏微分方程的正问题和反问题。通过在前馈神经网络中嵌入由偏微分方程描述的物理信息，在不需要标签数据的情况下，将 PINN 训练为偏微分方程的近似解的代理模型。受此启发，进一步发展出了多步神经网络LMNets，这本质上是使用给定相点上的流映射提供的信息来反演动力系统种的未知控制函数f的一种逆过程。</li>
</ul>
<p>在数值分析中，发展<strong>高阶方法</strong>是许多应用中的一个重要课题。传统上，在求解动力系统时，高阶离散化技术，如线性多步法和龙格-库塔方法已经得到了发展。近年来，线性多步法也被用于动力系统的发现。随着使用深度学习的发现取得令人满意的进展，对它在动力系统发现的理论理解也在进一步发展。</p>
<p>基于以上所陈述的微分方程反演问题的研究现状，针对<strong>无显式方程的非线性系统</strong>这一在真实物理世界中更为广泛且常见的情形，我们希望基于高阶离散化技术<strong>线性多步法</strong>，结合<strong>多步神经网络LMNets</strong>这一深度学习方法，<strong>拟合和反演出数据背后的动力学物理规律</strong>。</p>
<h2 id="算法框架与原理"><a href="#算法框架与原理" class="headerlink" title="算法框架与原理"></a>算法框架与原理</h2><h3 id="研究对象——常微分方程（组）"><a href="#研究对象——常微分方程（组）" class="headerlink" title="研究对象——常微分方程（组）"></a>研究对象——常微分方程（组）</h3><p>首先明确我们的研究对象——常微分方程：<br>$$<br>\begin{aligned}&amp;\frac{dx(t)}{dt}=f(x(t),t),\&amp;x(t_{0})=x_{0}\end{aligned}<br>$$<br>上述式子是一个最简单的二维常微分方程示例，其中x(t) ∈ R是关于时间t的函数，函数关系由一个微分方程刻画，且给出了一定的初值条件以唯一确定函数x的表达式，从而能够真实地刻画物理过程随时间变化的发展情况。</p>
<p>事实上大多数时候我们需要观测的物理量远远不止x这一个，因此为了后续更容易推广到高维情形（后续无特别说明，均围绕自治常微分方程展开讨论），我们将上述的<strong>非自治常微分方程</strong>转化为向量化的<strong>自治常微分方程</strong>：<br>$$<br>\begin{aligned}&amp;\frac{d\boldsymbol{u}(t)}{dt}=\hat{f}(\boldsymbol{u}(t)),\&amp;\boldsymbol{u}(t_{0})=\boldsymbol{u}<em>{0}\end{aligned}<br>$$<br>其中：<br>$$<br>u=(x,y)\in\mathbb{R}^{D+1},\hat{f}=(f,1)^{T},u</em>{0}=(x_{0},t_{0})^{T}<br>$$<br>在反演问题中，已知在若干时刻节点t_1、t_2、…、t_n的解x的数据，我们一般不会希望直接解出x(t)的表达式，而是通过确定光滑函数f，从而利用给定的数据反演出微分方程。这样的传统是从简单的反演情形推广并沿用的，因为比起得到一个精确的函数表达式，我们更希望探究这个表达式背后的物理规律，而这往往离不开微分方程，因此保留微分方程的形式具有一定的必要性。</p>
<h3 id="多步神经网络"><a href="#多步神经网络" class="headerlink" title="多步神经网络"></a>多步神经网络</h3><p>线性多步法是用于微分方程数值求解的常用方法。传统上，它们用于求解给定常微分方程的解，可以称为微分方程的正问题。但线性多步法也可以用于反演给定解的原微分方程，属于微分方程的反问题，尤其是将线性多步法的经典数值方法与神经网络相结合，例如多步神经网络。</p>
<h4 id="线性多步法"><a href="#线性多步法" class="headerlink" title="线性多步法"></a>线性多步法</h4><p>线性多步法是求解常微分方程数值解的一种常见方法。随着计算机技术的发展，线性多步法得到更加广泛的应用。人们逐渐发现，在某些情况下，线性多步法可以提供更高的数值精度和数值稳定性。此外，线性多步法可以与其他数值方法（如龙格-库塔法）结合使用，互补彼此的优点。</p>
<p>对于上述常微分方程（非自治），设其中结点总数为N，<strong>时间间隔h</strong>为常数:<br>$$<br>h = t_{n + 1} - t_n, n = 1, …, N - 1<br>$$<br>则第n个结点的<strong>步长为M</strong>的线性多步法有以下公式：<br>$$<br>\sum_{m=0}^{M}[\alpha_{m}x_{n-m}+h\beta_{m}f(x_{n-m},t)]=0,n=M、\ldots、N, \alpha_{_M}\neq0<br>$$<br>使用线性多步法求解常微分方程时，必须<strong>选择初始的步长M值</strong>，以及<strong>确定系数α、β的不同方法</strong>，然后可以依次计算n≥M的近似值x_n。</p>
<p>事实上，根据确定系数的方法不同，线性多步法也分为多种类型，本项目中主要使用如下的三种常见的线性多步法：</p>
<ul>
<li><strong>Adams-Moulton 法</strong>：作为一种<strong>隐式</strong>方法，需要通过迭代或其他数值求解技术来解决每个时间步长的方程组。它在求解非刚性和刚性常微分方程时都表现良好，并且具有较高的数值稳定性，其精度随步长的减小而提高，但响应地可能会产生更高的计算代价。对于高阶常微分方程，Adams-Moulton 法需要结合相应的差分格式将高阶常微分方程转化为一阶方程组进行求解。</li>
</ul>
<p>$$<br>\sum_{m=0}^M\alpha_mx_{n-m}=h\sum_{m=0}^M\beta_mf(x_{n-m})<br>$$</p>
<ul>
<li><strong>Adams-Bashforth法</strong>：作为一种<strong>显式</strong>方法，在求解非刚性和刚性常微分方程时都表现良好，计算效率较高，并且容易实现，但可能会收到稳定性条件地限制，其精度随步长的减小而提高。对于高阶常微分方程，Adams-Bashforth法需要结合相应的差分格式将高阶常微分方程转化为一阶方程组进行求解。</li>
</ul>
<p>$$<br>\sum_{m=0}^M\alpha_mx_{n-m}=h\sum_{m=0}^M\beta_mf(x(t_{n-m}))<br>$$</p>
<ul>
<li><strong>后向微分公式法</strong>(Backward Differentiation Formula, <strong>BDF</strong>)：同样是一种<strong>隐式</strong>方法，但与Adams-Moulton法不同，后向微分公式法是通过解一个非线性方程组来得到当前时间步长的数值解。这个方程组可以用牛顿迭代等数值求解技术来解决。该方法具有较好的数值稳定性，其精度依赖于使用的插值多项式的阶数。一般来说，其高阶形式可以提供更高的精度，但也会增加计算的复杂性。它适用于求解非刚性和刚性常微分方程，并且在稳定性和数值精度方面通常有良好的表现。</li>
</ul>
<p>$$<br>\sum_{m=0}^{M}\alpha_{m}x_{n-m}=h\beta f(x(t_{n}))<br>$$</p>
<p>除此之外，即使采用同一种系数确定方式，选择的步长M不同，得到的系数α、β也有所不同。具体而言，本项目中针对以上三种线性多步法，分别选取步长M=2/3/4的三种情况进行实验测试，下面列出各方法对应不同步长时的系数情况：</p>
<p><img src="/images/project3/1.png" alt="线性多步法系数"></p>
<p>线性多步法的<strong>优点</strong>在于<u>其高阶的精度和较小的计算代价，可以有效地逼近微分方程的数值解</u>。但线性多步法可能会受到<u>稳定性和初始条件</u>的<strong>限制</strong>，选择适当的步长和求解方法非常重要。</p>
<h4 id="多步神经网络算法"><a href="#多步神经网络算法" class="headerlink" title="多步神经网络算法"></a>多步神经网络算法</h4><p>从线性多步法的公式中得到启发，可以用神经网络去反演常微分方程右端的f。通过相等时间间隔h的数值解x的数据训练神经网络，该神经网络的参数可以通过使以下均方误差损失函数MSE最小化来训练得到：<br>$$<br>MSE=\frac{1}{N-M+1}\sum_{n=M}^N\parallel y_n\parallel^2<br>$$<br>其中：<br>$$<br>y_n=\sum_{m=0}^M\left[\alpha_mu_{n-m}+h\beta_mf_{NN}(u_{n-m})\right],n=M,\ldots,N<br>$$<br>式中f_NN表示神经网络对函数f的近似，y_n也称为线性多步法（LMM）残差。</p>
<p>在Python中编写函数<code>train()</code>以封装多步神经网络的训练流程：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型训练 </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">u_tensor, model, loss_func, h, M</span>): </span><br><span class="line">    <span class="comment"># 参数说明：训练集u_tensor,待训练模型model,损失函数loss_func,时间步长h,步数M,最大训练次数EPOCH</span></span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=LR)</span><br><span class="line">    loss_history = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCHS):          </span><br><span class="line">        model.train()        </span><br><span class="line">        batch_u = u_tensor.to(device)        </span><br><span class="line">        train_loss = loss_func(batch_u, model, h, M)   </span><br><span class="line">        optimizer.zero_grad()      </span><br><span class="line">        train_loss.backward()    </span><br><span class="line">        optimizer.step()    </span><br><span class="line">        epoch_avg_loss = train_loss.item()         </span><br><span class="line">        loss_history.append(epoch_avg_loss) </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss_history</span><br></pre></td></tr></tbody></table></figure>

<p>其中<code>loss_func</code>即为上述定义的均方误差损失函数MSE：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss_func</span>(<span class="params">u, model, h, M</span>):</span><br><span class="line">	<span class="comment"># 参数说明：模型输入数据u,待训练模型model,时间步长h,步数M</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据选定的方法确定系数α、β（示例：Adams-Moulton法，M=4）</span></span><br><span class="line">    alpha = [<span class="number">1</span>, -<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">    beta = [<span class="number">251</span>/<span class="number">720</span>, <span class="number">646</span>/<span class="number">720</span>, -<span class="number">264</span>/<span class="number">720</span>, <span class="number">106</span>/<span class="number">720</span>, -<span class="number">19</span>/<span class="number">720</span>]</span><br><span class="line">        </span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(M, <span class="built_in">len</span>(u)):</span><br><span class="line">        residual = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> <span class="built_in">range</span>(M+<span class="number">1</span>):</span><br><span class="line">            u_nm = u[n - m].unsqueeze(<span class="number">0</span>)</span><br><span class="line">            f_nm = model(u_nm)</span><br><span class="line">            residual += alpha[m] * u[n - m] + h * beta[m] * f_nm.squeeze()</span><br><span class="line">        loss += torch.norm(residual) ** <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> loss / (<span class="built_in">len</span>(u) - M + <span class="number">1</span>)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="基于多步神经网络改进的常微分方程反演算法"><a href="#基于多步神经网络改进的常微分方程反演算法" class="headerlink" title="基于多步神经网络改进的常微分方程反演算法"></a>基于多步神经网络<strong>改进</strong>的常微分方程反演算法</h3><h4 id="改进的多步神经网络算法"><a href="#改进的多步神经网络算法" class="headerlink" title="改进的多步神经网络算法"></a>改进的多步神经网络算法</h4><p>多步神经网络做出了比较理想的假设，即给出了所有的数值解数据是无噪声的，以及神经网络可以表示对常微分方程产生零残差的反演。这种假设是理想化的，我们可以在实际情况下使用改进的方法尝试发现未知常微分方程。  </p>
<p>对于多步神经网络，除线性多步法残差之外，如果神经网络能够满足一些通用近似性质，尤其是在多步神经网络反演法效果不佳的非自治常微分方程的反演问题中，可以扩展精确和完整数据的收敛结果。考虑到神经网络对函数近似的强大能力（见通用近似定理），这意味着至少在合适的光滑的常微分方程中存在良好的泛化误差。  </p>
<p>假设数据集为给定在若干时刻t_1、t_2、…、t_N的方程的数值解u的值，把每一时刻t_n对应的解u(t_n)记为u^(n)，而t_n对应的导数数据一般无从得知。当时间间隔h较小时，可用<strong>二阶中心差分法</strong>近似求出导数数据，记对于每一时刻t_n求出的导数数据为d^(n)，则：<br>$$<br>d^{(n)}=\begin{cases}(-3\boldsymbol{u}^{(n)}+4\boldsymbol{u}^{(n+1)}-\boldsymbol{u}^{(n+2)})/2h,\quad n=1,\(\boldsymbol{u}^{(n+1)}-\boldsymbol{u}^{(n-1)})/2h,\quad1&lt;n&lt;N,\(\boldsymbol{u}^{(n-2)}-4\boldsymbol{u}^{(n-1)}+3\boldsymbol{u}^{(n)})/2h,\quad n=N.&amp;\end{cases}<br>$$<br>显然二阶中心差分法求导数d^(n)的误差为o(h^2)。</p>
<p>对于常微分方程的反演，我们<strong>改进了多步神经网络的损失函数，其中既包含解数据，又包含导数数据</strong>：<br>$$<br>L=l_1(u,d)+l_2(u,d,f_{NN})<br>$$<br>其中：<br>$$<br>l_{1}=\frac{1}{N-M+1}\sum_{n=M}^{N}\left|\sum_{m=0}^{M}[\alpha_{m}u_{n-m}+h\beta_{m}f_{NN}(u_{n-m})]\right|^{2}<br>$$</p>
<p>$$<br>l_{2}=\frac{1}{N-M+1}\sum_{n=M}^{N}\left|f_{NN}(u_{n})-d_{n}\right|^{2}<br>$$</p>
<p>$$<br>n=M,\ldots,N<br>$$</p>
<p>式中向量范数采用二范数；u为数值解的数据，d为二阶中心差分法得到的导数数据，则有：</p>
<ul>
<li><strong>l_1</strong>：<strong>线性多步法残差</strong>，可以表示常微分方程的近似程度</li>
<li><strong>l_2</strong>：<strong>均方误差函数</strong>，可以表示网络结构的准确度</li>
</ul>
<p>把u^(n)作为输入，d^(n)作为期望输出进行训练，得到的f_NN即为对函数f的近似。训练流程与改进后的损失函数Python实现如下所示：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型训练 </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">u_tensor, model, loss_func, h, M</span>): </span><br><span class="line">    <span class="comment"># 参数说明：训练集u_tensor,待训练模型model,损失函数loss_func,时间步长h,步数M,最大训练次数EPOCH</span></span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=LR)</span><br><span class="line">    loss_history = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCHS):</span><br><span class="line">        model.train()</span><br><span class="line">        batch_u = u_tensor.to(device)</span><br><span class="line">        batch_d = d_tensor.to(device)</span><br><span class="line">        train_loss = loss_func(batch_u, batch_d, model, h, M)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        train_loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        epoch_avg_loss = train_loss.item()</span><br><span class="line">        loss_history.append(epoch_avg_loss)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> loss_history</span><br></pre></td></tr></tbody></table></figure>

<p>可以看到，与改进前相比，在训练过程中增加了对二阶中心差分法得到的导数的相关计算，进一步利用了训练数据中的信息。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss_func</span>(<span class="params">u, d, model, h, M</span>):</span><br><span class="line">    <span class="comment"># 参数说明：模型输入数据u,输入数据对应的真实微分数据d,待训练模型model,时间步长h,步数M</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据选定的方法确定系数α、β（示例：Adams-Moulton法，M=4）</span></span><br><span class="line">    alpha = [<span class="number">1</span>, -<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">    beta = [<span class="number">251</span>/<span class="number">720</span>, <span class="number">646</span>/<span class="number">720</span>, -<span class="number">264</span>/<span class="number">720</span>, <span class="number">106</span>/<span class="number">720</span>, -<span class="number">19</span>/<span class="number">720</span>]</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 多步法残差 l1</span></span><br><span class="line">    l1 = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(M, <span class="built_in">len</span>(u)):</span><br><span class="line">        residual = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> <span class="built_in">range</span>(M+<span class="number">1</span>):</span><br><span class="line">            u_nm = u[n - m].unsqueeze(<span class="number">0</span>)</span><br><span class="line">            f_nm = model(u_nm)</span><br><span class="line">            residual += alpha[m] * u[n - m] + h * beta[m] * f_nm.squeeze()</span><br><span class="line">        l1 += torch.norm(residual) ** <span class="number">2</span></span><br><span class="line">    l1 = l1 / (<span class="built_in">len</span>(u) - M + <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 导数拟合误差 l2</span></span><br><span class="line">    pred_d = model(u)</span><br><span class="line">    l2 = torch.mean(torch.norm(pred_d - d, dim=<span class="number">1</span>) ** <span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> l1 + l2  <span class="comment"># 总损失</span></span><br></pre></td></tr></tbody></table></figure>

<p>值得注意的是，针对非自治常微分方程，由于在将其变换为自治常微分方程时进行了升维操作，这意味着真实的导数向量在y=t这一分量上的值必然是1，因此后续在检验模型训练效果时可利用这一点对模型性能进行先决的把握。</p>
<h4 id="基于误差界理论论证改进算法的优越性"><a href="#基于误差界理论论证改进算法的优越性" class="headerlink" title="基于误差界理论论证改进算法的优越性"></a>基于误差界理论论证改进算法的优越性</h4><p>在上述损失函数中，l_1是对目标函数的良好近似，而l_2是神经网络中常用的损失函数。考虑到神经网络对函数近似的强大能力，这意味着即使对于噪声数据，改进的方法也应有良好的泛化能力和鲁棒性。在这里，我们先从理论上来论证改进方法在泛化能力与鲁棒性上的优越性，后续的数值实验部分将采用三类非自治方程的反演来验证改进的多步神经网络的效果。</p>
<p>对于二分类问题（可推广至函数近似的回归问题），当假设空间是有限个函数的集合F={f_1,f_2,…,f_d}时，对任意一个函数f∈F,至少以概率1-δ，以下不等式成立：<br>$$<br>R(f)\leq\hat{R}(f)+\varepsilon(d,N,\delta)<br>$$<br>其中：<br>$$<br>\begin{gathered}R(f)=E[L(Y,f(X))]\\hat{R}(f)=\frac{1}{N}\sum_{1}^{N}L(y_{i},f(x_{i}))\\varepsilon(d,N,\delta)=\sqrt{\frac{1}{2N}(\log d+\log\frac{1}{\delta})}\end{gathered}<br>$$<br>式中，R(f)为泛化误差（测试集上的测试风险），\hat{R}(f)为训练集上的经验风险，\hat{R}(f) + ε(d,N,δ)即为泛化误差界。观察上式可知，泛化误差界与样本数N成正比，与假设空间包含的函数数量d成反比。因此，当样本数N越大，泛化误差界越小，当假设空间F包含的函数越多，泛化误差界越大。</p>
<p>根据上述定理，针对改进后的神经网络，有如下不等式成立：<br>$$<br>\left|f_{NN}(u^{(n)})-\hat{f}(u^{(n)})\right|\leq\left|f_{NN}(u^{(n)})-d^{(n)}\right|+\left|\hat{f}(u^{(n)})-d^{(n)}\right|\leq w+o(h^{2})<br>$$<br>该不等式表明，改进后的损失函数可以拆解成两部分误差：</p>
<ul>
<li>第一部分为通过神经网络得到的导数近似值与通过二阶中心差分法求出的导数之间的误差，设该误差限为<strong>w</strong>；</li>
<li>第二部分为导数真实值与通过二阶中心差分法求出的导数之间的误差，根据二阶中心差分法的基本原理，其误差限为**o(h^2)**。</li>
</ul>
<p>显然改进后的多步神经网络的泛化误差相较改进前有了进一步的约束，尤其显著减小了<strong>噪声</strong>对模型的影响，具有<strong>更强的泛化能力和鲁棒性</strong>。</p>
<h2 id="数值实验与分析"><a href="#数值实验与分析" class="headerlink" title="数值实验与分析"></a>数值实验与分析</h2><p>为充分验证改进方法的泛化能力与鲁棒性，我们选取了六个实际的物理问题（对应常微分方程）作为测试用例，对应自治/非自治以及三种不同的线性多步法，在每一个测试用例内选取不同的线性多步法步数以及训练数据所含的高斯噪声方差，对改进前后的两种基于多步神经网络的常微分方程反演方法进行测试。下面我们以“受迫振动方程”这一用例详细展示测试流程，其他的测试用例仅作结果展示。</p>
<h3 id="数值实验流程"><a href="#数值实验流程" class="headerlink" title="数值实验流程"></a>数值实验流程</h3><h4 id="数据构建"><a href="#数据构建" class="headerlink" title="数据构建"></a>数据构建</h4><p>基于选定的测试用例对应的常微分方程，可采用如下步骤生成原始的真实数据集：</p>
<ol>
<li>在所给区域内均匀选取间隔为h的N个结点t_n,n=1,2,…,N ,使用Python中的Scipy库求出各结点对应的数值解u^(n)；</li>
<li>将数值解u^(n) 代入二阶中心差分法中得到每个x_n 对应的导数值d^(n) ；</li>
<li>噪声数据构建：在各结点对应的数值解u^(n) 上依次加上<strong>期望为0，方差为0、0.01、0.05的高斯噪声</strong>。</li>
</ol>
<p>在Python中，用函数<code>get_data()</code>函数实现了数据构建的代码封装：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_data</span>(<span class="params">t0, t_end, h, y0, noise_std=<span class="number">0.0</span></span>):     </span><br><span class="line">    t_eval = np.arange(t0, t_end + h, h)     </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 数值解    </span></span><br><span class="line">    sol = solve_ivp(forced_oscillator, [t0, t_end], y0, t_eval=t_eval)    </span><br><span class="line">    x = sol.y.T  <span class="comment"># shape: [N, 2]     </span></span><br><span class="line">    t = sol.t.reshape(-<span class="number">1</span>, <span class="number">1</span>)  <span class="comment"># shape: [N, 1]     </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 添加高斯噪声     </span></span><br><span class="line">    <span class="keyword">if</span> noise_std &gt; <span class="number">0</span>:         </span><br><span class="line">        x += np.random.normal(<span class="number">0</span>, noise_std, size=x.shape)      </span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 构造 u = [x1, x2, t]     </span></span><br><span class="line">    u = np.hstack([x, t])      </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 二阶中心差分求导数 d ≈ du/dt     </span></span><br><span class="line">    d = np.zeros_like(u)     </span><br><span class="line">    <span class="comment"># 边界点使用前向/后向差分     </span></span><br><span class="line">    d[<span class="number">0</span>] = (-<span class="number">3</span> * u[<span class="number">0</span>] + <span class="number">4</span> * u[<span class="number">1</span>] - u[<span class="number">2</span>]) / (<span class="number">2</span> * h)     </span><br><span class="line">    d[-<span class="number">1</span>] = (u[-<span class="number">3</span>] - <span class="number">4</span> * u[-<span class="number">2</span>] + <span class="number">3</span> * u[-<span class="number">1</span>]) / (<span class="number">2</span> * h)     </span><br><span class="line">    <span class="comment"># 内部点使用中心差分     </span></span><br><span class="line">    d[<span class="number">1</span>:-<span class="number">1</span>] = (u[<span class="number">2</span>:] - u[:-<span class="number">2</span>]) / (<span class="number">2</span> * h)      </span><br><span class="line">    </span><br><span class="line">    u_tensor = torch.tensor(u, dtype=torch.float32)     </span><br><span class="line">    d_tensor = torch.tensor(d, dtype=torch.float32)      </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> u_tensor, d_tensor, t<span class="comment"># 模型训练 </span></span><br></pre></td></tr></tbody></table></figure>

<p>以“受迫振动方程”这一测试框架为例，<code>force_oscillator</code>函数中刻画了该场景下的微分方程规律：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forced_oscillator</span>(<span class="params">t, y</span>):</span><br><span class="line">    x1, x2 = y</span><br><span class="line">    dx1dt = x2</span><br><span class="line">    dx2dt = -np.cos(t) * x1 - x2 + t / <span class="number">50</span></span><br><span class="line">    <span class="keyword">return</span> [dx1dt, dx2dt]</span><br></pre></td></tr></tbody></table></figure>

<p>生成的无噪声数据如下图所示：</p>
<p><img src="/images/project3/2.png" alt="受迫振动方程数据构建x1"></p>
<p><img src="/images/project3/3.png" alt="受迫振动方程数据构建x2"></p>
<h4 id="神经网络训练"><a href="#神经网络训练" class="headerlink" title="神经网络训练"></a>神经网络训练</h4><p>根据上述算法理论部分列出的训练流程框架，适用数据（u，d）对神经网络进行训练，训练后得到的函数f_NN即为对函数f的近似。</p>
<p>关于神经网络的结构与参数，本项目中所使用的<strong>神经网络结构</strong>均相同（后续不再赘述），均<strong>包含4个隐藏层，每层128个神经元，激活函数为tanh</strong>，Python中基于pytorch框架对神经网络架构的实现如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ODEApproximator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim=<span class="number">3</span>, hidden_dim=<span class="number">128</span>, output_dim=<span class="number">3</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(ODEApproximator, self).__init__()</span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Linear(input_dim, hidden_dim),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(hidden_dim, hidden_dim),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(hidden_dim, hidden_dim),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(hidden_dim, hidden_dim),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(hidden_dim, output_dim)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.net(x)</span><br></pre></td></tr></tbody></table></figure>

<p>训练过程中实时记录了每一个epoch的损失函数值，下图分别绘制了改进前后的多步神经网络在无噪声数据集上训练过程中损失函数值的变化情况（总Epoch=100），可以看到损失函数均呈收敛态势，且改进后的多步神经网络收敛更快：</p>
<p><img src="/images/project3/4.png" alt="改进前多步神经网络训练过程损失函数值变化"></p>
<p><img src="/images/project3/5.png" alt="改进后多步神经网络训练过程损失函数值变化"></p>
<h4 id="效果检验"><a href="#效果检验" class="headerlink" title="效果检验"></a>效果检验</h4><p>为定量地比较改进前后的多步神经网络的泛化性能与鲁棒性，采取如下的实验步骤进行训练后模型的效果检验：</p>
<ol>
<li>用训练得到的函数f_NN解方程得到数值解u_NN^(n) ,观察近似效果，计算均方误差mse_x 和平均绝对误差mae_x并与多步神经网络作比较；</li>
<li>用数值解u^(n)代入训练得到的函数f_NN得到的导数的近似值d_NN^(n)，将u^(n)代入给定的常微分方程中的 f 得到导数的真实值d^(n),计算均方误差mse_f和平均绝对误差mae_f并与多步神经网络作比较。</li>
</ol>
<p>其中用于比较性能的指标均方误差mse和平均绝对误差mae的定义式如下所示：<br>$$<br>mse_{x}=\frac{1}{ND}\sum_{n=1}^{N}\left|u_{NN}^{(n)}-u^{(n)}\right|_{2}^{2}<br>$$</p>
<p>$$<br>mse_{f}=\frac{1}{ND}\sum_{n=1}^{N}\left|d_{NN}^{(n)}-d^{(n)}\right|_{2}^{2}<br>$$</p>
<p>$$<br>mae_{x}=\frac{1}{ND}\sum_{n=1}^{N}\left|u_{NN}^{(n)}-u^{(n)}\right|_{1}<br>$$</p>
<p>$$<br>mae_{f}=\frac{1}{ND}\sum_{n=1}^{N}\left|d_{NN}^{(n)}-d^{(n)}\right|_{1}<br>$$</p>
<p>Python中将利用训练好的模型进行预测的过程封装成函数<code>predict_u</code>，指标计算直接调用scikit-learn函数库中的相关函数：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">rhs</span>(<span class="params">t, u_np</span>):      </span><br><span class="line">    <span class="string">"""用于 solve_ivp：输入 u = [x1, x2, t] 输出 du/dt"""</span>      </span><br><span class="line">    u_tensor = torch.tensor(u_np, dtype=torch.float32).unsqueeze(<span class="number">0</span>).to(device)      </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():          </span><br><span class="line">        du_dt = model(u_tensor).squeeze().cpu().numpy()      </span><br><span class="line">    <span class="keyword">return</span> du_dt    </span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict_u</span>(<span class="params">u_tensor</span>):      </span><br><span class="line">    <span class="comment"># 初始条件与时间点一致     </span></span><br><span class="line">    u0 = u_tensor[<span class="number">0</span>].cpu().numpy()  <span class="comment"># [x1, x2, t]    </span></span><br><span class="line">    t_span = (<span class="number">0</span>, <span class="number">100</span>)  </span><br><span class="line">    t_eval = np.arange(<span class="number">0</span>, <span class="number">100</span> + <span class="number">0.05</span>, <span class="number">0.05</span>)    </span><br><span class="line">    </span><br><span class="line">    sol_nn = solve_ivp(rhs, t_span, u0, t_eval=t_eval)   </span><br><span class="line">    <span class="keyword">return</span> sol_nn.y.T  <span class="comment"># shape: [N, 3]</span></span><br></pre></td></tr></tbody></table></figure>

<p>在“受迫振动方程”这一测试案例中，基于训练轮数Epoch=50的模型，运行以上检验流程，得到如下检验指标数据：</p>
<p><img src="/images/project3/6.png" alt="Epoch=50时数值解预测误差"></p>
<p>可以看到，随着<strong>噪声强度增加</strong>，两种方法的<strong>误差均增大</strong>，但<strong>改进方法对数值解的预测误差在各种情况下均小于多步神经网络</strong>。 </p>
<p><img src="/images/project3/7.png" alt="Epoch=50时导数值预测误差"></p>
<p>可以看到，当数据无噪声时，本文提出的<strong>改进方法</strong>对于受迫振动方程的<strong>近似效果强</strong>于多步神经网络;随着<strong>噪声强度的增加</strong>，<strong>多步神经网络的误差大幅增加</strong>，而<strong>改进的方法的误差一直保持较小</strong>且始终强于多步神经网络。</p>
<p>除此之外，针对在无噪声数据集上训练轮数分别为Epoch=100和Epoch=500时的模型，将其作为微分方程中的f用于预测数据的生成，效果分别如下图所示：</p>
<p><img src="/images/project3/8.png" alt="Epoch=100时预测效果与真实值对比"></p>
<p><img src="/images/project3/9.png" alt="Epoch=500时预测效果与真实值对比"></p>
<p>可以看到，随着训练轮数的增加，模型对于实际微分方程的拟合效果越来越好，生成的预测数据也越来越接近用于训练的真实数据；事实上，从理论上讲，大概在训练轮数Epoch达到10^4数量级时才会有较为精准的拟合效果，本项目由于用于训练的计算资源有限，仅进行了最大训练轮数为1000的测试。在后续的结果分析部分，将直接采用高训练轮数下的精确结果进行展示。</p>
<h3 id="数值实验结果分析"><a href="#数值实验结果分析" class="headerlink" title="数值实验结果分析"></a>数值实验结果分析</h3><h4 id="非自治常微分方程"><a href="#非自治常微分方程" class="headerlink" title="非自治常微分方程"></a>非自治常微分方程</h4><h5 id="受迫振动方程——Adams-Moulton法"><a href="#受迫振动方程——Adams-Moulton法" class="headerlink" title="受迫振动方程——Adams-Moulton法"></a>受迫振动方程——Adams-Moulton法</h5><p>受迫振动方程(Forced Oscillator)是描述一个振动系统在外界力的作用下进行振动的方程。这个方程可以用来研究各种物理系统中的振动现象，例如弹簧振子、摆锤、电路振荡等。受迫振动方程的核心思想是将外界力引入方程中，以描述振动系统在外界激励下的行为。受迫振动方程的研究对于科学、工程和技术领域具有重要意义。它不仅有助于我们深入理解振动现象的本质和规律，还为我们设计和优化振动控制系统、减振装置等提供了理论基础。同时，它在电子设备、结构工程、交通运输等方面也有广泛的应用。</p>
<p>实验使用的受迫振动方程如下：<br>$$<br>\begin{aligned}&amp;\frac{dx_1}{dt}=x_2,\&amp;\frac{dx_2}{dt}=-cosy\cdot x_1-x_2+\frac{y}{50},\&amp;\frac{dy}{dt}=1\end{aligned}<br>$$<br>使用[0,1,0]T作为初值，生成从t=0到t=100，间隔h=0.05的数据作为训练集，最终得到的多步神经网络对常微分方程的反演效果如下图所示，其中红色曲线是原方程数值解的曲线，蓝色点是用无噪声数据训练的函数f_NN代替f得到的数值解:</p>
<p><img src="/images/project3/10.png" alt="受迫振动方程反演效果"></p>
<h5 id="线性标量方程——Adams-Bashforth法"><a href="#线性标量方程——Adams-Bashforth法" class="headerlink" title="线性标量方程——Adams-Bashforth法"></a>线性标量方程——Adams-Bashforth法</h5><p>线性标量方程(Linear Scalar Equation)是一种描述某个未知函数与其导数之间的关系的方程，其中未知函数的导数的最高次数为1。线性标量方程是微分方程中的一类重要问题，它在数学物理等领域中有广泛应用。线性标量方程的研究对于数学、物理学和工程学等领域具有重要意义。它可以用来描述动力学系统的行为、传热和传质过程、量子力学中的波函数演化等。通过分析和求解线性标量方程，可以深入理解系统的特征、稳定性、渐近行为等，为问题的模拟、控制和优化提供理论基础。在现代科学和工程中，线性标量方程的研究得到了进一步推广和延伸。例如，非线性标量方程、偏微分方程等是线性标量方程的扩展和推广，它们更好地描述了一些复杂的物理现象和现实问题。</p>
<p>实验使用的线性标量方程如下：<br>$$<br>\frac{dx_1}{dt}=-x_1(sin4y+1)+cos\frac{y^2}{1000},<br>$$</p>
<p>$$<br>\frac{dy}{dt}=1<br>$$</p>
<p>使用[2,0]T作为初值，生成从t=0到t=20，间隔h=0.05的数据作为训练集，最终得到的多步神经网络对常微分方程的反演效果如下图所示，其中红色曲线是原方程数值解的曲线，蓝色点是用无噪声数据训练的函数f_NN代替f得到的数值解:</p>
<p><img src="/images/project3/11.png" alt="线性标量方程反演效果"></p>
<h5 id="食饵-捕食者模型方程——后向微分公式法"><a href="#食饵-捕食者模型方程——后向微分公式法" class="headerlink" title="食饵-捕食者模型方程——后向微分公式法"></a>食饵-捕食者模型方程——后向微分公式法</h5><p>食饵-捕食者模型(Predator-prey Model)是生态学中研究食物链和生物群落动力学的重要模型之一，描述了食物链中食饵(被捕食者)和捕食者之间的相互作用关系。这种模型的研究对于我们理解生态系统的稳定性、物种相互作用以及生态系统中物种数量的变化具有重要意义。通过食饵-捕食者模型，我们可以研究食饵与捕食者之间的数量关系以及它们之间的相互作用。一般来说，食饵的数量被认为是捕食者的食物来源，并且捕食者的数量受到食饵的供给和捕食行为的影响。该模型通常描述了食饵数量随时间的变化，以及捕食者数量随时间的变化。食饵-捕食者模型的研究对于生态学、环境保护和资源管理等领域具有重要意义。通过该模型，我们可以深入了解生态系统中物种数量的变化规律，从而预测物种灭绝和生态系统崩溃的风险，以及寻找保护和管理生物多样性的方法。</p>
<p>实验使用的食饵-捕食者模型方程如下：<br>$$<br>\begin{aligned}&amp;\frac{dx_1}{dt}=x_1-x_1\cdot x_2+sin\frac{y}{2}+cosy+2,\&amp;\frac{dx_2}{dt}=x_1\cdot x_2-x_2,\&amp;\frac{dy}{dt}=1\end{aligned}<br>$$<br>使用[3,3,0]T作为初值，生成从t=0到t=50，间隔h=0.05的数据作为训练集，最终得到的多步神经网络对常微分方程的反演效果如下图所示，其中红色曲线是原方程数值解的曲线，蓝色点是用无噪声数据训练的函数f_NN代替f得到的数值解:</p>
<p><img src="/images/project3/12.png" alt="食饵-捕食者模型方程反演效果"></p>
<h4 id="自治常微分方程"><a href="#自治常微分方程" class="headerlink" title="自治常微分方程"></a>自治常微分方程</h4><h5 id="线性常微分方程——Adams-Moulton法"><a href="#线性常微分方程——Adams-Moulton法" class="headerlink" title="线性常微分方程——Adams-Moulton法"></a>线性常微分方程——Adams-Moulton法</h5><p>线性常微分方程(Linear ODEs)是微分方程中的一类重要问题，它描述了未知函数及其导数之间的线性关系。线性常微分方程的研究对于数学、物理学和工程学等领域具有重要意义。它们出现在物理学中许多基础问题的数学描述中，例如弹簧振动、电路分析、传热过程等。在工程学中，线性常微分方程也广泛应用于控制系统的建模和分析。随着科学技术的发展，研究者提出了各种各样的数值方法和近似方法，以求解复杂的线性常微分方程。这些方法包括欧拉方法、龙格-库塔方法、有限差分方法、变分法等，它们为实际问题的求解提供了有效的数值工具。随着对非线性动力学系统的研究和认识的深入，研究者们开始将非线性常微分方程引入到线性常微分方程中，以描述更为复杂的现象和现实问题。</p>
<p>实验使用的线性常微分方程如下：<br>$$<br>\frac{dx_1}{dt}=x_1-4x_2,<br>$$</p>
<p>$$<br>\frac{dx_2}{dt}=4x_1-7x_2<br>$$</p>
<p>使用[0,-1]T作为初值，生成从t=0到t=10，间隔h=0.01的数据作为训练集，最终得到的多步神经网络对常微分方程的反演效果如下图所示，其中红色曲线是原方程数值解的曲线，蓝色点是用无噪声数据训练的函数f_NN代替f得到的数值解:</p>
<p><img src="/images/project3/13.png" alt="线性常微分方程反演效果"></p>
<h5 id="阻尼三次振子方程——Adams-Bashforth法"><a href="#阻尼三次振子方程——Adams-Bashforth法" class="headerlink" title="阻尼三次振子方程——Adams-Bashforth法"></a>阻尼三次振子方程——Adams-Bashforth法</h5><p>阻尼三次振子(Damped Cubic Oscillator)是一种物理系统，它描述了受到阻尼力和弹力作用的振动系统。阻尼指的是系统中存在能量损耗的因素，如摩擦力，它会导致振动的逐渐减弱；三次振子表示系统的势能函数是一个三次函数，它具有非线性的特性。阻尼三次振子最早出现在振动力学和非线性动力学的研究中。对于振动系统的研究是物理学、工程学和应用数学等领域的重要课题之一。阻尼三次振子的研究对于理解和预测复杂动力学系统的行为具有重要意义。通过分析阻尼三次振子的运动规律和稳定性，我们可以深入了解非线性振动系统的动力学特性，例如振动的周期性、混沌行为等。这些研究也为控制系统、电子电路、力学系统等领域的工程应用提供了参考和启示。 </p>
<p>实验使用的阻尼三次振子方程如下：<br>$$<br>\frac{dx_1}{dt}=-0.1x_1^3+2x_2^3,<br>$$</p>
<p>$$<br>\frac{dx_2}{dt}=-2x_1^3-0.1x_2^3<br>$$</p>
<p>使用[1,1]T作为初值，生成从t=0到t=25，间隔h=0.01的数据作为训练集，最终得到的多步神经网络对常微分方程的反演效果如下图所示，其中红色曲线是原方程数值解的曲线，蓝色点是用无噪声数据训练的函数f_NN代替f得到的数值解:</p>
<p><img src="/images/project3/14.png" alt="阻尼三次振子方程反演效果"></p>
<h5 id="阻尼简谐摆方程——后向微分公式法"><a href="#阻尼简谐摆方程——后向微分公式法" class="headerlink" title="阻尼简谐摆方程——后向微分公式法"></a>阻尼简谐摆方程——后向微分公式法</h5><p>阻尼简谐摆(Damped Pendulum)是一个经典力学中的物理系统，它由一个具有质量的物体通过一根轻质绳或杆与一个固定支点相连接组成。阻尼简谐摆在受到重力作用下，沿着一条弧线进行周期性振动。阻尼简谐摆的研究对于理解和预测振动系统的行为具有重要意义。通过分析阻尼简谐摆的运动规律和稳定性，我们可以深入了解振动系统在存在阻尼时的响应特性，例如振动的振幅、频率等。这些研究不仅在理论物理学中具有重要意义，也为工程学中的控制系统、机械振动和结构动力学等领域的应用提供了基础。</p>
<p>实验使用的阻尼简谐摆方程如下：<br>$$<br>\begin{aligned}&amp;\frac{dx_1}{dt}=x_2,\&amp;\frac{dx_2}{dt}=-\alpha x_2-\beta\sin x_1,\&amp;\alpha=0.2,\beta=8.91\end{aligned}<br>$$<br>使用[-1.193,-3.876]T作为初值，生成从t=0到t=20，间隔h=0.01的数据作为训练集，最终得到的多步神经网络对常微分方程的反演效果如下图所示，其中红色曲线是原方程数值解的曲线，蓝色点是用无噪声数据训练的函数f_NN代替f得到的数值解:</p>
<p><img src="/images/project3/15.png" alt="阻尼简谐摆方程反演效果"></p>
<h2 id="研究结论与创新"><a href="#研究结论与创新" class="headerlink" title="研究结论与创新"></a>研究结论与创新</h2><h3 id="研究结论"><a href="#研究结论" class="headerlink" title="研究结论"></a>研究结论</h3><p>本项目<strong>针对常微分方程的反演问题提出了一种基于多步神经网络改进的常微分方程反演算法</strong>，旨在<strong>从数据中提取内含的常微分方程</strong>。通过<strong>将导数数据融入训练数据</strong>，我们改进了多步神经网络的数据集，并<strong>优化了神经网络的损失函数</strong>，以<strong>提高模型的准确性和泛化能力</strong>。经过训练，改进的神经网络成功拟合原方程中的函数，实现了对非自治常微分方程的反演。我们还推导并讨论了改进算法的误差界。我们进一步将改进的算法应用于自治常微分方程反演，通过训练网络拟合原方程函数，拓展了算法在自治常微分方程反演中的适用性。  </p>
<p>本项目通过实验以非自治常微分方程中的受迫振动方程、线性标量方程和食饵-捕食者模型方程，以及自治常微分方程中的线性常微分方程、阻尼三次振子方程和阻尼简谐摆方程为例，展示了算法的有效性。在实验中，我们添加了不同强度的噪声，并利用改进算法对常微分方程进行反演。实验结果表明，<strong>改进方法在大多数情况下优于多步神经网络，在处理有噪声数据时表现更为出色</strong>。这些结果充分验证了本文提出的方法在反演问题中的可行性和优越性。</p>
<h3 id="研究方法创新性"><a href="#研究方法创新性" class="headerlink" title="研究方法创新性"></a>研究方法创新性</h3><p>传统方法通常利用多步神经网络来处理常微分方程的反演问题，主要集中在处理自治常微分方程，对于非自治常微分方程的反演效果不尽理想，特别是在使用带有噪声数据进行训练时表现不佳。本研究首次将研究重点聚焦于非自治常微分方程的反演，通过<strong>将非自治方程转化为自治方程</strong>的形式，提出了一种新的研究思路。针对带有噪声数据的方程反演问题，采用了<strong>改进损失函数</strong>的方法以提升算法的鲁棒性。此外，将这种新方法扩展应用于自治常微分方程的反演问题，结果显示其反演效果明显优于多步神经网络。这一新思路对解决常微分方程反演问题带来了一种新的方法。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] 王开荣,杨大地编著.应用数值分析[M].高等教育出版社,2010.</p>
<p>[2] 付长铠.常微分方程反演的机器学习方法研究[D].长春工业大学,2024.</p>
<p>[3] Raissi M, Perdikaris P, Karniadakis G E. Multistep neural networks for data-driven discovery of nonlinear dynamical systems[J]. arXiv preprint arXiv:1801.01236, 2018.</p>
<p>[4] 李航. 统计学习方法[M]. 第二版. 北京：清华大学出版社, 2019.</p>
<p>[5] 陈新海, 刘杰, 万仟, 等. 一种改进的基于深度神经网络的偏微分方程求解方法[J]. 计算机工程与科学, 2022, 44(11): 1932-1940.</p>
</div></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2025-04-22T18:07:03.000Z" title="2025/4/23 02:07:03">2025-04-23</time>发表</span><span class="level-item"><time datetime="2025-04-29T15:58:08.821Z" title="2025/4/29 23:58:08">2025-04-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/">课程项目</a><span>&nbsp;/&nbsp;</span><a class="link-muted" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E5%B7%A5%E7%A8%8B%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90/">工程数值分析</a></span><span class="level-item">1 小时读完 (大约8883个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2025/04/23/project02/">图像插值算法及其优化</a></p><div class="content"><div id="postchat_postcontent"><h2 id="研究背景及其意义"><a href="#研究背景及其意义" class="headerlink" title="研究背景及其意义"></a>研究背景及其意义</h2><p>图像<strong>放大</strong>与<strong>旋转</strong>是数字图像处理中最基础的几何变换操作，其核心在于如何通过插值算法重建原始图像中不存在的像素信息。当对图像进行放大操作时，输出图像的像素网格会超出原始图像的采样范围，需要通过插值来填补这些新增像素点的颜色值；而在旋转操作中，即使保持图像尺寸不变，原始像素的整数坐标经过旋转变换后也会落在新图像的非整数位置，同样需要通过插值来重新确定每个输出像素的颜色值。</p>
<p>图像插值是利用原图像中的颜色值通过一定的方法计算出待插入像素点的颜色值的过程。对图像进行插值一般有两个步骤：首先定义一个图像插值公式，然后利用该插值公式计算待插入点的颜色值。常见的图像插值算法有双线性法、最近邻法、非均匀法、双三次卷积插值法、双立方法、Lagrange法、 样条插值法、 克里金（Krijing） 插值法等。这些插值方法通常定义一个插值数据点的隐式函数，再提取该函数的等值面作为图像插值方法，常用的插值核包括线性插值核、样条插值核等。</p>
<ul>
<li><strong>最近邻插值</strong>作为最简单的算法，直接将距离待插值点最近的已知像素值作为结果，虽然计算效率极高（时间复杂度O(1)），但会产生明显的块状伪影（“马赛克”）和锯齿形边缘；</li>
<li><strong>双线性插值</strong>通过考虑2×2邻域内四个像素的加权平均，在计算成本（O(n)）和视觉效果之间取得平衡，但仍会导致高频信息丢失和边缘模糊；</li>
<li>更高阶的<strong>双三次插值</strong>（使用4×4邻域）和样条插值虽然能提供更平滑的结果，但计算复杂度显著增加（O(n²)），且可能引入不必要的振铃效应。</li>
</ul>
<p>现有算法的根本<strong>局限</strong>在于<strong>采用统一的插值核函数处理整幅图像，忽视了图像不同区域的特征差异</strong>。例如，在平坦区域使用复杂插值会造成计算资源浪费，而在纹理丰富区域使用简单插值又会导致细节损失。基于此，我们希望通过改良的<strong>四平面插值</strong>算法对图像的放大与旋转效果进行优化，<strong>根据图像局部特征自适应地选择不同的插值策略</strong>，以规避用同一个插值公式对所有像素进行插值存在的不足。</p>
<h2 id="常用图像插值算法"><a href="#常用图像插值算法" class="headerlink" title="常用图像插值算法"></a>常用图像插值算法</h2><p>课本在6.5节中提到，在插值节点数量较多时，为避免Runge振荡现象的发生，并不提倡用高次多项式进行插值，而宁可用低次多项式作分段插值。在图像处理这一特定的应用场景中，需要处理的图像尺寸规模往往较大，且同一行（列）的所有像素颜色值显然并不具有可以用一个多项式函数显式表达的规律，但相邻的像素点颜色值之间又存在一定的关联性，因此分段插值仅考虑局部特征的特性在这里能够良好地契合所需性能。根据对于待插入像素点周围已有的像素点信息的利用情况，这里列举了几种常见的图像插值算法：</p>
<ul>
<li>最近邻法：仅利用待插值像素点转换至原图像坐标后距离其最近的一个像素点的颜色值，将其直接作为待插值像素点的颜色值</li>
<li>双线性法：利用待插值像素点转换至原图像坐标后距离其最近的四个像素点的颜色值，加权平均后作为待插值像素点的颜色值</li>
<li>双立方法：利用待插值像素点转换至原图像坐标后距离其最近的十六个像素点的颜色值，加权平均后作为待插值像素点的颜色值</li>
</ul>
<h3 id="最近邻法"><a href="#最近邻法" class="headerlink" title="最近邻法"></a>最近邻法</h3><p><img src="/images/project2/3.png" alt="一维最近邻插值示意图"></p>
<p>如上图所示，在一维最近邻插值中，坐标轴上各点 xi-1，xi，xi+1 … 两两对半等分间隔 (红色虚线划分)，从而非边界的各坐标点都有一个等宽的邻域，并根据每个坐标点的值构成一个类似分段函数的函数约束，从而使各插值坐标点的值等同于所在邻域原坐标点的值。例如，插值点 x 坐落于 坐标点 xi 的邻域，那么其值 f(x) 就等于 f(xi)。</p>
<p>在二维的图像插值场景中，可以对上述一维最近邻插值进行推广，如下图所示：</p>
<p><img src="/images/project2/4.png" alt="二维最近邻插值示意图"></p>
<p>可以看到，(x0, y0)、(x0, y1)、(x1, y0)、(x1, y1) 都是原图像上的坐标点，颜色值分别对应为 Q11、Q12、Q21、Q22。而颜色值未知的插值点 (x, y)（需转换至原图像坐标），根据最近邻插值方法的约束，其与坐标点 (x0, y0) 位置最接近 (即位于  (x0, y0) 的邻域内)，故插值点 (x, y) 的颜色值 P = Q11。</p>
<p>总而言之，最近邻法的基本思想即：<strong>将待插入点的坐标进行四舍五入，再以该行列坐标都是整数点的颜色值（灰度值）替代待插入点(x, y)处的颜色值。</strong>事实上，这也正是机器学习中KNN（K-Nearest Neighbor）算法在K=1时的情形。</p>
<p>基于以上算法思想，编写python函数代码实现图像放缩与旋转过程中的最近邻法插值：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 最近邻法插值实现图像放缩</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">nearest_neighbor_interpolation</span>(<span class="params">image, scale_factor</span>):</span><br><span class="line">    h, w, channel = image.shape</span><br><span class="line">    new_h, new_w = <span class="built_in">int</span>(h * scale_factor), <span class="built_in">int</span>(w * scale_factor)</span><br><span class="line">    resized_image = np.zeros((new_h, new_w, <span class="built_in">int</span>(channel)), dtype=image.dtype)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(new_h):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(new_w):</span><br><span class="line">            src_i = <span class="built_in">int</span>(<span class="built_in">round</span>((i + <span class="number">1</span>) / scale_factor, <span class="number">0</span>))</span><br><span class="line">            src_j = <span class="built_in">int</span>(<span class="built_in">round</span>((j + <span class="number">1</span>) / scale_factor, <span class="number">0</span>))</span><br><span class="line">            resized_image[i, j] = image[src_i - <span class="number">1</span>, src_j - <span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> resized_image</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最近邻法插值实现图像旋转</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">nearest_neighbor_rotation</span>(<span class="params">image, angle</span>):</span><br><span class="line">    h, w, channel = image.shape</span><br><span class="line">    angle_rad = math.radians(angle)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算旋转后的图像尺寸</span></span><br><span class="line">    cos_theta = <span class="built_in">abs</span>(math.cos(angle_rad))</span><br><span class="line">    sin_theta = <span class="built_in">abs</span>(math.sin(angle_rad))</span><br><span class="line">    new_w = <span class="built_in">int</span>(h * sin_theta + w * cos_theta)</span><br><span class="line">    new_h = <span class="built_in">int</span>(h * cos_theta + w * sin_theta)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 旋转中心</span></span><br><span class="line">    cx, cy = w / <span class="number">2</span>, h / <span class="number">2</span></span><br><span class="line">    new_cx, new_cy = new_w / <span class="number">2</span>, new_h / <span class="number">2</span></span><br><span class="line">    </span><br><span class="line">    rotated_image = np.zeros((new_h, new_w, channel), dtype=image.dtype)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(new_h):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(new_w):</span><br><span class="line">            <span class="comment"># 将新图像坐标转换回原图像坐标</span></span><br><span class="line">            x = (j - new_cx) * math.cos(angle_rad) + (i - new_cy) * math.sin(angle_rad) + cx</span><br><span class="line">            y = -(j - new_cx) * math.sin(angle_rad) + (i - new_cy) * math.cos(angle_rad) + cy</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 最近邻插值</span></span><br><span class="line">            <span class="keyword">if</span> <span class="number">0</span> &lt;= x &lt; w <span class="keyword">and</span> <span class="number">0</span> &lt;= y &lt; h:</span><br><span class="line">                src_x = <span class="built_in">int</span>(<span class="built_in">round</span>(x))</span><br><span class="line">                src_y = <span class="built_in">int</span>(<span class="built_in">round</span>(y))</span><br><span class="line">                rotated_image[i, j] = image[src_y - <span class="number">1</span>, src_x - <span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> rotated_image</span><br></pre></td></tr></tbody></table></figure>

<h3 id="双线性法"><a href="#双线性法" class="headerlink" title="双线性法"></a>双线性法</h3><p><img src="/images/project2/5.png" alt="一维线性插值示意图"></p>
<p>如上图所示，在一维的线性插值中，坐标轴上各点 xi-1，xi，xi+1 … 的值“两两直接相连”为线段，从而构成了一条连续的约束函数。而插值坐标点例如 x，根据约束函数其值应为 f(x)。因为每两个坐标点之间的约束函数曲线是一次线性的线段，对插值结果而言是“线性” 的，所以该方法称为线性插值。基于线性函数的特性，可以便捷地求取原图像上的两个像素点间任一待插值点的颜色值：</p>
<p><img src="/images/project2/6.png" alt="一维线性插值计算示意图"></p>
<p>可以看到，图中 x0 和 x1 都是原有的坐标点，颜色值分别对应为 y0 和 y1，此时根据线性插值法约束，在 (x0, y0) 和 (x1, y1) 构成的一次函数上，颜色值未知的插值点 x的颜色值 y 即为：<br>$$<br>y=y_0+(x-x_0)\frac{y_1-y_0}{x_1-x_0}=y_0+\frac{(x-x_0)y_1-(x-x_0)y_0}{x_1-x_0}<br>$$<br>实际上，即便 x 不在 x0 与 x1 之间，该公式也成立（此时为线性外插），但图像处理中不需涉及此情形。 </p>
<p>从一维的线性插值出发，很容易拓展到二维图像的双线性插值，通过三次一阶线性插值（本质为加权求和）获得最终结果，下图便展示了该过程的定性斜视与定量俯视示意图：</p>
<p><img src="/images/project2/7.png" alt="二维线性插值定性斜视示意图"></p>
<p><img src="/images/project2/8.png" alt="二维线性插值定量俯视示意图"></p>
<p>其中，(x0, y0)、(x0, y1)、(x1, y0)、(x1, y1) 均为原图像上的像素坐标点，颜色值分别对应为 f(x0, y0)、f(x0, y1)、f(x1, y0)、f(x1, y1)。而颜色值未知的插值点 (x, y)，根据双线性插值法的约束，可以先由像素坐标点 (x0, y0) 和 (x0, y1) 在 y 轴向作一维线性插值得到 f(x0, y)、由像素坐标点 (x1, y0) 和 (x1, y1) 在 y 轴向作一维线性插值得到 f(x1, y)，然后再由 (x0, y) 和 (x1, y) 在 x 轴向作一维线性插值得到插值点 (x, y) 的灰度值 f(x, y)。</p>
<p>事实上，一维线性插值先作 x 轴向再作 y 轴向，得到的结果完全相同，仅为顺序先后的区别。这里不妨先由像素坐标点 (x0, y0) 和 (x1, y0) 在 x 轴向作一维线性插值得到 f(x, y0)、由像素坐标点 (x0, y1) 和 (x1, y1) 在 x 轴向作一维线性插值得到 f(x, y1)：<br>$$<br>f(x,y_0)=\frac{x_1-x}{x_1-x_0}f(x_0,y_0)+\frac{x-x_0}{x_1-x_0}f(x_1,y_0)<br>$$</p>
<p>$$<br>f(x,y_1)=\frac{x_1-x}{x_1-x_0}f(x_0,y_1)+\frac{x-x_0}{x_1-x_0}f(x_1,y_1)<br>$$</p>
<p>然后再由 (x, y0) 和 (x, y1) 在 y 轴向作一维线性插值得到插值点 (x, y) 的灰度值 f(x, y)：<br>$$<br>f(x,y)=\frac{y_1-y}{y_1-y_0}f(x,y_0)+\frac{y-y_0}{y_1-y_0}f(x,y_1)<br>$$<br>合并上述式子，得到最终的双线性插值结果：<br>$$<br>f(x,y)=\frac{(y_1-y)(x_1-x)}{(y_1-y_0)(x_1-x_0)}f(x_0,y_0)+\frac{(y_1-y)(x-x_0)}{(y_1-y_0)(x_1-x_0)}f(x_1,y_0)+\frac{(y-y_0)(x_1-x)}{(y_1-y_0)(x_1-x_0)}f(x_0,y_1)+\frac{(y-y_0)(x-x_0)}{(y_1-y_0)(x_1-x_0)}<br>$$<br>值得注意的是，在实际的图像插值处理过程中，为尽量保证插值效果的准确性，往往仅采用距离待插值点（转换至原图像坐标）最近的四个点，即:（[]符号表示待插值点转换至原图像坐标后向下取整）<br>$$<br>x_0=[x]，y_0=[y]<br>$$</p>
<p>$$<br>x_1=x_0+1，y_1=y_0+1<br>$$</p>
<p>从加权求和的角度理解，可以进一步地将双线性插值结果改写为如下形式：<br>$$<br>p=x-[x], q=y-[y]<br>$$</p>
<p>$$<br>\begin{array}{rcl}f(x,y)=(1-q){(1-p)f([x][y])+pf([x]+1,[y])}+q{(1-p)f([x],[y]+1)+pf([x]+1,[y]+1)}\end{array}<br>$$</p>
<p><img src="/images/project2/9.png" alt="二维线性插值加权求和角度示意图"></p>
<p>基于以上算法思想，编写python函数代码实现图像放缩与旋转过程中的双线性法插值：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 双线性法插值实现图像放缩</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bilinear_interpolation</span>(<span class="params">image, scale_factor</span>):</span><br><span class="line">    h, w, channel = image.shape</span><br><span class="line">    new_h, new_w = <span class="built_in">int</span>(h * scale_factor), <span class="built_in">int</span>(w * scale_factor)</span><br><span class="line">    resized_image = np.zeros((new_h, new_w, <span class="built_in">int</span>(channel)), dtype=image.dtype)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(new_h):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(new_w):</span><br><span class="line">            x = (j + <span class="number">1</span>) / scale_factor</span><br><span class="line">            y = (i + <span class="number">1</span>) / scale_factor</span><br><span class="line">            x1 = <span class="built_in">int</span>(x)</span><br><span class="line">            y1 = <span class="built_in">int</span>(y)</span><br><span class="line">            x2 = x1 + <span class="number">1</span></span><br><span class="line">            y2 = y1 + <span class="number">1</span></span><br><span class="line">            p = x - x1</span><br><span class="line">            q = y - y1</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 边界问题处理</span></span><br><span class="line">            <span class="keyword">if</span> x2 == w + <span class="number">1</span>:</span><br><span class="line">                x2 = x1</span><br><span class="line">            <span class="keyword">if</span> y2 == h + <span class="number">1</span>:</span><br><span class="line">                y2 = y1</span><br><span class="line">                </span><br><span class="line">            resized_image[i ,j] = (<span class="number">1</span> - q) * ((<span class="number">1</span> - p) * image[y1 - <span class="number">1</span>, x1 - <span class="number">1</span>] + p * image[y1 - <span class="number">1</span>, x2 - <span class="number">1</span>]) + q * ((<span class="number">1</span> - p) * image[y2 - <span class="number">1</span>, x1 - <span class="number">1</span>] + p * image[y2 - <span class="number">1</span>, x2 - <span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> resized_image</span><br><span class="line"></span><br><span class="line"><span class="comment"># 双线性法插值实现图像旋转</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bilinear_rotation</span>(<span class="params">image, angle</span>):</span><br><span class="line">    h, w, channel = image.shape</span><br><span class="line">    angle_rad = math.radians(angle)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算旋转后的图像尺寸</span></span><br><span class="line">    cos_theta = <span class="built_in">abs</span>(math.cos(angle_rad))</span><br><span class="line">    sin_theta = <span class="built_in">abs</span>(math.sin(angle_rad))</span><br><span class="line">    new_w = <span class="built_in">int</span>(h * sin_theta + w * cos_theta)</span><br><span class="line">    new_h = <span class="built_in">int</span>(h * cos_theta + w * sin_theta)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 旋转中心</span></span><br><span class="line">    cx, cy = w / <span class="number">2</span>, h / <span class="number">2</span></span><br><span class="line">    new_cx, new_cy = new_w / <span class="number">2</span>, new_h / <span class="number">2</span></span><br><span class="line">    </span><br><span class="line">    rotated_image = np.zeros((new_h, new_w, channel), dtype=image.dtype)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(new_h):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(new_w):</span><br><span class="line">            <span class="comment"># 将新图像坐标转换回原图像坐标</span></span><br><span class="line">            x = (j - new_cx) * math.cos(angle_rad) + (i - new_cy) * math.sin(angle_rad) + cx</span><br><span class="line">            y = -(j - new_cx) * math.sin(angle_rad) + (i - new_cy) * math.cos(angle_rad) + cy</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 双线性插值</span></span><br><span class="line">            <span class="keyword">if</span> <span class="number">0</span> &lt;= x &lt; w-<span class="number">1</span> <span class="keyword">and</span> <span class="number">0</span> &lt;= y &lt; h-<span class="number">1</span>:</span><br><span class="line">                x1, y1 = <span class="built_in">int</span>(x), <span class="built_in">int</span>(y)</span><br><span class="line">                x2, y2 = <span class="built_in">min</span>(x1 + <span class="number">1</span>, w - <span class="number">1</span>), <span class="built_in">min</span>(y1 + <span class="number">1</span>, h - <span class="number">1</span>)</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 计算权重</span></span><br><span class="line">                a = x - x1</span><br><span class="line">                b = y - y1</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 边界处理</span></span><br><span class="line">                <span class="keyword">if</span> x2 &gt;= w:</span><br><span class="line">                    x2 = x1</span><br><span class="line">                <span class="keyword">if</span> y2 &gt;= h:</span><br><span class="line">                    y2 = y1</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 插值计算</span></span><br><span class="line">                rotated_image[i, j] = (<span class="number">1</span> - a) * (<span class="number">1</span> - b) * image[y1, x1] + \</span><br><span class="line">                                     a * (<span class="number">1</span> - b) * image[y1, x2] + \</span><br><span class="line">                                     (<span class="number">1</span> - a) * b * image[y2, x1] + \</span><br><span class="line">                                     a * b * image[y2, x2]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> rotated_image    </span><br></pre></td></tr></tbody></table></figure>

<h3 id="双立方法"><a href="#双立方法" class="headerlink" title="双立方法"></a>双立方法</h3><p>双立方法插值又称立方卷积插值/双三次插值，这也是数值分析中最常用的二维插值方法。在这种方法中，插值点 (x, y) 的像素颜色值 f(x, y) 通过矩形网格中最近的十六个采样点的加权平均得到，而各采样点的权重由该点到待求插值点的距离确定，此距离包括水平和竖直两个方向上的距离。相比之下，双线性插值仅由周围的四个采样点加权得到。</p>
<p><img src="/images/project2/10.png" alt="双立方法插值示意图"></p>
<p>如上图所示，设（转换至原图像中）待求插值点坐标为 (i+u, j+v)【i、j为整数部分，u、v为小数部分】，已知其周围的 16 个像素坐标点 (网格) 的颜色值，还需要计算 16 个点各自的权重。以像素坐标点 (i, j) 为例，因为该点在 y 轴和 x 轴方向上与待求插值点 (i+u, j+v) 的距离分别为 u 和 v，所以其权重为 w(u) × w(v)，其中 w(·) 是插值权重核 (可以理解为定义的权重函数)。同理可得其余 15 个像素坐标点各自的权重。那么，待求插值点 (i+u, j+v) 的颜色值 f(i+u, j+v) 将通过如下计算得到：<br>$$<br>f(i+u,j+v)=A\times B\times C<br>$$<br>其中各项由向量或矩阵表示为：<br>$$<br>\mathrm{A}=[w(1+u)w(u)w(1-u)w(2-u)]<br>$$</p>
<p>$$<br>\mathrm{B}=\begin{bmatrix}f(i-1,j-1)&amp;f(i-1,j+0)&amp;f(i-1,j+1)&amp;f(i-1,j+2)\f(i+0,j-1)&amp;f(i+0,j+0)&amp;f(i+0,j+1)&amp;f(i+0,j+2)\f(i+1,j-1)&amp;f(i+1,j+0)&amp;f(i+1,j+1)&amp;f(i+1,j+2)\f(i+2,j-1)&amp;f(i+2,j+0)&amp;f(i+2,j+1)&amp;f(i+2,j+2)\end{bmatrix}<br>$$</p>
<p>$$<br>\mathbb{C}=[w(1+v)w(v)w(1-v)w(2-v)]^T<br>$$</p>
<p>插值权重核 w(·) 为：<br>$$<br>w(x)=\begin{cases}1-2|x|^2+|x|^3&amp;,|x|&lt;1\4-8|x|+5|x|^2-|x|^3&amp;,1\leq|x|&lt;2\0&amp;,|x|\geq2&amp;\end{cases}<br>$$<br>插值权重核 w(·) 的函数图像：</p>
<p><img src="/images/project2/11.png" alt="双立方法插值权重核函数图像"></p>
<p>为方便后续算法实现，将以上加权求和过程各步骤展开，合并后化简得到待插入点的颜色值计算公式：<br>$$<br>f(i+u,j+v)=\sum_{m=0}^{3}\sum_{n=0}^{3}a_{mn}u^{m}v^{n}<br>$$<br>其中多项式的系数a_{mn}计算公式如下：(式中p <em>{qr}与上述矩阵B中元素一一对应，如p <em>00=f(i-1,j-1))<br>$$<br>\begin{aligned}<br>&amp;a</em>{00}=p</em>{11}\&amp;a_{01}=-\frac{1}{2}p_{10}+\frac{1}{2}p_{12}\&amp;a_{02}=p_{10}-\frac{5}{2}p_{11}+2p_{12}-\frac{1}{2}p_{13}\&amp;a_{03}=-\frac{1}{2}p_{10}+\frac{3}{2}p_{11}-\frac{3}{2}p_{12}+\frac{1}{2}p_{13}\&amp;a_{10}=-\frac{1}{2}p_{01}+\frac{1}{2}p_{21}\&amp;a_{11}=\frac{1}{4}p_{00}-\frac{1}{4}p_{02}-\frac{1}{4}p_{20}+\frac{1}{4}p_{22}\&amp;a_{12}=-\frac{1}{2}p_{00}+\frac{1}{4}p_{01}-p_{02}+\frac{1}{4}p_{03}+\frac{1}{2}p_{20}-\frac{5}{4}p_{21}+p_{22}-\frac{1}{4}p_{23}\&amp;a_{13}=\frac{1}{4}p_{00}-\frac{3}{4}p_{01}+\frac{3}{4}p_{02}-\frac{1}{4}p_{03}-\frac{1}{4}p_{20}+\frac{3}{4}p_{21}-\frac{3}{4}p_{22}+\frac{1}{4}p_{23}\<br>&amp;a_{20}=p_{01}-\frac{5}{2}p_{11}+2p_{21}-\frac{1}{2}p_{31}\<br>&amp;a_{21}=-\frac{1}{2}p_{00}+\frac{1}{2}p_{02}+\frac{5}{4}p_{10}-\frac{5}{4}p_{12}-p_{20}+p_{22}+\frac{1}{4}p_{30}-\frac{1}{4}p_{32}\&amp;a_{22}=p_{00}-\frac{5}{2}p_{01}+2p_{02}-\frac{1}{2}p_{03}-\frac{5}{2}p_{10}+\frac{25}{4}p_{11}-5p_{12}+\frac{5}{4}p_{13}+2p_{20}-5p_{21}+4p_{22}-p_{23}-\frac{1}{2}p_{30}+\frac{5}{4}p_{31}-p_{32}+\frac{1}{4}p_{33}\<br>&amp;a_{23}=-\frac{1}{2}p_{00}+\frac{3}{2}p_{01}-\frac{3}{2}p_{02}+\frac{1}{2}p_{03}+\frac{5}{4}p_{10}-\frac{15}{4}p_{11}+\frac{15}{4}p_{12}-\frac{5}{4}p_{13}-p_{20}+3p_{21}-3p_{22}+p_{23}+\frac{1}{4}p_{30}-\frac{3}{4}p_{31}+\frac{3}{4}p_{32}-\frac{1}{4}p_{33}\<br>&amp;a_{30}=-\frac{1}{2}p_{01}+\frac{3}{2}p_{11}-\frac{3}{2}p_{21}+\frac{1}{2}p_{31}\<br>&amp;a_{31}=\frac{1}{4}p_{00}-\frac{1}{4}p_{02}-\frac{3}{4}p_{10}+\frac{3}{4}p_{12}+\frac{3}{4}p_{20}-\frac{3}{4}p_{22}-\frac{1}{4}p_{30}+\frac{1}{4}p_{32}\&amp;a_{32}=-\frac{1}{2}p_{00}+\frac{5}{4}p_{01}-p_{02}+\frac{1}{4}p_{03}+\frac{3}{2}p_{10}-\frac{15}{4}p_{11}+3p_{12}-\frac{3}{4}p_{13}-\frac{3}{2}p_{20}+\frac{15}{4}p_{21}-3p_{22}+\frac{3}{4}p_{23}+\frac{1}{2}p_{30}-\frac{5}{4}p_{31}+p_{32}-\frac{1}{4}p_{33}\&amp;a_{33}=\frac{1}{4}p_{00}-\frac{3}{4}p_{01}+\frac{3}{4}p_{02}-\frac{1}{4}p_{03}-\frac{3}{4}p_{10}+\frac{9}{4}p_{11}-\frac{9}{4}p_{12}+\frac{3}{4}p_{13}+\frac{3}{4}p_{20}-\frac{9}{4}p_{21}+\frac{9}{4}p_{22}-\frac{3}{4}p_{23}-\frac{1}{4}p_{30}+\frac{3}{4}p_{31}-\frac{3}{4}p_{32}+\frac{1}{4}p_{33}<br>\end{aligned}<br>$$<br>基于以上算法思想，编写python函数代码实现图像放缩与旋转过程中的双立方法插值：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 双立方法插值实现图像放缩</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bicubic_interpolation</span>(<span class="params">image, scale_factor</span>):</span><br><span class="line">    h, w, channel = image.shape</span><br><span class="line">    new_h, new_w = <span class="built_in">int</span>(h * scale_factor), <span class="built_in">int</span>(w * scale_factor)</span><br><span class="line">    resized_image = np.zeros((new_h, new_w, channel))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(new_h):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(new_w):</span><br><span class="line">            x = i / scale_factor</span><br><span class="line">            y = j / scale_factor</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 确定16个邻域像素的坐标</span></span><br><span class="line">            x0 = <span class="built_in">max</span>(<span class="number">0</span>, <span class="built_in">int</span>(np.floor(x)) - <span class="number">1</span>)</span><br><span class="line">            x1 = x0 + <span class="number">1</span></span><br><span class="line">            x2 = x0 + <span class="number">2</span></span><br><span class="line">            x3 = <span class="built_in">min</span>(w-<span class="number">1</span>, x0 + <span class="number">3</span>)</span><br><span class="line">            </span><br><span class="line">            y0 = <span class="built_in">max</span>(<span class="number">0</span>, <span class="built_in">int</span>(np.floor(y)) - <span class="number">1</span>)</span><br><span class="line">            y1 = y0 + <span class="number">1</span></span><br><span class="line">            y2 = y0 + <span class="number">2</span></span><br><span class="line">            y3 = <span class="built_in">min</span>(h-<span class="number">1</span>, y0 + <span class="number">3</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 获取16个邻域像素的值</span></span><br><span class="line">            p = np.zeros((<span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>))</span><br><span class="line">            <span class="keyword">for</span> m <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                    xi = x0 + n</span><br><span class="line">                    yi = y0 + m</span><br><span class="line">                    xi = <span class="built_in">min</span>(<span class="built_in">max</span>(xi, <span class="number">0</span>), w-<span class="number">1</span>)  <span class="comment"># 边界处理</span></span><br><span class="line">                    yi = <span class="built_in">min</span>(<span class="built_in">max</span>(yi, <span class="number">0</span>), h-<span class="number">1</span>)</span><br><span class="line">                    p[m, n] = image[yi, xi]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 计算相对位置</span></span><br><span class="line">            dx = x - x1</span><br><span class="line">            dy = y - y1</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 系数</span></span><br><span class="line">            a = np.zeros((<span class="number">4</span>, <span class="number">4</span>, channel))</span><br><span class="line">            a[<span class="number">0</span>, <span class="number">0</span>] = p[<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">            a[<span class="number">0</span>, <span class="number">1</span>] = -<span class="number">0.5</span>*p[<span class="number">1</span>, <span class="number">0</span>] + <span class="number">0.5</span>*p[<span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">            a[<span class="number">0</span>, <span class="number">2</span>] = p[<span class="number">1</span>, <span class="number">0</span>] - <span class="number">2.5</span>*p[<span class="number">1</span>, <span class="number">1</span>] + <span class="number">2</span>*p[<span class="number">1</span>, <span class="number">2</span>] - <span class="number">0.5</span>*p[<span class="number">1</span>, <span class="number">3</span>]</span><br><span class="line">            a[<span class="number">0</span>, <span class="number">3</span>] = -<span class="number">0.5</span>*p[<span class="number">1</span>, <span class="number">0</span>] + <span class="number">1.5</span>*p[<span class="number">1</span>, <span class="number">1</span>] - <span class="number">1.5</span>*p[<span class="number">1</span>, <span class="number">2</span>] + <span class="number">0.5</span>*p[<span class="number">1</span>, <span class="number">3</span>]</span><br><span class="line">            </span><br><span class="line">            a[<span class="number">1</span>, <span class="number">0</span>] = -<span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">1</span>] + <span class="number">0.5</span>*p[<span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">            a[<span class="number">1</span>, <span class="number">1</span>] = <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">0</span>] - <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">2</span>] - <span class="number">0.25</span>*p[<span class="number">2</span>, <span class="number">0</span>] + <span class="number">0.25</span>*p[<span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line">            a[<span class="number">1</span>, <span class="number">2</span>] = -<span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">0</span>] + <span class="number">1.25</span>*p[<span class="number">0</span>, <span class="number">1</span>] - p[<span class="number">0</span>, <span class="number">2</span>] + <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">3</span>] + <span class="number">0.5</span>*p[<span class="number">2</span>, <span class="number">0</span>] - <span class="number">1.25</span>*p[<span class="number">2</span>, <span class="number">1</span>] + p[<span class="number">2</span>, <span class="number">2</span>] - <span class="number">0.25</span>*p[<span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">            a[<span class="number">1</span>, <span class="number">3</span>] = <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">0</span>] - <span class="number">0.75</span>*p[<span class="number">0</span>, <span class="number">1</span>] + <span class="number">0.75</span>*p[<span class="number">0</span>, <span class="number">2</span>] - <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">3</span>] - <span class="number">0.25</span>*p[<span class="number">2</span>, <span class="number">0</span>] + <span class="number">0.75</span>*p[<span class="number">2</span>, <span class="number">1</span>] - <span class="number">0.75</span>*p[<span class="number">2</span>, <span class="number">2</span>] + <span class="number">0.25</span>*p[<span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">            </span><br><span class="line">            a[<span class="number">2</span>, <span class="number">0</span>] = p[<span class="number">0</span>, <span class="number">1</span>] - <span class="number">2.5</span>*p[<span class="number">1</span>, <span class="number">1</span>] + <span class="number">2</span>*p[<span class="number">2</span>, <span class="number">1</span>] - <span class="number">0.5</span>*p[<span class="number">3</span>, <span class="number">1</span>]</span><br><span class="line">            a[<span class="number">2</span>, <span class="number">1</span>] = -<span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">0</span>] + <span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">2</span>] + <span class="number">1.25</span>*p[<span class="number">1</span>, <span class="number">0</span>] - <span class="number">1.25</span>*p[<span class="number">1</span>, <span class="number">2</span>] - p[<span class="number">2</span>, <span class="number">0</span>] + p[<span class="number">2</span>, <span class="number">2</span>] + <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">0</span>] - <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">2</span>]</span><br><span class="line">            a[<span class="number">2</span>, <span class="number">2</span>] = p[<span class="number">0</span>, <span class="number">0</span>] - <span class="number">2.5</span>*p[<span class="number">0</span>, <span class="number">1</span>] + <span class="number">2</span>*p[<span class="number">0</span>, <span class="number">2</span>] - <span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">3</span>] - <span class="number">2.5</span>*p[<span class="number">1</span>, <span class="number">0</span>] + <span class="number">6.25</span>*p[<span class="number">1</span>, <span class="number">1</span>] - <span class="number">5</span>*p[<span class="number">1</span>, <span class="number">2</span>] + <span class="number">1.25</span>*p[<span class="number">1</span>, <span class="number">3</span>] + <span class="number">2</span>*p[<span class="number">2</span>, <span class="number">0</span>] - <span class="number">5</span>*p[<span class="number">2</span>, <span class="number">1</span>] + <span class="number">4</span>*p[<span class="number">2</span>, <span class="number">2</span>] - p[<span class="number">2</span>, <span class="number">3</span>] - <span class="number">0.5</span>*p[<span class="number">3</span>, <span class="number">0</span>] + <span class="number">1.25</span>*p[<span class="number">3</span>, <span class="number">1</span>] - p[<span class="number">3</span>, <span class="number">2</span>] + <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">3</span>]</span><br><span class="line">            a[<span class="number">2</span>, <span class="number">3</span>] = -<span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">0</span>] + <span class="number">1.5</span>*p[<span class="number">0</span>, <span class="number">1</span>] - <span class="number">1.5</span>*p[<span class="number">0</span>, <span class="number">2</span>] + <span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">3</span>] + <span class="number">1.25</span>*p[<span class="number">1</span>, <span class="number">0</span>] - <span class="number">3.75</span>*p[<span class="number">1</span>, <span class="number">1</span>] + <span class="number">3.75</span>*p[<span class="number">1</span>, <span class="number">2</span>] - <span class="number">1.25</span>*p[<span class="number">1</span>, <span class="number">3</span>] - p[<span class="number">2</span>, <span class="number">0</span>] + <span class="number">3</span>*p[<span class="number">2</span>, <span class="number">1</span>] - <span class="number">3</span>*p[<span class="number">2</span>, <span class="number">2</span>] + p[<span class="number">2</span>, <span class="number">3</span>] + <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">0</span>] - <span class="number">0.75</span>*p[<span class="number">3</span>, <span class="number">1</span>] + <span class="number">0.75</span>*p[<span class="number">3</span>, <span class="number">2</span>] - <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">3</span>]</span><br><span class="line">            </span><br><span class="line">            a[<span class="number">3</span>, <span class="number">0</span>] = -<span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">1</span>] + <span class="number">1.5</span>*p[<span class="number">1</span>, <span class="number">1</span>] - <span class="number">1.5</span>*p[<span class="number">2</span>, <span class="number">1</span>] + <span class="number">0.5</span>*p[<span class="number">3</span>, <span class="number">1</span>]</span><br><span class="line">            a[<span class="number">3</span>, <span class="number">1</span>] = <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">0</span>] - <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">2</span>] - <span class="number">0.75</span>*p[<span class="number">1</span>, <span class="number">0</span>] + <span class="number">0.75</span>*p[<span class="number">1</span>, <span class="number">2</span>] + <span class="number">0.75</span>*p[<span class="number">2</span>, <span class="number">0</span>] - <span class="number">0.75</span>*p[<span class="number">2</span>, <span class="number">2</span>] - <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">0</span>] + <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">2</span>]</span><br><span class="line">            a[<span class="number">3</span>, <span class="number">2</span>] = -<span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">0</span>] + <span class="number">1.25</span>*p[<span class="number">0</span>, <span class="number">1</span>] - p[<span class="number">0</span>, <span class="number">2</span>] + <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">3</span>] + <span class="number">1.5</span>*p[<span class="number">1</span>, <span class="number">0</span>] - <span class="number">3.75</span>*p[<span class="number">1</span>, <span class="number">1</span>] + <span class="number">3</span>*p[<span class="number">1</span>, <span class="number">2</span>] - <span class="number">0.75</span>*p[<span class="number">1</span>, <span class="number">3</span>] - <span class="number">1.5</span>*p[<span class="number">2</span>, <span class="number">0</span>] + <span class="number">3.75</span>*p[<span class="number">2</span>, <span class="number">1</span>] - <span class="number">3</span>*p[<span class="number">2</span>, <span class="number">2</span>] + <span class="number">0.75</span>*p[<span class="number">2</span>, <span class="number">3</span>] + <span class="number">0.5</span>*p[<span class="number">3</span>, <span class="number">0</span>] - <span class="number">1.25</span>*p[<span class="number">3</span>, <span class="number">1</span>] + p[<span class="number">3</span>, <span class="number">2</span>] - <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">3</span>]</span><br><span class="line">            a[<span class="number">3</span>, <span class="number">3</span>] = <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">0</span>] - <span class="number">0.75</span>*p[<span class="number">0</span>, <span class="number">1</span>] + <span class="number">0.75</span>*p[<span class="number">0</span>, <span class="number">2</span>] - <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">3</span>] - <span class="number">0.75</span>*p[<span class="number">1</span>, <span class="number">0</span>] + <span class="number">2.25</span>*p[<span class="number">1</span>, <span class="number">1</span>] - <span class="number">2.25</span>*p[<span class="number">1</span>, <span class="number">2</span>] + <span class="number">0.75</span>*p[<span class="number">1</span>, <span class="number">3</span>] + <span class="number">0.75</span>*p[<span class="number">2</span>, <span class="number">0</span>] - <span class="number">2.25</span>*p[<span class="number">2</span>, <span class="number">1</span>] + <span class="number">2.25</span>*p[<span class="number">2</span>, <span class="number">2</span>] - <span class="number">0.75</span>*p[<span class="number">2</span>, <span class="number">3</span>] - <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">0</span>] + <span class="number">0.75</span>*p[<span class="number">3</span>, <span class="number">1</span>] - <span class="number">0.75</span>*p[<span class="number">3</span>, <span class="number">2</span>] + <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">3</span>]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 计算插值结果</span></span><br><span class="line">            value = np.zeros(channel)</span><br><span class="line">            <span class="keyword">for</span> m <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                    value += a[m, n] * (dx**n) * (dy**m)</span><br><span class="line">            </span><br><span class="line">            resized_image[i, j] = np.clip(value, <span class="number">0</span>, <span class="number">255</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> resized_image.astype(np.uint8)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 双立方法插值实现图像旋转</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bicubic_rotation</span>(<span class="params">image, angle</span>):</span><br><span class="line">    h, w, channel = image.shape</span><br><span class="line">    angle_rad = math.radians(angle)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算旋转后的图像尺寸</span></span><br><span class="line">    cos_theta = <span class="built_in">abs</span>(math.cos(angle_rad))</span><br><span class="line">    sin_theta = <span class="built_in">abs</span>(math.sin(angle_rad))</span><br><span class="line">    new_w = <span class="built_in">int</span>(h * sin_theta + w * cos_theta)</span><br><span class="line">    new_h = <span class="built_in">int</span>(h * cos_theta + w * sin_theta)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 旋转中心</span></span><br><span class="line">    cx, cy = w / <span class="number">2</span>, h / <span class="number">2</span></span><br><span class="line">    new_cx, new_cy = new_w / <span class="number">2</span>, new_h / <span class="number">2</span></span><br><span class="line">    </span><br><span class="line">    rotated_image = np.zeros((new_h, new_w, channel))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(new_h):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(new_w):</span><br><span class="line">            <span class="comment"># 将新图像坐标转换回原图像坐标</span></span><br><span class="line">            x = (j - new_cx) * math.cos(angle_rad) + (i - new_cy) * math.sin(angle_rad) + cx</span><br><span class="line">            y = -(j - new_cx) * math.sin(angle_rad) + (i - new_cy) * math.cos(angle_rad) + cy</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> <span class="number">0</span> &lt;= x &lt; w <span class="keyword">and</span> <span class="number">0</span> &lt;= y &lt; h:</span><br><span class="line">                <span class="comment"># 确定16个邻域像素的坐标</span></span><br><span class="line">                x0 = <span class="built_in">max</span>(<span class="number">0</span>, <span class="built_in">int</span>(np.floor(x)) - <span class="number">1</span>)</span><br><span class="line">                x1 = x0 + <span class="number">1</span></span><br><span class="line">                x2 = x0 + <span class="number">2</span></span><br><span class="line">                x3 = <span class="built_in">min</span>(w-<span class="number">1</span>, x0 + <span class="number">3</span>)</span><br><span class="line">                </span><br><span class="line">                y0 = <span class="built_in">max</span>(<span class="number">0</span>, <span class="built_in">int</span>(np.floor(y)) - <span class="number">1</span>)</span><br><span class="line">                y1 = y0 + <span class="number">1</span></span><br><span class="line">                y2 = y0 + <span class="number">2</span></span><br><span class="line">                y3 = <span class="built_in">min</span>(h-<span class="number">1</span>, y0 + <span class="number">3</span>)</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 获取16个邻域像素的值</span></span><br><span class="line">                p = np.zeros((<span class="number">4</span>, <span class="number">4</span>, channel))</span><br><span class="line">                <span class="keyword">for</span> m <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                        xi = x0 + n</span><br><span class="line">                        yi = y0 + m</span><br><span class="line">                        xi = <span class="built_in">min</span>(<span class="built_in">max</span>(xi, <span class="number">0</span>), w-<span class="number">1</span>)  <span class="comment"># 边界处理</span></span><br><span class="line">                        yi = <span class="built_in">min</span>(<span class="built_in">max</span>(yi, <span class="number">0</span>), h-<span class="number">1</span>)</span><br><span class="line">                        p[m, n] = image[yi, xi]</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 计算相对位置</span></span><br><span class="line">                dx = x - x1</span><br><span class="line">                dy = y - y1</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 系数</span></span><br><span class="line">                a = np.zeros((<span class="number">4</span>, <span class="number">4</span>, channel))</span><br><span class="line">                a[<span class="number">0</span>, <span class="number">0</span>] = p[<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">                a[<span class="number">0</span>, <span class="number">1</span>] = -<span class="number">0.5</span>*p[<span class="number">1</span>, <span class="number">0</span>] + <span class="number">0.5</span>*p[<span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">                a[<span class="number">0</span>, <span class="number">2</span>] = p[<span class="number">1</span>, <span class="number">0</span>] - <span class="number">2.5</span>*p[<span class="number">1</span>, <span class="number">1</span>] + <span class="number">2</span>*p[<span class="number">1</span>, <span class="number">2</span>] - <span class="number">0.5</span>*p[<span class="number">1</span>, <span class="number">3</span>]</span><br><span class="line">                a[<span class="number">0</span>, <span class="number">3</span>] = -<span class="number">0.5</span>*p[<span class="number">1</span>, <span class="number">0</span>] + <span class="number">1.5</span>*p[<span class="number">1</span>, <span class="number">1</span>] - <span class="number">1.5</span>*p[<span class="number">1</span>, <span class="number">2</span>] + <span class="number">0.5</span>*p[<span class="number">1</span>, <span class="number">3</span>]</span><br><span class="line">                </span><br><span class="line">                a[<span class="number">1</span>, <span class="number">0</span>] = -<span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">1</span>] + <span class="number">0.5</span>*p[<span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">                a[<span class="number">1</span>, <span class="number">1</span>] = <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">0</span>] - <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">2</span>] - <span class="number">0.25</span>*p[<span class="number">2</span>, <span class="number">0</span>] + <span class="number">0.25</span>*p[<span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line">                a[<span class="number">1</span>, <span class="number">2</span>] = -<span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">0</span>] + <span class="number">1.25</span>*p[<span class="number">0</span>, <span class="number">1</span>] - p[<span class="number">0</span>, <span class="number">2</span>] + <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">3</span>] + <span class="number">0.5</span>*p[<span class="number">2</span>, <span class="number">0</span>] - <span class="number">1.25</span>*p[<span class="number">2</span>, <span class="number">1</span>] + p[<span class="number">2</span>, <span class="number">2</span>] - <span class="number">0.25</span>*p[<span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">                a[<span class="number">1</span>, <span class="number">3</span>] = <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">0</span>] - <span class="number">0.75</span>*p[<span class="number">0</span>, <span class="number">1</span>] + <span class="number">0.75</span>*p[<span class="number">0</span>, <span class="number">2</span>] - <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">3</span>] - <span class="number">0.25</span>*p[<span class="number">2</span>, <span class="number">0</span>] + <span class="number">0.75</span>*p[<span class="number">2</span>, <span class="number">1</span>] - <span class="number">0.75</span>*p[<span class="number">2</span>, <span class="number">2</span>] + <span class="number">0.25</span>*p[<span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">                </span><br><span class="line">                a[<span class="number">2</span>, <span class="number">0</span>] = p[<span class="number">0</span>, <span class="number">1</span>] - <span class="number">2.5</span>*p[<span class="number">1</span>, <span class="number">1</span>] + <span class="number">2</span>*p[<span class="number">2</span>, <span class="number">1</span>] - <span class="number">0.5</span>*p[<span class="number">3</span>, <span class="number">1</span>]</span><br><span class="line">                a[<span class="number">2</span>, <span class="number">1</span>] = -<span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">0</span>] + <span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">2</span>] + <span class="number">1.25</span>*p[<span class="number">1</span>, <span class="number">0</span>] - <span class="number">1.25</span>*p[<span class="number">1</span>, <span class="number">2</span>] - p[<span class="number">2</span>, <span class="number">0</span>] + p[<span class="number">2</span>, <span class="number">2</span>] + <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">0</span>] - <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">2</span>]</span><br><span class="line">                a[<span class="number">2</span>, <span class="number">2</span>] = p[<span class="number">0</span>, <span class="number">0</span>] - <span class="number">2.5</span>*p[<span class="number">0</span>, <span class="number">1</span>] + <span class="number">2</span>*p[<span class="number">0</span>, <span class="number">2</span>] - <span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">3</span>] - <span class="number">2.5</span>*p[<span class="number">1</span>, <span class="number">0</span>] + <span class="number">6.25</span>*p[<span class="number">1</span>, <span class="number">1</span>] - <span class="number">5</span>*p[<span class="number">1</span>, <span class="number">2</span>] + <span class="number">1.25</span>*p[<span class="number">1</span>, <span class="number">3</span>] + <span class="number">2</span>*p[<span class="number">2</span>, <span class="number">0</span>] - <span class="number">5</span>*p[<span class="number">2</span>, <span class="number">1</span>] + <span class="number">4</span>*p[<span class="number">2</span>, <span class="number">2</span>] - p[<span class="number">2</span>, <span class="number">3</span>] - <span class="number">0.5</span>*p[<span class="number">3</span>, <span class="number">0</span>] + <span class="number">1.25</span>*p[<span class="number">3</span>, <span class="number">1</span>] - p[<span class="number">3</span>, <span class="number">2</span>] + <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">3</span>]</span><br><span class="line">                a[<span class="number">2</span>, <span class="number">3</span>] = -<span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">0</span>] + <span class="number">1.5</span>*p[<span class="number">0</span>, <span class="number">1</span>] - <span class="number">1.5</span>*p[<span class="number">0</span>, <span class="number">2</span>] + <span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">3</span>] + <span class="number">1.25</span>*p[<span class="number">1</span>, <span class="number">0</span>] - <span class="number">3.75</span>*p[<span class="number">1</span>, <span class="number">1</span>] + <span class="number">3.75</span>*p[<span class="number">1</span>, <span class="number">2</span>] - <span class="number">1.25</span>*p[<span class="number">1</span>, <span class="number">3</span>] - p[<span class="number">2</span>, <span class="number">0</span>] + <span class="number">3</span>*p[<span class="number">2</span>, <span class="number">1</span>] - <span class="number">3</span>*p[<span class="number">2</span>, <span class="number">2</span>] + p[<span class="number">2</span>, <span class="number">3</span>] + <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">0</span>] - <span class="number">0.75</span>*p[<span class="number">3</span>, <span class="number">1</span>] + <span class="number">0.75</span>*p[<span class="number">3</span>, <span class="number">2</span>] - <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">3</span>]</span><br><span class="line">                </span><br><span class="line">                a[<span class="number">3</span>, <span class="number">0</span>] = -<span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">1</span>] + <span class="number">1.5</span>*p[<span class="number">1</span>, <span class="number">1</span>] - <span class="number">1.5</span>*p[<span class="number">2</span>, <span class="number">1</span>] + <span class="number">0.5</span>*p[<span class="number">3</span>, <span class="number">1</span>]</span><br><span class="line">                a[<span class="number">3</span>, <span class="number">1</span>] = <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">0</span>] - <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">2</span>] - <span class="number">0.75</span>*p[<span class="number">1</span>, <span class="number">0</span>] + <span class="number">0.75</span>*p[<span class="number">1</span>, <span class="number">2</span>] + <span class="number">0.75</span>*p[<span class="number">2</span>, <span class="number">0</span>] - <span class="number">0.75</span>*p[<span class="number">2</span>, <span class="number">2</span>] - <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">0</span>] + <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">2</span>]</span><br><span class="line">                a[<span class="number">3</span>, <span class="number">2</span>] = -<span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">0</span>] + <span class="number">1.25</span>*p[<span class="number">0</span>, <span class="number">1</span>] - p[<span class="number">0</span>, <span class="number">2</span>] + <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">3</span>] + <span class="number">1.5</span>*p[<span class="number">1</span>, <span class="number">0</span>] - <span class="number">3.75</span>*p[<span class="number">1</span>, <span class="number">1</span>] + <span class="number">3</span>*p[<span class="number">1</span>, <span class="number">2</span>] - <span class="number">0.75</span>*p[<span class="number">1</span>, <span class="number">3</span>] - <span class="number">1.5</span>*p[<span class="number">2</span>, <span class="number">0</span>] + <span class="number">3.75</span>*p[<span class="number">2</span>, <span class="number">1</span>] - <span class="number">3</span>*p[<span class="number">2</span>, <span class="number">2</span>] + <span class="number">0.75</span>*p[<span class="number">2</span>, <span class="number">3</span>] + <span class="number">0.5</span>*p[<span class="number">3</span>, <span class="number">0</span>] - <span class="number">1.25</span>*p[<span class="number">3</span>, <span class="number">1</span>] + p[<span class="number">3</span>, <span class="number">2</span>] - <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">3</span>]</span><br><span class="line">                a[<span class="number">3</span>, <span class="number">3</span>] = <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">0</span>] - <span class="number">0.75</span>*p[<span class="number">0</span>, <span class="number">1</span>] + <span class="number">0.75</span>*p[<span class="number">0</span>, <span class="number">2</span>] - <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">3</span>] - <span class="number">0.75</span>*p[<span class="number">1</span>, <span class="number">0</span>] + <span class="number">2.25</span>*p[<span class="number">1</span>, <span class="number">1</span>] - <span class="number">2.25</span>*p[<span class="number">1</span>, <span class="number">2</span>] + <span class="number">0.75</span>*p[<span class="number">1</span>, <span class="number">3</span>] + <span class="number">0.75</span>*p[<span class="number">2</span>, <span class="number">0</span>] - <span class="number">2.25</span>*p[<span class="number">2</span>, <span class="number">1</span>] + <span class="number">2.25</span>*p[<span class="number">2</span>, <span class="number">2</span>] - <span class="number">0.75</span>*p[<span class="number">2</span>, <span class="number">3</span>] - <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">0</span>] + <span class="number">0.75</span>*p[<span class="number">3</span>, <span class="number">1</span>] - <span class="number">0.75</span>*p[<span class="number">3</span>, <span class="number">2</span>] + <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">3</span>]</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 计算插值结果</span></span><br><span class="line">                value = np.zeros(channel)</span><br><span class="line">                <span class="keyword">for</span> m <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                        value += a[m, n] * (dx**n) * (dy**m)</span><br><span class="line">                </span><br><span class="line">                rotated_image[i, j] = np.clip(value, <span class="number">0</span>, <span class="number">255</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> rotated_image.astype(np.uint8)</span><br></pre></td></tr></tbody></table></figure>

<h2 id="图像插值算法优化：基于四平面"><a href="#图像插值算法优化：基于四平面" class="headerlink" title="图像插值算法优化：基于四平面"></a>图像插值算法优化：基于四平面</h2><p>在上述的多种基于分段插值的图像插值算法中，均采用f(i, j)来表示图像的像素点坐标处的颜色值，其中ｉ表示行坐标，ｊ表示列坐标。为进一步地体现图像的局部特征差异并将其用于插值过程，我们引入“平面”的概念，并对图像数据进行升维处理，用三维空间点(i, j, f(i, j))来表示一个像素，并将其对应至空间坐标系中的一个点(x, y, z)。</p>
<p>对一个待插入点而言，可以通过坐标平移将其周围4 个像素点转换为：（注意：此处z0<del>z3为像素坐标点s0</del>s3的颜色值，下同）<br>$$<br>s_0(0,0,z_0),s_1(0,1,z_1),s_2(1,0,z_2),s_3(1,1,z_3)<br>$$<br>从上述４个点的坐标可以看出它们任意３个点一定不在同一条直线上， 不在同一直线上的３个点可以确定一个平面， 下面讨论具体的插值方法：</p>
<ol>
<li><p>先求出这４个点可能的４个平面方程</p>
<p>已知空间平面的一般方程为：<br>$$<br>Ax+By+Cz+D=0<br>$$<br>将s0、s1、s2分别带入上式可得：<br>$$<br>\begin{cases}Cz_0+D=0\B+Cz_1+D=0\A+Cz_2+D=0&amp;\end{cases}<br>$$<br>则有：<br>$$<br>D=-Cz_0,B=C(z_0-z_1),A=C(z_0-z_2)<br>$$<br>再将其带回空间平面方程，整理后用f(x, y)代替z得到插值公式：<br>$$<br>f(x,y)=(z_{2}-z_{0})x+(z_{1}-z_{2})y+z_{0}<br>$$<br>同理，将s0、s1、s3带入空间平面方程可得插值公式：<br>$$<br>f(x,y)=(z_{3}-z_{1})x+(z_{1}-z_{0})y+z_{0}<br>$$<br>将s0、s2、s3带入空间平面方程可得插值公式：<br>$$<br>f(x,y)=(z_{2}-z_{0})x+(z_{3}-z_{2})y+z_{0}<br>$$<br>将s1、s2、s3带入空间平面方程可得插值公式：<br>$$<br>f(x,y)=(z_{3}-z_{1})x+(z_{3}-z_{2})y+(z_{2}+z_{1}-z_{3})<br>$$</p>
</li>
<li><p>如果s0、s1、s2、s3这４ 个点在同一平面上， 则使用上述任意一个插值公式进行插值均可。 【平面法】</p>
<blockquote>
<p>判断这４个点是否在同一平面上， 只需要比较z1+z2 与 z0+z3是否相等：</p>
<p>线段s0s3中点坐标为<br>$$<br>(\frac12,\frac12,\frac{z_0+z_3}2)<br>$$<br>线段s1s2中点坐标为<br>$$<br>(\frac12,\frac12,\frac{z_1+z_2}2)<br>$$<br>如果它们的中点坐标相同，则说明两条线段相交，相交的两条直线可以决定一个平面，即如果待插人点周围的四个点满足：<br>$$<br>z_1+z_2=z_0+z_3<br>$$<br>则这它们就是同一平面上的 4 个点，否则就不是同一平面上的 4 个点。</p>
</blockquote>
</li>
<li><p>从４个可能的平面中选择一个平面进行插值【四平面法】</p>
<p>如果它们不是同一平面上的４个点， 情况比较复杂， 需认真讨论，s0、s1、s2、s3４个点的位置关系如下图所示：</p>
<p><img src="/images/project2/12.png" alt="四点不在同一平面"></p>
<p>在插值的过程中如果一半的区域选择由s0、s1、s2 所确定的平面进行插值， 则另一半必须选择由s1、s2、s3所确定的平面进行插值， 以保证对角线的每一边都是在同一个平面上， 避免出现 “锯齿形” 边缘，为了便于描述， 称s0、s1、s2所确定的平面为 “左下平面”，s1、s2、s3 所确定的平面为 “右上平面”，s0、s1、s3 所确定的平面 “左上平面”，s0、s2、s3所确定的平面 “右下平面”。 为此， 需要参考周围其他点的情况以决定选择哪个平面进行插值。 具体情况如下图所示（黑点是待插入点周围的４个点，白点是参考点）：</p>
<p><img src="/images/project2/13.png" alt="待插入点周围的像素点"></p>
<ol>
<li>对于“左下平面”， 只能参考s0、s1、s2三点左面和下面的点， 即s0、s1、s2三点与s4、s5、s6、s8四个点中的任意一点在同一平面上即可。 </li>
<li>对于“右上平面”，只能参考s1、s2、s3三点右面和上面的点， 即s1、s2、s3三点与s7、s9、s10、s11四个点中的任意一点在同一平面上即可。</li>
<li>对于 “左上平面”，只能参考s0、s1、s3三点左面和上面的点， 即s0、s1、s3三点与s6、s8、s10、s11四个点中的任意一点在同一平面上即可。 </li>
<li>对于 “右下平面”，只能参考s0、s2、s3三点右面和下面的点， 即s0、s2、s3三点与s4、s5、s7、s9四 个点中的任意一点在同一平面上即可。</li>
</ol>
<p>针对1、2两种情况， 当y = 1 + x时，用 “左下平面” 进行插值， 否则用 “右上平面” 进行插值；针对3、4两种情况， 当y = x ^ 3时，用 “左上平面” 进行插值， 否则用 “右下平面” 进行插值。</p>
<blockquote>
<p>判断４个点在同一平面上的方法：（以情况1为例）</p>
<ul>
<li><p>对于判断s0、s2、s1、s8 ４ 点是否在同一平面上， 只需要判断z0 + z1与z2 + z8是否相等即可； </p>
</li>
<li><p>对于s0、s1、s2、s5 ４点， 只需要判断z0 + z2与z1 + z5是否相等即可； </p>
</li>
<li><p>对于s0、s1、s2、s6 ４点：如果s0、s2、s6 ３点在同一直线上， 则直线外一点s1与该直线就可以确定一个平面，而要判断这三点是否在同一直线上，只需判断z2 + z6与2 * z0是否相等即可【线段s2(1, 0, z2) s6(-1, 0, z6) 的中点坐标为(0, 0, z2 + z6)，若z2 + z6 = 2 * z0， 则点s0(0, 0, z0)就是它们的中点坐标，当然这３点就在同一条直线上】； </p>
</li>
<li><p>对于s0、s1、s2、s4   ４点， 与s0、s1、s2、s6 ４ 点的情况相同。</p>
</li>
</ul>
</blockquote>
</li>
<li><p>如果2和3两点中的情形均不满足， 说明待插入点周围的情况太复杂（不符合平面插值）， 此时采用<strong>双线性法</strong>进行插值。</p>
</li>
</ol>
<p>基于以上算法思想，编写python函数代码实现图像放缩与旋转过程中的四平面法插值：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 四平面法插值实现图像放缩</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">four_plane_interpolation</span>(<span class="params">img, scale</span>):</span><br><span class="line">    H, W, C = img.shape</span><br><span class="line">    new_H, new_W = <span class="built_in">int</span>(H * scale), <span class="built_in">int</span>(W * scale)</span><br><span class="line">    output = np.zeros((new_H, new_W, C), dtype=img.dtype)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(new_H):</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(new_W):</span><br><span class="line">            <span class="comment"># 计算原图对应坐标（浮点数）</span></span><br><span class="line">            src_x = x / scale</span><br><span class="line">            src_y = y / scale</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 获取周围4个整数坐标点</span></span><br><span class="line">            x0, y0 = <span class="built_in">int</span>(np.floor(src_x)), <span class="built_in">int</span>(np.floor(src_y))</span><br><span class="line">            x1, y1 = <span class="built_in">min</span>(x0 + <span class="number">1</span>, W - <span class="number">1</span>), <span class="built_in">min</span>(y0 + <span class="number">1</span>, H - <span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 获取4个点的颜色值（z坐标）</span></span><br><span class="line">            s0 = img[y0, x0]</span><br><span class="line">            s1 = img[y0, x1]</span><br><span class="line">            s2 = img[y1, x0]</span><br><span class="line">            s3 = img[y1, x1]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 计算相对位置（归一化到[0,1]）</span></span><br><span class="line">            dx = src_x - x0</span><br><span class="line">            dy = src_y - y0</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 判断四点是否共面（z1 + z2 ≈ z0 + z3）</span></span><br><span class="line">            <span class="keyword">if</span> np.allclose(s1 + s2, s0 + s3, atol=<span class="number">1e-6</span>):</span><br><span class="line">                <span class="comment"># 共面时，选择任意平面（此处用左下平面）</span></span><br><span class="line">                interpolated = s0 + (s2 - s0) * dx + (s1 - s0) * dy</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 不共面时，动态选择平面</span></span><br><span class="line">                <span class="comment"># 获取周围12个参考点（简化实现，仅取最近邻）</span></span><br><span class="line">                <span class="comment"># 注：论文中需判断参考点是否共面，此处简化逻辑</span></span><br><span class="line">                <span class="keyword">if</span> dy &gt; <span class="number">1</span> - dx:  <span class="comment"># 对角线 y = 1 - x 上方</span></span><br><span class="line">                    <span class="comment"># 选择右上平面</span></span><br><span class="line">                    interpolated = (s3 - s1) * dx + (s3 - s2) * dy + (s2 + s1 - s3)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="comment"># 选择左下平面</span></span><br><span class="line">                    interpolated = s0 + (s2 - s0) * dx + (s1 - s0) * dy</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 边界检查</span></span><br><span class="line">            interpolated = np.clip(interpolated, <span class="number">0</span>, <span class="number">255</span> <span class="keyword">if</span> img.dtype == np.uint8 <span class="keyword">else</span> <span class="number">1.0</span>)</span><br><span class="line">            output[y, x] = interpolated</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 四平面法插值实现图像旋转</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">four_plane_rotation</span>(<span class="params">image, angle</span>):</span><br><span class="line">    h, w, channel = image.shape</span><br><span class="line">    angle_rad = math.radians(angle)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算旋转后的图像尺寸</span></span><br><span class="line">    cos_theta = <span class="built_in">abs</span>(math.cos(angle_rad))</span><br><span class="line">    sin_theta = <span class="built_in">abs</span>(math.sin(angle_rad))</span><br><span class="line">    new_w = <span class="built_in">int</span>(h * sin_theta + w * cos_theta)</span><br><span class="line">    new_h = <span class="built_in">int</span>(h * cos_theta + w * sin_theta)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 旋转中心</span></span><br><span class="line">    cx, cy = w / <span class="number">2</span>, h / <span class="number">2</span></span><br><span class="line">    new_cx, new_cy = new_w / <span class="number">2</span>, new_h / <span class="number">2</span></span><br><span class="line">    </span><br><span class="line">    rotated_image = np.zeros((new_h, new_w, channel), dtype=image.dtype)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(new_h):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(new_w):</span><br><span class="line">            <span class="comment"># 将新图像坐标转换回原图像坐标</span></span><br><span class="line">            x = (j - new_cx) * math.cos(angle_rad) + (i - new_cy) * math.sin(angle_rad) + cx</span><br><span class="line">            y = -(j - new_cx) * math.sin(angle_rad) + (i - new_cy) * math.cos(angle_rad) + cy</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 边界检查</span></span><br><span class="line">            <span class="keyword">if</span> <span class="number">0</span> &lt;= x &lt; w <span class="keyword">and</span> <span class="number">0</span> &lt;= y &lt; h:</span><br><span class="line">                <span class="comment"># 获取周围4个整数坐标点</span></span><br><span class="line">                x0, y0 = <span class="built_in">int</span>(np.floor(x)), <span class="built_in">int</span>(np.floor(y))</span><br><span class="line">                x1, y1 = <span class="built_in">min</span>(x0 + <span class="number">1</span>, w - <span class="number">1</span>), <span class="built_in">min</span>(y0 + <span class="number">1</span>, h - <span class="number">1</span>)</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 获取4个点的颜色值</span></span><br><span class="line">                s0 = image[y0, x0]</span><br><span class="line">                s1 = image[y0, x1]</span><br><span class="line">                s2 = image[y1, x0]</span><br><span class="line">                s3 = image[y1, x1]</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 计算相对位置</span></span><br><span class="line">                dx = x - x0</span><br><span class="line">                dy = y - y0</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 判断四点是否共面</span></span><br><span class="line">                <span class="keyword">if</span> np.allclose(s1 + s2, s0 + s3, atol=<span class="number">1e-6</span>):</span><br><span class="line">                    <span class="comment"># 共面时，选择任意平面（此处用左下平面）</span></span><br><span class="line">                    interpolated = s0 + (s2 - s0) * dy + (s1 - s0) * dx</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="comment"># 不共面时，动态选择平面</span></span><br><span class="line">                    <span class="keyword">if</span> dy &gt; <span class="number">1</span> - dx:  <span class="comment"># 对角线 y = 1 - x 上方</span></span><br><span class="line">                        <span class="comment"># 选择右上平面</span></span><br><span class="line">                        interpolated = (s3 - s1) * dx + (s3 - s2) * dy + (s2 + s1 - s3)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="comment"># 选择左下平面</span></span><br><span class="line">                        interpolated = s0 + (s2 - s0) * dy + (s1 - s0) * dx</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 边界检查</span></span><br><span class="line">                interpolated = np.clip(interpolated, <span class="number">0</span>, <span class="number">255</span> <span class="keyword">if</span> image.dtype == np.uint8 <span class="keyword">else</span> <span class="number">1.0</span>)</span><br><span class="line">                rotated_image[i, j] = interpolated</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> rotated_image</span><br></pre></td></tr></tbody></table></figure>

<h2 id="实验测试结果分析"><a href="#实验测试结果分析" class="headerlink" title="实验测试结果分析"></a>实验测试结果分析</h2><p>一个理想的插值算法对一幅图像逆时针旋转若干度，再顺时针旋转若干度，应该与原图像相同；同理，对一幅图像放大若干倍，再缩小若干倍，也应该与原图像相同。 基于此，将下面的4幅图像分别用4种算法先逆时针旋转45°，再顺时针旋转45°；先放大４倍，再缩小４倍，然后分别用峰值信噪比（PSNR）验证各算法的优劣。 </p>
<p><img src="/images/project2/5.jpg" alt="1琳娜"></p>
<p><img src="/images/project2/6.jpg" alt="2辣椒"></p>
<p><img src="/images/project2/7.jpg" alt="3狒狒"></p>
<p><img src="/images/project2/8.jpg" alt="4房子"></p>
<p>从定性实验的效果角度，上述四幅图像通过常用的三种分段插值算法完成上述的放大与旋转任务后得到的结果如下图所示：</p>
<p><img src="/images/project2/Figure_1.png" alt="1琳娜传统result"></p>
<p><img src="/images/project2/Figure_2.png" alt="2辣椒传统result"></p>
<p><img src="/images/project2/Figure_3.png" alt="3狒狒传统result"></p>
<p><img src="/images/project2/Figure_4.png" alt="4房子传统result"></p>
<p>从实验结果上来看，最近邻算法的边缘颜色“最醒目”，且出现了较为严重的“锯齿形”边缘现象；双线性算法的边缘颜色“最暗淡”；双线性算法和双三次算法也有“锯齿形”边缘现象， 但视觉效果相比最近邻算法而言并不明显。</p>
<p>通过改进的四平面插值算法，对上述四幅图像完成上述的放大与旋转任务后得到的结果如下图所示：</p>
<p><img src="/images/project2/Figure_5.png" alt="1琳娜四平面result"></p>
<p><img src="/images/project2/Figure_6.png" alt="2辣椒四平面result"></p>
<p><img src="/images/project2/Figure_7.png" alt="3狒狒四平面result"></p>
<p><img src="/images/project2/Figure_8.png" alt="4房子四平面result"></p>
<p>可以看到，四平面插值算法处理后的图像斜线边缘部分是 “光滑连续” 的， 视觉效果比较好，同时有效避免了“锯齿形”边缘现象和“马赛克”现象。</p>
<p>从定量实验的数据角度，我们对于各图像用不同算法完成上述旋转与放缩任务后得到的图像峰值信噪比与算法运行时间进行了计算与统计，结果如下表所示：</p>
<blockquote>
<p>峰值信噪比(PSNR)用于表示信号的最大可能功率与影响其表示的保真度的破坏噪声的功率之间的比率。PSNR在图像处理上主要用于量化受有损压缩影响的图像和视频的重建质量。</p>
<p>PSNR 通过均方误差( MSE ) 定义。</p>
<p>给定一个无噪声的m×n单色图像I及其噪声近似值K，MSE定义为：<br>$$<br>MSE=\frac{1}{mn}\sum_{i=0}^{m-1}\sum_{j=0}^{n-1}[I(i,j)-K(i,j)]^2.<br>$$<br>故PSNR定义为：<br>$$<br>\begin{aligned}\mathrm{PSNR}&amp;=10\cdot\log_{10}\left(\frac{MAX_I^2}{MSE}\right)\&amp;=20\cdot\log_{10}\left(\frac{MAX_I}{\sqrt{MSE}}\right)\&amp;=20\cdot\log_{10}(MAX_I)-10\cdot\log_{10}(MSE).\end{aligned}<br>$$<br>一般而言，通过PSNR来判断处理后图像的失真情况有如下通用结论：</p>
<ul>
<li>PSNR &gt; 30 dB：图像质量较好，失真不明显。</li>
<li>PSNR 20~30 dB：中等质量，存在可察觉失真。</li>
<li>PSNR &lt; 20 dB：质量较差，失真显著。</li>
</ul>
<p>实际计算时，采用opencv自带的PSNR方法cv2.PSNR(img, output)对原始图像与处理后图像的PSNR进行比较计算。</p>
</blockquote>
<table>
<thead>
<tr>
<th>测试图像</th>
<th>最近邻插值PSNR</th>
<th>双线性插值PSNR</th>
<th>双立方插值PSNR</th>
<th>四平面插值PSNR</th>
</tr>
</thead>
<tbody><tr>
<td>琳娜（269*269）</td>
<td>20.74217399</td>
<td>27.11906575</td>
<td>29.36532325</td>
<td>36.70765842</td>
</tr>
<tr>
<td>辣椒（268*268）</td>
<td>22.92424674</td>
<td>27.91435345</td>
<td>31.04713312</td>
<td>39.25866529</td>
</tr>
<tr>
<td>狒狒（268*268）</td>
<td>21.8194312</td>
<td>28.06968286</td>
<td>29.12614241</td>
<td>38.2193871</td>
</tr>
<tr>
<td>房子（256*256）</td>
<td>22.34146151</td>
<td>26.21366716</td>
<td>30.67389681</td>
<td>38.8204405</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>插值算法</th>
<th>最近邻法</th>
<th>双线性法</th>
<th>双立方法</th>
<th>四平面法</th>
</tr>
</thead>
<tbody><tr>
<td>算法运行平均用时</td>
<td>0.678485751</td>
<td>3.293492556</td>
<td>92.66596091</td>
<td>15.02119243</td>
</tr>
</tbody></table>
<p>通过对比上述定量实验结果可以发现，在传统的三种分段插值算法中，随着运算阶数（采样待插值点周围的原图像像素点颜色值信息）的增加，图像经过放缩与旋转处理后的失真程度有明显降低，但仍大致处于存在可察觉失真的区间，且算法运行用时也逐渐增加（事实上双立方法的实现可以在编程层面实现优化，这里只是为更直观地展现O（n^2）时间复杂度在图像大小达到一定规模时的显著影响）；而引入的四平面算法不仅在失真程度上较传统的插值算法均有显著改善，算法运行用时也明显优于传统算法中效果最好的双立方法。</p>
<p>综合以上的定性与定量实验结果及分析，本文提出的基于四平面的图像插值算法在图像处理效果（失真）与运行效率上均较传统算法有明显提升，这充分证明了该算法的有效性。</p>
<p>将上文提到的全部四种算法及旋转与放缩两种功能集成到基于python的gui可视化系统中，并打包成exe可执行文件，制作了一个基于插值的图像处理系统，基本功能演示如下图所示：</p>
<p><img src="/images/project2/1.png" alt="gui演示1"></p>
<p><img src="/images/project2/2.png" alt="gui演示2"></p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] 王开荣,杨大地编著.应用数值分析[M].高等教育出版社,2010.</p>
<p>[2] 毛伟伟,于素萍,石念峰.一种基于四平面的图像插值算法[J].洛阳理工学院学报(自然科学版),2024,34(01):76-81.</p>
<p>[3] 刘显德,李笑.任意大小图像的量子描述及双线性插值方法[J].计算机工程与设计,2024,45(08):2423-2432.</p>
<p>[4] 张喜民,詹海生.基于双三次插值的Canny-Devernay亚像素图像边缘检测算法[J].现代制造工程,2025,(03):107-114.</p>
<p>[5] 陈玲玲,周宁,殷永,等.插值方法在光声图像重建中的应用[J].计算机与数字工程,2013,41(10):1676-1677+1694.</p>
</div></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2025-01-04T17:02:03.000Z" title="2025/1/5 01:02:03">2025-01-05</time>发表</span><span class="level-item"><time datetime="2025-03-01T18:06:45.232Z" title="2025/3/2 02:06:45">2025-03-02</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/">课程项目</a><span>&nbsp;/&nbsp;</span><a class="link-muted" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E6%99%BA%E8%83%BD%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/">智能图像处理</a></span><span class="level-item">1 小时读完 (大约7630个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2025/01/05/graph/">基于自编码器和卷积网络的肺炎图像识别</a></p><div class="content"><div id="postchat_postcontent"><h2 id="1-项目背景与研究意义"><a href="#1-项目背景与研究意义" class="headerlink" title="1 项目背景与研究意义"></a>1 项目背景与研究意义</h2><h3 id="1-1-项目背景"><a href="#1-1-项目背景" class="headerlink" title="1.1 项目背景"></a>1.1 项目背景</h3><p>肺炎作为一种常见的呼吸系统疾病，对人类健康构成了长期威胁，特别是随着COVID-19新冠肺炎疫情的全球爆发，其公共健康影响更为显著。COVID-19肺炎具有传播速度快、感染范围广、诊断难度大的特点，对全球医疗系统和社会经济产生了深远影响。特别是在疫情高峰期，医疗资源的短缺和诊断效率的瓶颈，进一步突显了快速、准确诊断工具的重要性。</p>
<p>传统肺炎诊断方法主要依赖医生对胸部X光片或CT图像的人工分析，既耗时又容易受到经验和疲劳的影响，尤其在COVID-19疫情期间，大量影像数据的涌现使得人工诊断难以满足需求。与此同时，COVID-19的影像表现与其他类型肺炎的重叠性增加了诊断的复杂性，这进一步加剧了对智能化诊断系统的需求。</p>
<p>随着人工智能技术的快速发展，深度学习为医疗影像分析带来了全新的解决方案。卷积神经网络（Convolutional Neural Network, CNN）凭借其强大的图像特征提取能力，在自动化诊断中展现了巨大潜力。同时，由于不同医疗机构的CT扫描设备性能差异显著，特别是在医疗资源较为匮乏的地区，CT影像常常受到设备老化、分辨率低或操作不规范等因素的影响，图像质量参差不齐，这不仅增加了诊断的复杂性，还对自动化系统的鲁棒性提出了更高要求；而自编码器（Autoencoder）作为一种有效的降噪工具，为医疗影像数据预处理提供了重要支持。通过自编码器的引入，可以有效消除图像中的噪声干扰，减少不同设备间的成像差异，为后续的分类和识别模型提供高质量的输入数据。因此，本项目提出结合自编码器和卷积神经网络的深度学习框架，开发一套针对肺炎（包括COVID-19）CT影像识别的智能诊断系统。</p>
<p><img src="/images/graph/media/image1.jpeg" alt="图1.1：新冠肺炎疫情概况（主要症状、防治措施与传播状况）"></p>
<p><img src="/images/graph/media/image2.jpeg" alt="图1.2：新冠肺炎患者的肺部CT影像"></p>
<h3 id="1-2-研究意义"><a href="#1-2-研究意义" class="headerlink" title="1.2 研究意义"></a>1.2 研究意义</h3><p>本项目以新型冠状病毒肺炎为切入点，面向未来医学智能化需求，开发的诊断系统不仅能够应对当下疫情挑战，还具有推广至其他医学影像诊断场景的潜力，从而为全球公共健康事业的发展提供有力支持。</p>
<p>面向新冠肺炎疫情期间大规模肺部影像数据的快速诊断需求，本系统依托深度学习技术，有效提升了诊断效率，为疫情防控和患者管理提供重要的技术支持。通过先进的模型算法，系统能够精准识别肺部CT影像中的病变特征，减少人为误差，确保诊断结果的准确性和一致性，大幅降低不同医疗机构和医生之间的诊断差异，避免误诊和漏诊风险，从而更好地保障患者安全。</p>
<p>针对基层医院或偏远地区医疗资源匮乏的现状，本系统可作为一种可靠的辅助诊断工具，为医生提供科学的决策支持，帮助缓解诊断能力不足带来的压力。其高效的处理能力不仅提高了基层医疗服务水平，也为疫情防控的全面推进提供了技术保障，为应对紧急医疗需求的地区解决实际困难。</p>
<p>此外，本系统在高效处理和分析海量肺部CT影像数据的基础上，还为研究新冠肺炎的病理特征及流行规律提供了宝贵的数据支持。这些分析结果可进一步应用于疫情传播趋势预测和公共卫生政策制定，为疫情防控策略的科学性和有效性奠定了坚实基础。</p>
<p>同时，本系统也为患者病情的动态管理提供了重要帮助。通过智能分析新冠肺炎影像特征的变化趋势，系统能够为临床医生提供精确的病情评估建议，有助于及时调整治疗方案。这种基于影像数据的技术支持，不仅提高了患者管理的科学性和有效性，还为医疗资源的合理分配提供了重要依据，进一步推动了疫情防控工作的高效开展。</p>
<h2 id="2-数据集获取与预处理"><a href="#2-数据集获取与预处理" class="headerlink" title="2 数据集获取与预处理"></a>2 数据集获取与预处理</h2><h3 id="2-1-数据集介绍"><a href="#2-1-数据集介绍" class="headerlink" title="2.1 数据集介绍"></a>2.1 数据集介绍</h3><p>本项目使用的肺部X-光片数据集从Kaggle网站（链接：<a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/alsaniipe/chest-x-ray-image">https://www.kaggle.com/datasets/alsaniipe/chest-x-ray-image</a>）获取，共分为三类标签：新冠肺炎COVID19、正常NORMAL和普通肺炎PNEUMONIA，该数据集已经事先划分好了训练集与测试集。此外，为测试模型对于不同质量CT影像的识别精度，还对测试集中的部分图像使用高斯噪声进行扰动，以模拟实际CT扫描的成像质量差异。</p>
<p><img src="/images/graph/media/image3.png" alt="图2.1：肺部CT影像数据集概况"><br><img src="/images/graph/media/image4.png" alt="图2.2：肺部CT影像数据集概况"></p>
<p><img src="/images/graph/media/image5.png" alt="图2.3：数据集中部分肺部CT影像展示（上行为训练集（干净），下行为含噪声测试集）"></p>
<p>数据分布如下表所示：</p>
<p>表2.1：数据集数据分布情况</p>
<table>
<thead>
<tr>
<th></th>
<th>COVID19</th>
<th>NORMAL</th>
<th>PNEUMONIA</th>
</tr>
</thead>
<tbody><tr>
<td>Train</td>
<td>460</td>
<td>1266</td>
<td>3418</td>
</tr>
<tr>
<td>Test</td>
<td>116</td>
<td>317</td>
<td>855</td>
</tr>
<tr>
<td>Noisy_Test</td>
<td>26</td>
<td>20</td>
<td>20</td>
</tr>
</tbody></table>
<h3 id="2-2-编程环境搭建"><a href="#2-2-编程环境搭建" class="headerlink" title="2.2 编程环境搭建"></a>2.2 编程环境搭建</h3><p>本项目中所有的代码编写与运行均是在配备NVIDIA GeForce RTX 3060显卡、16GB运行内存、12th Gen Inter(R) Core(TM) <a href="mailto:i7-12700@2.10GHz">i7-12700@2.10GHz</a>处理器与Microsoft Windows 11操作系统的工作站上使用Python编程语言完成的。</p>
<p>软件环境方面，采用Conda进行环境管理，在控制台中通过命令”conda create -n covid python=3.12”创建虚拟环境，并在激活环境后使用pip install命令依次安装所需的各种依赖库；全部安装并测试完成后，通过命令”pip freeze &gt;<br>requirements.txt”将虚拟环境中安装的所有依赖库及对应版本写入文件requirements.txt，后续移植时可在新的运行环境中运行命令”pip install -r requirements.txt”完成环境的一键配置。</p>
<p>值得注意的是，在PyTorch（包括torch、torchvision与torchaudio库）安装时需要根据自己电脑使用的CUDA版本（使用CPU则直接在命令行中使用pip install安装即可）在<a target="_blank" rel="noopener" href="https://pytorch.org/">PyTorch官网</a>中找到对应的安装命令进行安装。我使用的CUDA版本为12.6，安装命令的选取如下图所示：</p>
<p><img src="/images/graph/media/image6.png" alt="图2.4：PyTorch官网获取对应CUDA版本的安装命令"></p>
<p>下面对于项目中使用到的主要依赖库进行简要介绍：</p>
<p><img src="/images/graph/media/image7.png" alt="图2.5：项目中使用的主要依赖库"></p>
<p>其中，plt用于绘图，nn中包含了用于构建神经网络的隐藏层（全连接层、卷积层等），F中包含了各种激活函数（ReLu、Sigmoid等），DataLoader用于在训练时加载数据，datasets和transforms用于读取和处理数据集，tqdm用于进度条可视化，torchmetrics用于模型精度的测试，torchviz和torchsummary用于以图形与文字的方式描述模型架构概况。</p>
<h3 id="2-3-图像数据读取"><a href="#2-3-图像数据读取" class="headerlink" title="2.3 图像数据读取"></a>2.3 图像数据读取</h3><p>torch对于一些常用的数据集做了封装，可以直接调用，例如datasets.MNIST()。但此处我们使用的是本地的图片数据，可以使用ImageFolder将一个文件夹下的图片读取成数据集并完成数据增强工作。在读取完数据集后，还需要定义DataLoader用于加载数据为可分批次（batch）读取的迭代器以供后续使用。为使得代码更加简洁，将上述的数据读取与加载过程为封装在getDataLoader函数中，并在主函数中通过指定不同的目录加载训练集、测试集或是含噪声测试集。</p>
<p><img src="/images/graph/media/image8.png" alt="图2.6：数据加载函数getDataLoader代码"></p>
<p>可以看到，其中构建了数据增强器transform，在读取数据时进行相应处理：</p>
<ul>
<li><p>Grayscale: 指以灰度图的形式读取。</p>
</li>
<li><p>Resize: 由于图像尺寸各不相同，在训练前需将它们重塑成相同尺寸256*256。</p>
</li>
<li><p>ToTensor: 将图片格式转换成张量形式，torch的计算以张量的形式进行。</p>
</li>
</ul>
<p>除此之外，在构建数据加载器时需要指定一个批次（batch）中的图片数据数量batch_size，在模型训练时训练批次大小TRAIN_BATCH_SIZE也是会影响最终模型性能的重要超参数之一。在训练过程中，设定TRAIN_BATCH_SIZE为32，而在测试过程中，为提高测试效率，将TEST_BATCH_SIZE设置为66并对函数进行对应修改。</p>
<h3 id="2-4-叠加噪声函数"><a href="#2-4-叠加噪声函数" class="headerlink" title="2.4 叠加噪声函数"></a>2.4 叠加噪声函数</h3><p>不论是构建噪声测试集，还是在利用无噪声的训练集进行训练时，都需要手动添加噪声，故编写add_noise函数，默认的噪声强度为0.5，并在添加噪声后进行归一化以确保图像值位于[0,1]范围内。</p>
<p><img src="/images/graph/media/image9.png" alt="图2.7：加噪函数add_noise代码"></p>
<p><img src="/images/graph/media/image10.JPG" alt="图2.8：加噪前后效果对比"></p>
<h2 id="3-模型构建与网络训练"><a href="#3-模型构建与网络训练" class="headerlink" title="3 模型构建与网络训练"></a>3 模型构建与网络训练</h2><h3 id="3-1-整体模型框架"><a href="#3-1-整体模型框架" class="headerlink" title="3.1 整体模型框架"></a>3.1 整体模型框架</h3><p>整体模型框架由两个核心部分组成，分别是用于去噪的数据预处理模块和负责分类的卷积神经网络（CNN）。去噪模块采用自编码器（Autoencoder）的架构，专注于从输入数据中去除噪声，以提升后续分类的准确性；分类模块基于卷积神经网络，其强大的特征提取和模式识别能力使其成为分类任务的理想选择。</p>
<p>两个模块相辅相成，通过有效的数据处理和特征提取，确保模型能够在噪声干扰较大的环境中实现高精度分类。噪声数据首先经过自编码器处理，生成质量优化的特征表示，然后被CNN接收并完成分类任务。这一整体框架设计非常适合肺炎图像识别任务，通过结合去噪和分类两大模块的优势，模型不仅能够有效提高数据质量，还能充分挖掘数据中的有用特征，从而能够在复杂的医学影像处理中表现出卓越的鲁棒性和准确性，满足肺炎诊断的实际需求。</p>
<p><img src="/images/graph/media/image11.png" alt="图3.1：模型框架图示"></p>
<h3 id="3-2-自编码器"><a href="#3-2-自编码器" class="headerlink" title="3.2 自编码器"></a>3.2 自编码器</h3><p>自编码器模型用于处理输入数据中的噪声问题，提升后续分类的准确性。其核心思想是通过编码器将输入数据压缩至低维潜在表示（latent representation），再由解码器将其还原至去噪后的重构数据，从而实现降噪效果。</p>
<h4 id="3-2-1-网络结构设计"><a href="#3-2-1-网络结构设计" class="headerlink" title="3.2.1 网络结构设计"></a>3.2.1 网络结构设计</h4><p><img src="/images/graph/media/image12.png" alt="图3.2：自编码器模型结构"></p>
<p>自编码器网络结构由编码器encoder与解码器decoder组成：</p>
<ul>
<li><p>编码器由两层卷积（Conv2d）和两次池化（MaxPool2d）操作组成，用于提取特征；</p>
</li>
<li><p>解码器通过两次反卷积（ConvTranspose2d）和两次上采样（UpsamplingNearest2d）逐步恢复图像尺寸到原始大小；</p>
</li>
<li><p>最后使用Sigmoid激活函数将输出值限制在[0,1]区间。</p>
</li>
</ul>
<p>模型定义代码如下：</p>
<p><img src="/images/graph/media/image13.png" alt="图3.3：自编码器模型定义代码"></p>
<p>模型继承自nn.Module类，在__init__()函数中定义模型的结构，在forward()函数中定义模型的前向传播过程。</p>
<p>通过调用torchviz和torchsummary库，可以输出该模型结构的基本信息：</p>
<p><img src="/images/graph/media/image14.png" alt="图3.4：调用torchsummary库输出自编码器网络结构的文字信息"></p>
<p><img src="/images/graph/media/image15.JPG" alt="图3.5：调用torchviz库输出自编码器网络结构的架构图示"></p>
<h4 id="3-2-2-模型训练"><a href="#3-2-2-模型训练" class="headerlink" title="3.2.2 模型训练"></a>3.2.2 模型训练</h4><p>基本的训练流程集成在函数train_autoencoder_process中，如下图所示：</p>
<p><img src="/images/graph/media/image16.png" alt="图3.6：自编码器模型训练函数train_autoencoder_process代码"></p>
<p>其中指定优化器optimizer为Adam，损失函数为均方误差MSE，并使用超参数：训练轮数Epochs=50、学习率lr=0.001。每轮（Epoch）训练中均需要以多个batch的形式遍历训练集中的所有数据，并在每个batch后对模型进行更新，具体而言每次更新均需执行如下操作：</p>
<ul>
<li><p>从加载器中获取输入数据</p>
</li>
<li><p>使用add_noise函数对干净图像加噪</p>
</li>
<li><p>将加噪后图像输入自编码器模型并计算模型输出</p>
</li>
<li><p>根据模型输出和标签计算损失Loss</p>
</li>
<li><p>清空梯度</p>
</li>
<li><p>反向传播</p>
</li>
<li><p>更新模型</p>
</li>
</ul>
<p>值得注意的是，由于用于训练的图像数据没有噪声，因此训练时首先需要对输入的图像进行加噪处理，再输入自编码器模型进行训练。</p>
<p>训练过程中还利用tqdm进度条函数对训练进程进行可视化，并在每轮训练完成后打印出当轮训练过程中模型的平均损失：</p>
<p><img src="/images/graph/media/image17.png" alt="图3.7：自编码器模型训练过程进度条（前5个Epoch）"></p>
<p>在训练过程中，将每轮训练的平均损失存储在列表中，并在训练结束后将平均损失的变化过程以图像形式呈现：</p>
<p><img src="/images/graph/media/image18.JPG" alt="图3.8：自编码器模型训练损失变化"></p>
<p>可以看到，经过多轮训练，模型的损失函数值在不断减小且逐渐趋近于0，这意味着该自编码器的模型训练过程是收敛的，模型具有较稳定的工作性能。</p>
<h3 id="3-3-卷积神经网络"><a href="#3-3-卷积神经网络" class="headerlink" title="3.3 卷积神经网络"></a>3.3 卷积神经网络</h3><p>卷积神经网络负责从图像中提取多层次的空间特征，通过逐步减少图像尺寸和增加特征通道来捕捉关键信息，从而实现去噪后肺部CT图像的分类功能。CNN以其强大的特征提取能力，能够有效处理图像的局部依赖性和空间不变性，高效处理结构化数据（如图像、时序数据）。模型简单且高效，具有较强的泛化能力，适合处理小规模数据集的图像分类问题。</p>
<h4 id="3-3-1-网络结构设计"><a href="#3-3-1-网络结构设计" class="headerlink" title="3.3.1 网络结构设计"></a>3.3.1 网络结构设计</h4><p><img src="/images/graph/media/image19.png" alt="图3.9：卷积神经网络模型结构"></p>
<p>卷积神经网络结构（如上图，通过<a target="_blank" rel="noopener" href="http://alexlenail.me/NN-SVG/AlexNet.html">NN-SVG工具</a>绘制）由两层卷积层（Conv2d）和池化层（MaxPool2d）组成，激活函数均选用ReLU，逐步提取特征并将输入图像的尺寸从原始大小减小到64×64。卷积后的特征图展平后通过三个全连接层（Linear），分别将特征维度从32×64×64降至128，再降至32，最后输出3个类别（Covid19、Normal、Pneumonia）的预测结果。</p>
<p>模型定义代码如下：</p>
<p><img src="/images/graph/media/image20.png" alt="图3.10：卷积神经网络模型定义代码"></p>
<p>模型继承自nn.Module类，在__init__()函数中定义模型的结构，在forward()函数中定义模型的前向传播过程。</p>
<p>通过调用torchviz和torchsummary库，可以输出该模型结构的基本信息：</p>
<p><img src="/images/graph/media/image21.png" alt="图3.11：调用torchsummary库输出卷积神经网络结构的文字信息"></p>
<p><img src="/images/graph/media/image22.png" alt="图3.12：调用torchviz库输出卷积神经网络结构的架构图示"></p>
<h4 id="3-3-2-模型训练"><a href="#3-3-2-模型训练" class="headerlink" title="3.3.2 模型训练"></a>3.3.2 模型训练</h4><p>基本的训练流程集成在函数train_cnn_process中，如下图所示：</p>
<p><img src="/images/graph/media/image23.png" alt="图3.13：卷积神经网络模型训练函数train_cnn_process代码"></p>
<p>其中指定优化器optimizer为Adam，损失函数为交叉熵损失CrossEntropy，并使用超参数：训练轮数Epochs=50、学习率lr=0.001。每轮（Epoch）训练中均需要以多个batch的形式遍历训练集中的所有数据，并在每个batch后对模型进行更新，具体而言每次更新均需执行如下操作：</p>
<ul>
<li><p>从加载器中获取输入数据</p>
</li>
<li><p>使用add_noise函数对干净图像加噪</p>
</li>
<li><p>将加噪后图像输入训练好的自编码器模型trained_autoencoder_model</p>
</li>
<li><p>将经过自编码器去噪后的图像输入CNN模型并计算模型输出</p>
</li>
<li><p>根据模型输出和标签计算损失Loss</p>
</li>
<li><p>清空梯度</p>
</li>
<li><p>反向传播</p>
</li>
<li><p>更新模型</p>
</li>
</ul>
<p>值得注意的是，由于用于训练的图像数据没有噪声，为与实际的输入情况一致，首先需要对输入的图像进行加噪处理，再利用训练好的自编码器模型进行降噪（为了不在更新CNN的同时更新自编码器，这一步不需要产生梯度），才能输入CNN分类模型进行训练。</p>
<p>训练过程中还利用tqdm进度条函数对训练进程进行可视化，并在每轮训练完成后打印出当轮训练过程中模型的平均损失与在训练集上的测试精度：</p>
<p><img src="/images/graph/media/image24.png" alt="图3.14：卷积神经网络模型训练过程进度条（最后5个Epoch）"></p>
<p>在训练过程中，将每轮训练的平均损失与模型在训练集上的测试精度存储在列表中，并在训练结束后将两者的变化过程以图像形式呈现：</p>
<p><img src="/images/graph/media/image25.png" alt="图3.15：卷积神经网络模型训练损失变化"></p>
<p><img src="/images/graph/media/image26.png" alt="图3.16：卷积神经网络模型训练过程中在训练集上的精度变化"></p>
<p>可以看到，经过多轮训练，模型的损失函数值在不断减小且逐渐趋近于0，这意味着该自编码器的模型训练过程是收敛的，模型具有较稳定的工作性能；同时随着训练轮数增加，模型在训练集上的精度也逐渐增高（波动上升），在模型训练完成时，卷积神经网络在训练集上的分类精度已经可以达到99.59%（一度达到99.90%），接近百分之百，说明模型的分类能力较好。</p>
<h2 id="4-模型测试及应用"><a href="#4-模型测试及应用" class="headerlink" title="4 模型测试及应用"></a>4 模型测试及应用</h2><h3 id="4-1-自编码器降噪效果"><a href="#4-1-自编码器降噪效果" class="headerlink" title="4.1 自编码器降噪效果"></a>4.1 自编码器降噪效果</h3><p>在自编码器模型的训练过程中，每隔10轮对模型参数进行了一次存档；在测试过程中，分别使用训练轮数为10、20、30、40、50的自编码器模型对于加噪后的模型进行降噪处理，效果如下图所示：</p>
<p><img src="/images/graph/media/image27.png" alt="图4.1：训练轮数Epoch=10的自编码器模型降噪效果"></p>
<p><img src="/images/graph/media/image28.png" alt="图4.2：训练轮数Epoch=20的自编码器模型降噪效果"></p>
<p><img src="/images/graph/media/image29.png" alt="图4.3：训练轮数Epoch=30的自编码器模型降噪效果"></p>
<p><img src="/images/graph/media/image30.png" alt="图4.4：训练轮数Epoch=40的自编码器模型降噪效果"></p>
<p><img src="/images/graph/media/image31.png" alt="图4.5：训练轮数Epoch=50的自编码器模型降噪效果"></p>
<p>通过对比不同训练轮数的自编码器模型降噪效果可以发现，随着训练轮数的增加，自编码器模型的降噪效果在逐渐提升，但在Epoch到达30之后，训练带来的降噪效果提升就不如先前显著了。尽管由于较大的噪声强度（0.5）导致降噪后的图像仍然比较模糊，但通过肉眼还是能粗略观察处肺部骨骼的轮廓等特征，后续实验也证明了卷积神经网络确实可以从这样清晰度的图像中提取相应的特征来进行分类，该自编码器模型的设计有效。</p>
<h3 id="4-2-卷积神经网络分类精度"><a href="#4-2-卷积神经网络分类精度" class="headerlink" title="4.2 卷积神经网络分类精度"></a>4.2 卷积神经网络分类精度</h3><p>在卷积神经网络的分类精度上，训练过程中已经实时对于每一轮训练后的模型在训练集上进行了精度测试（3.2.2节中已有提及），而在测试集上，可以编写与训练过程类似的代码利用torchmetrics库对模型分类精度进行测试，只是不会更新模型，代码如下：</p>
<p><img src="/images/graph/media/image32.png" alt="图4.6：卷积神经网络分类精度在测试集上的测试函数代码"></p>
<p>可以看到，由于我们的测试集分为含噪声和不含噪声两类，因此编写了不同的函数对模型分类精度进行测试。两个函数的主要差别就在于，由于含噪声测试集是已经加噪的图片（噪声与手动通过add_noise函数添加的不同），因此在含噪声测试集的测试代码中不必再次手动添加噪声，而是直接将图像输入自编码器降噪后再输入CNN分类模型中进行分类；而对于不含噪声的测试集而言，为模拟与训练集同样的处理流程，会先进行手动加噪再通过自编码器降噪之后才输入CNN分类模型中进行分类。</p>
<p>运行测试代码后，得到模型在含噪测试集上的分类精度为96.97%，在不含噪声的测试集上的分类精度为94.57%，在两个测试集上的分类精度水平均较高，说明该模型具有良好的分类效果。</p>
<h3 id="4-3-模型应用：基于CT影像的肺炎诊断Web服务"><a href="#4-3-模型应用：基于CT影像的肺炎诊断Web服务" class="headerlink" title="4.3 模型应用：基于CT影像的肺炎诊断Web服务"></a>4.3 模型应用：基于CT影像的肺炎诊断Web服务</h3><p>通过对比多组超参数的模型降噪与分类效果，最终选定如下的超参数：</p>
<ul>
<li><p>训练轮数Epochs=50；</p>
</li>
<li><p>学习率LR=0.001；</p>
</li>
<li><p>训练批次大小Train_Batch_Size=32。</p>
</li>
</ul>
<p>选定参数后，将整体代码抽离为model.py（包含模型定义类代码），run.py（服务端代码）和train.py（训练函数），并将模型部署到实际应用中，使用Flask作为服务端，以Web形式用户提供操作接口以上传图片进行诊断。由于主要功能是提供接口，故网页只做了很简易的一个index.html，给用户提供上传图片的按钮，并在用户上传有噪声的CT影像后返回诊断结果及去噪后的图像。除此之外，还将挂载在本地端口上的Web通过内网穿透映射到公网，以供实时访问。</p>
<p>网页初始界面如下图所示：</p>
<p><img src="/images/graph/media/image33.png" alt="图4.7：网页初始界面"></p>
<p>接下来分别测试当输入COVID19、NORMAL和PNEUMONIA三个组别的图片，模型能否正确判断：</p>
<p><img src="/images/graph/media/image34.png" alt="图4.8：输入类型为COVID19，识别为COVID19（正确）"></p>
<p><img src="/images/graph/media/image35.png" alt="图4.9：输入类型为COVID19，识别为PNEUMONIA（错误）"></p>
<p><img src="/images/graph/media/image36.png" alt="图4.10：输入类型为NORMAL，识别为NORMAL（正确）"></p>
<p><img src="/images/graph/media/image37.png" alt="图4.11：输入类型为PNEUMONIA，识别为PNEUMONIA（正确）"></p>
<p>可以发现，模型在大多数情况下可以正确识别图像来源，但也会出现错误识别的情况，这和Test 集上的Accuracy相符合；此外，在测试时还注意到，模型识别结果偶尔会出现不稳定的现象，即输入同一张图像有时识别为某一类别，有时又会识别为另一类别，这是由模型内部部分随机参数导致的，这也反映了模型在一些模棱两可的情况下（两类别概率接近）做出判断时的不稳定性。在实际应用中，为尽可能减少误诊对于患者带来的各方面影响，还需要采取更多优化措施提升模型性能，并对模型在模棱两可的情况下做出的判断进行合理的限制。</p>
<h2 id="5-总结与展望"><a href="#5-总结与展望" class="headerlink" title="5 总结与展望"></a>5 总结与展望</h2><p>本项目全部代码（不包含数据集）已上传至Github仓库，仓库URL地址：<a target="_blank" rel="noopener" href="https://github.com/Asgard-Tim/Pneumonia-Image-Recognition">https://github.com/Asgard-Tim/Pneumonia-Image-Recognition</a></p>
<h3 id="5-1-项目总结"><a href="#5-1-项目总结" class="headerlink" title="5.1 项目总结"></a>5.1 项目总结</h3><p>本项目基于深度学习技术，结合自编码器和卷积神经网络，开发了一套智能诊断系统，用于快速、高效地识别肺部的CT影像并判断该患者是否患有肺炎（包括COVID-19）。自编码器模块有效去除了噪声，提升了图像质量，而卷积神经网络以其强大的特征提取能力，实现了高精度的分类。本项目在数据预处理、模型设计、网络训练及测试等环节中均采用了创新性的技术方案，最终实现了在含噪声测试集上96.97%和在无噪声测试集上94.57%的分类精度，表现出了较高的鲁棒性和实用价值。同时，系统已通过Flask框架部署为Web服务，能够实时接收CT影像并给出诊断结果，为疫情期间大规模影像数据的快速诊断及基层医疗资源匮乏地区的医疗支持提供了重要的技术保障。</p>
<h3 id="5-2-课程收获与反思"><a href="#5-2-课程收获与反思" class="headerlink" title="5.2 课程收获与反思"></a>5.2 课程收获与反思</h3><p>本次选修《智能图像处理》这门课程确实让我学到了很多东西，其实自己之前也自己看过一些机器学习方面的内容，有一定的知识基础与环境搭建经验，但由于各方面原因总是没有系统性的去学习计算机视觉的相关知识，也缺乏足够的实战代码与项目经验。通过这门课程的学习，很大程度上锻炼了我Python的代码能力，也在Coding的过程中不断熟悉OpenCV、Pytorch等库的使用，更在实践的过程中不断加深对于各种算法模型（AlexNet、ResNet、YOLO等）的理解。</p>
<p>本次项目让我完整地经历了从数据集获取、论文调研及算法代码实现，再到代码调试与模型训练测试，最终将模型应用到实际系统中的全过程，在项目实现的过程中收获了很多课程教学与实验中涉及不到的东西，包括数据集的收集、模型的选择以及作为一个完整项目的代码实现等等多个方面，这也是我第一次使用GPU资源去进行。虽然由于时间等条件的限制，在模型选择上并没有进行深入的调研与充分的对比试验，只是基于自己已知的一些知识对于架构较为简单的自编码器模型与卷积神经网络进行了复现与设计，最终模型的分类精度还有一定的提升空间，但是这也为我后续的自主学习打下了一个良好的基础，希望未来我能在计算机视觉方面有更加深入的学习与探索，也感谢老师的耐心指导与悉心教学。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Nosa-Omoruyi M, Oghenekaro L U. AutoEncoder Convolutional Neural Network for Pneumonia Detection[J]. arXiv preprint arXiv:2409.02142, 2024.</p>
<p>[2] Ratiphaphongthon W, Panup W, Wangkeeree R. An improved technique for pneumonia infected patients image recognition based on combination algorithm of smooth generalized pinball SVM and variational autoencoders[J]. IEEE Access, 2022, 10: 107431-107445.</p>
<p>[3] Gayathri J L, Abraham B, Sujarani M S, et al. A computer-aided diagnosis system for the classification of COVID-19 and non-COVID-19 pneumonia on chest X-ray images by integrating CNN with sparse autoencoder and feed forward neural network[J]. Computers in biology and medicine, 2022, 141: 105134.</p>
<p>[4] García-Ordás M T, Benítez-Andrades J A, García-Rodríguez I, et al. Detecting respiratory pathologies using convolutional neural networks and variational autoencoders for unbalancing data[J]. Sensors, 2020,20(4): 1214.</p>
<p>[5] Xia Y. Enhanced Pneumonia Detection in Chest X-Rays Based on Integrated Denoising Autoencoders and Convolutional Neural Networks[J].</p>
<p>[6] El-Shafai W, El-Nabi S A, El-Rabaie E S M, et al. Efficient Deep-Learning-Based Autoencoder Denoising Approach for Medical Image Diagnosis[J]. Computers, Materials &amp; Continua, 2022, 70(3).</p>
<p>[7] Rana N, Marwaha H. Auto encoder-guided Feature Extraction for Pneumonia Identification from Chest X-ray Images[C]//E3S Web of Conferences. EDP Sciences, 2024, 556: 01011.</p>
<p>[8] Ankayarkanni B, Sangeetha P. An Autoencoder-BiLSTM framework for classifying multiple types of lung diseases from CXR images[J]. Multimedia Tools and Applications, 2024: 1-30.</p>
<p>[9] 孙敬,丁嘉伟,冯光辉.一种基于自编码器降维的神经卷积网络入侵检测模型[J/OL].电信科学,1-7[2025-01-05].</p>
<p>[10] 张淙越,杨晓玲.基于卷积神经网络的新冠肺炎CT图像识别系统[J].电脑与信息技术,2022,30(03):12-14+40.</p>
</div></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2024-02-11T14:13:58.000Z" title="2024/2/11 22:13:58">2024-02-11</time>发表</span><span class="level-item"><time datetime="2024-02-13T05:54:08.702Z" title="2024/2/13 13:54:08">2024-02-13</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/About-XJH/">About XJH</a><span>&nbsp;/&nbsp;</span><a class="link-muted" href="/categories/About-XJH/%E6%98%8E%E6%85%B5/">明慵</a></span><span class="level-item">34 分钟读完 (大约5057个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/02/11/QQmsganalysis/">QQ聊天记录分析 QQMsgAnalysis</a></p><div class="content"><p>从PC端QQ中以<code>txt</code>格式导出聊天记录，存为<code>message.txt</code>。</p>
<p>需要安装的库: <code>numpy, seaborn, pandas, wordcloud, tdqm, paddlepaddle, paddlenlp</code></p></div><a class="article-more button is-small is-size-7" href="/2024/02/11/QQmsganalysis/#more">阅读更多</a></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/head2.png" alt="Jinghua Xu"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Jinghua Xu</p><p class="is-size-6 is-block">明月科创实验班人工智能专业 本科大三在读</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>重庆 重庆大学国家卓越工程师学院</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">37</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">24</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">102</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Asgard-Tim" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Asgard-Tim"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Weibo" href="https://www.weibo.com/u/6315188431"><i class="fab fa-weibo"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Bilibili" href="https://space.bilibili.com/171895120"><i class="fab fa-bilibili"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="mailto:20224546@stu.cqu.edu.cn"><i class="fas fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Phone" href="tel:+86 19132050174"><i class="fas fa-phone"></i></a></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><figure class="media-left"><a class="image" href="/2025/06/29/manufac/"><img src="/images/manufac/b99d7042c32e0662693c9051eb8316a5.jpg" alt="小提琴自动演奏机器人中的齿轮系统设计与制造"></a></figure><div class="media-content"><p class="date"><time datetime="2025-06-29T08:40:43.000Z">2025-06-29</time></p><p class="title"><a href="/2025/06/29/manufac/">小提琴自动演奏机器人中的齿轮系统设计与制造</a></p><p class="categories"><a href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/">课程项目</a> / <a href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E4%BA%A7%E5%93%81%E5%88%B6%E9%80%A0/">产品制造</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2025/06/21/sweeping/"><img src="/images/sweeping/0c9ca142c80746ccde051fd86d54a57c.png" alt="SmartRobot扫地机器人"></a></figure><div class="media-content"><p class="date"><time datetime="2025-06-20T20:02:03.000Z">2025-06-21</time></p><p class="title"><a href="/2025/06/21/sweeping/">SmartRobot扫地机器人</a></p><p class="categories"><a href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/">课程项目</a> / <a href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E5%BE%AE%E7%94%B5%E8%B7%AF%E8%AE%BE%E8%AE%A1/">微电路设计</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2025/06/10/project3/"><img src="/images/project3/9.png" alt="常微分方程反演的机器学习方法"></a></figure><div class="media-content"><p class="date"><time datetime="2025-06-09T18:59:03.000Z">2025-06-10</time></p><p class="title"><a href="/2025/06/10/project3/">常微分方程反演的机器学习方法</a></p><p class="categories"><a href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/">课程项目</a> / <a href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E5%B7%A5%E7%A8%8B%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90/">工程数值分析</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2025/05/15/Survey/"><img src="/images/survey/2.png" alt="A Survey on Vision-Language-Action Models for Embodied AI"></a></figure><div class="media-content"><p class="date"><time datetime="2025-05-15T15:32:03.000Z">2025-05-15</time></p><p class="title"><a href="/2025/05/15/Survey/">A Survey on Vision-Language-Action Models for Embodied AI</a></p><p class="categories"><a href="/categories/%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">具身智能论文阅读</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2025/05/09/PaLM-E/"><img src="/images/palm-e/0.png" alt="PaLM-E：An Embodied Multimodal Language Model"></a></figure><div class="media-content"><p class="date"><time datetime="2025-05-09T11:57:03.000Z">2025-05-09</time></p><p class="title"><a href="/2025/05/09/PaLM-E/">PaLM-E：An Embodied Multimodal Language Model</a></p><p class="categories"><a href="/categories/%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">具身智能论文阅读</a></p></div></article></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/About-XJH/"><span class="level-start"><span class="level-item">About XJH</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/About-XJH/%E6%98%8E%E6%85%B5/"><span class="level-start"><span class="level-item">明慵</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/About-XJH/%E6%98%8E%E8%AF%9A/"><span class="level-start"><span class="level-item">明诚</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE/"><span class="level-start"><span class="level-item">个人项目</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"><span class="level-start"><span class="level-item">具身智能论文阅读</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">动手学深度学习</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%E8%AF%BE/"><span class="level-start"><span class="level-item">算法基础课</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/"><span class="level-start"><span class="level-item">课程项目</span></span><span class="level-end"><span class="level-item tag">26</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E4%BA%A7%E5%93%81%E5%88%B6%E9%80%A0/"><span class="level-start"><span class="level-item">产品制造</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E4%BA%A7%E5%93%81%E8%AE%BE%E8%AE%A1/"><span class="level-start"><span class="level-item">产品设计</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E5%AE%9A%E9%87%8F%E5%B7%A5%E7%A8%8B%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%B3%95I/"><span class="level-start"><span class="level-item">定量工程设计方法I</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E5%AE%9A%E9%87%8F%E5%B7%A5%E7%A8%8B%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%B3%95II/"><span class="level-start"><span class="level-item">定量工程设计方法II</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E5%B7%A5%E6%95%88%E5%AD%A6/"><span class="level-start"><span class="level-item">工效学</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E5%B7%A5%E7%A8%8B%E5%8E%9F%E7%90%86/"><span class="level-start"><span class="level-item">工程原理</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E5%B7%A5%E7%A8%8B%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90/"><span class="level-start"><span class="level-item">工程数值分析</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E5%B7%A5%E7%A8%8B%E8%AE%BE%E8%AE%A1/"><span class="level-start"><span class="level-item">工程设计</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E5%BE%AE%E7%94%B5%E8%B7%AF%E8%AE%BE%E8%AE%A1/"><span class="level-start"><span class="level-item">微电路设计</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E6%95%B0%E5%AD%A6%E7%89%A9%E7%90%86%E6%96%B9%E6%B3%95/"><span class="level-start"><span class="level-item">数学物理方法</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E6%99%BA%E8%83%BD%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"><span class="level-start"><span class="level-item">智能图像处理</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%9F%BA%E7%A1%80/"><span class="level-start"><span class="level-item">机器人基础</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/"><span class="level-start"><span class="level-item">概率论与数理统计</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"><span class="level-start"><span class="level-item">线性代数</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E8%87%AA%E5%8A%A8%E6%8E%A7%E5%88%B6%E5%8E%9F%E7%90%86/"><span class="level-start"><span class="level-item">自动控制原理</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E8%BD%AF%E4%BB%B6%E8%AE%BE%E8%AE%A1/"><span class="level-start"><span class="level-item">软件设计</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2025/06/"><span class="level-start"><span class="level-item">六月 2025</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/05/"><span class="level-start"><span class="level-item">五月 2025</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/04/"><span class="level-start"><span class="level-item">四月 2025</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/03/"><span class="level-start"><span class="level-item">三月 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/02/"><span class="level-start"><span class="level-item">二月 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/01/"><span class="level-start"><span class="level-item">一月 2025</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/12/"><span class="level-start"><span class="level-item">十二月 2024</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/06/"><span class="level-start"><span class="level-item">六月 2024</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/02/"><span class="level-start"><span class="level-item">二月 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/01/"><span class="level-start"><span class="level-item">一月 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/12/"><span class="level-start"><span class="level-item">十二月 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/11/"><span class="level-start"><span class="level-item">十一月 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/10/"><span class="level-start"><span class="level-item">十月 2023</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/09/"><span class="level-start"><span class="level-item">九月 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/07/"><span class="level-start"><span class="level-item">七月 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/06/"><span class="level-start"><span class="level-item">六月 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/05/"><span class="level-start"><span class="level-item">五月 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/03/"><span class="level-start"><span class="level-item">三月 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/02/"><span class="level-start"><span class="level-item">二月 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/%E5%8D%95%E7%89%87%E6%9C%BA/"><span class="tag">单片机</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/STM32/"><span class="tag">STM32</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VLA/"><span class="tag">VLA</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD/"><span class="tag">具身智能</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/"><span class="tag">多模态大模型</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C++</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%AF%E7%89%B9%E6%9E%97%E5%8F%91%E5%8A%A8%E6%9C%BA/"><span class="tag">斯特林发动机</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MATLAB/"><span class="tag">MATLAB</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Matlab/"><span class="tag">Matlab</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="tag">机器学习</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="tag">动手学深度学习</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><span class="tag">学习笔记</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%B0%8F%E8%BD%A6/"><span class="tag">小车</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BF%A1%E5%8F%B7%E4%B8%8E%E7%B3%BB%E7%BB%9F/"><span class="tag">信号与系统</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%A2%91%E8%B0%B1%E5%88%86%E6%9E%90/"><span class="tag">频谱分析</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%AF%E7%89%B9%E6%9E%97%E5%BE%AA%E7%8E%AF/"><span class="tag">斯特林循环</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8A%A8%E5%8A%9B%E5%AD%A6%E4%BB%BF%E7%9C%9F/"><span class="tag">动力学仿真</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%89%E9%99%90%E5%85%83%E4%BB%BF%E7%9C%9F/"><span class="tag">有限元仿真</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"><span class="tag">算法与数据结构</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/3D-VLA/"><span class="tag">3D-VLA</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PaLM-E/"><span class="tag">PaLM-E</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%B7%AF%E5%BE%84%E8%A7%84%E5%88%92/"><span class="tag">路径规划</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RANSAC/"><span class="tag">RANSAC</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%88%86%E6%9E%90/"><span class="tag">数据处理分析</span><span class="tag">1</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/title1.png" alt="Homepage of Jinghua Xu" height="28"></a><p class="is-size-7"><span>© 2025 Tim</span>&nbsp;&nbsp;Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>&nbsp;&amp;&nbsp;<a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© Copyright by Jinghua Xu</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Asgard-Tim"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer=""></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer=""></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer=""></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer=""></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer=""></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" defer=""></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" defer=""></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mhchem.min.js" defer=""></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer=""></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer=""></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer=""></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script>
    <link rel="stylesheet" href="https://ai.tianli0.top/static/public/postChatUser_summary.min.css">
    <script>
        let tianliGPT_key = 'S-TA3IX28M1ZT7TILW';
        let tianliGPT_postSelector = '#postchat_postcontent';
        let tianliGPT_Title = '文章摘要';
        let tianliGPT_postURL = '/^https?://[^/]+/[0-9]{4}/[0-9]{2}/[0-9]{2}/';
        let tianliGPT_blacklist = '';
        let tianliGPT_wordLimit = '1000';
        let tianliGPT_typingAnimate = true;
        let tianliGPT_theme = 'default';
        var postChatConfig = {
          backgroundColor: "#3e86f6",
          bottom: "16px",
          left: "16px",
          fill: "#FFFFFF",
          width: "44px",
          frameWidth: "375px",
          frameHeight: "600px",
          defaultInput: true,
          upLoadWeb: true,
          showInviteLink: true,
          userTitle: "PostChat",
          userDesc: "如果你对网站的内容有任何疑问，可以来问我哦～",
          addButton: true,
          beginningText: "这篇文章介绍了",
          userIcon: "https://ai.tianli0.top/static/img/PostChat.webp",
          userMode: "magic",
          defaultChatQuestions: ["你好","你是谁"],
          defaultSearchQuestions: ["视频压缩","设计"]
        };
    </script>
    <script data-postchat_key="S-TA3IX28M1ZT7TILW" src="https://ai.tianli0.top/static/public/tianli_gpt.min.js"></script>
  <script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/chitose.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body></html>