<!DOCTYPE html><html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta name="theme-color" content="#123456"><meta name="generator" content="Hexo 4.2.0"><title>分类: 工程数值分析 - Homepage of Jinghua Xu</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="#3273dc"><meta name="application-name" content="Homepage of Jinghua Xu"><meta name="msapplication-TileImage" content="/img/photo.jpg"><meta name="msapplication-TileColor" content="#3273dc"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Homepage of Jinghua Xu"><meta name="apple-mobile-web-app-status-bar-style" content="default"><link rel="apple-touch-icon" sizes="144x144" href="/img/photo.jpg"><meta name="description" content="重庆大学2022级明月科创实验班人工智能专业本科在读"><meta property="og:type" content="blog"><meta property="og:title" content="Homepage of Jinghua Xu"><meta property="og:url" content="http://asgard-tim.github.io/"><meta property="og:site_name" content="Homepage of Jinghua Xu"><meta property="og:description" content="重庆大学2022级明月科创实验班人工智能专业本科在读"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://asgard-tim.github.io/img/og_image.png"><meta property="article:author" content="Tim"><meta property="article:tag" content="Blog"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://asgard-tim.github.io/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://Asgard-Tim.github.io"},"headline":"Homepage of Jinghua Xu","image":["http://asgard-tim.github.io/img/og_image.png"],"author":{"@type":"Person","name":"Tim"},"publisher":{"@type":"Organization","name":"Homepage of Jinghua Xu","logo":{"@type":"ImageObject","url":"http://asgard-tim.github.io/img/title1.png"}},"description":"重庆大学2022级明月科创实验班人工智能专业本科在读"}</script><link rel="icon" href="/img/photo.jpg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/xt256.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/title1.png" alt="Homepage of Jinghua Xu" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">时间轴</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com">GitHub</a><a class="navbar-item" target="_blank" rel="noopener" title="Contect me on GitHub" href="https://github.com/Asgard-Tim"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">分类</a></li><li><a href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/">课程项目</a></li><li class="is-active"><a href="#" aria-current="page">工程数值分析</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2025-06-09T18:59:03.000Z" title="2025/6/10 02:59:03">2025-06-10</time>发表</span><span class="level-item"><time datetime="2025-07-01T15:22:18.826Z" title="2025/7/1 23:22:18">2025-07-01</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/">课程项目</a><span>&nbsp;/&nbsp;</span><a class="link-muted" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E5%B7%A5%E7%A8%8B%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90/">工程数值分析</a></span><span class="level-item">1 小时读完 (大约9294个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2025/06/10/project3/">常微分方程反演的机器学习方法</a></p><div class="content"><div id="postchat_postcontent"><h2 id="选题背景与意义"><a href="#选题背景与意义" class="headerlink" title="选题背景与意义"></a>选题背景与意义</h2><p><strong>微分方程</strong>是数学中一个重要且广泛应用的领域，涉及到描述变化和相互关系的方程。它是一种包含导数或微分的方程，常用于自然现象建模以及解决科学和工程领域中的问题。微分方程研究是数学中的一个重要分支，涵盖了广泛的领域和应用。通过研究微分方程，我们可以<strong>理解和预测自然现象的行为</strong>，以及<strong>解决科学和工程中的实际问题</strong>。同时，微分方程的研究也促进了数学的发展和数学工具在其他学科中的应用。 </p>
<p>常微分方程被广泛应用于物理、生物、化学、经济学等各个领域。在许多情况下，观测数据是已知的，而其中隐含的微分方程仍然难以捉摸。因此，<strong>数据驱动的常微分方程(或动力系统)发现和反演</strong>是一个重要的研究方向。</p>
<p>微分方程反演问题是相对于微分方程的求解而言的：</p>
<ul>
<li>微分方程的求解通常是一个正向问题，即给定初始条件，求解关于未知函数的方程；</li>
<li>微分方程反演问题是指根据已知的结果，推导出产生这些结果的微分方程，是从结果向方程的逆向推导和反演。</li>
</ul>
<p>在数学上，微分方程的反演问题具有一系列的数学<strong>挑战和困难</strong>：</p>
<ul>
<li>反问题的<strong>病态性</strong>：微小扰动在反问题的输出中会导致较大的误差，因此需要稳定性分析和正则化方法来解决数值计算中的误差；</li>
<li>存在<strong>非唯一解</strong>情况：需要利用先验信息、约束条件和经验知识等来进行问题的约束和规定，以得到合理的解。</li>
</ul>
<p>微分方程反演问题研究在数学理论和实际应用中具有重要意义。通过解决微分方程的反演问题，我们可以从有限的观测数据中重建和推断未知的边界条件、初始条件或未知函数，从而深入理解系统的行为和性质，并提供科学、工程和医学等领域的相关应用。  </p>
<p>随着计算机技术的发展，数值计算和机器学习在微分方程反演问题研究中的应用也呈现出迅猛发展的趋势。传统的微分方程推断方法依赖于领域专家的知识和经验，而<strong>机器学习方法</strong>，如神经网络、深度学习和遗传算法等，<strong>可以自动从大量数据中学习模式和关系，从而更准确地推断出微分方程模型</strong>。如今我们可以获取到<strong>大规模的、高维度的数据集</strong>，这为从数据中推断微分方程提供了更多的信息和挑战。而传统的模型推断方法在处理大数据集时可能受到维度灾难和计算复杂度的限制。机器学习的强大计算能力和自适应建模能力为解决这些问题提供了新的思路。  </p>
<p>利用机器学习从数据中反演微分方程的方法可以分为两个方面：</p>
<ul>
<li>在<strong>模型选择</strong>方面，机器学习可以通过自动搜索和学习合适的微分方程模型，利用测度函数或结构特征来评估不同模型的性能，并选择最适合数据的微分方程模型；</li>
<li>在<strong>参数估计</strong>方面，机器学习可以利用大数据集中的样本来优化微分方程模型的参数，以最小化模型与实际数据之间的误差。</li>
</ul>
<p>利用机器学习从数据中反演微分方程是微分方程和机器学习交叉领域的重要研究方向。它利用机器学习的能力和大数据的优势，可以更准确地建模和预测复杂的动力学系统，推进科学研究和实际应用的发展。</p>
<p>针对常微分方程，目前<strong>常用的反演方法</strong>有如下几种：</p>
<ul>
<li><strong>高斯过程</strong>：将高斯过程置于状态函数之上，然后通过最大化边际对数似然性从数据中推断参数，这种方法适用于解决高维问题，但是对系统的形式有限制，并且仅用于估计系统的参数。</li>
<li><strong>符号回归</strong>：创建并更正与观测数据相对应的符号模型，为控制函数提供更具表现力的函数形式，但缺点是对于大型系统来说计算成本高，并可能容易过拟合。</li>
<li><strong>稀疏回归</strong>：找到候选基函数的稀疏组合来近似控制函数，其系数由最小二乘法或贝叶斯回归确定。这种方法提供系统的明确形式，不需要太多先验知识，但是依靠适当的候选函数；对于没有简单或稀疏表示的复杂动力系统来说可能是低效的。</li>
<li><strong>统计学习</strong>：通过最小化经验误差来学习系统在某个假设空间中的相互作用核，避免维度诅咒，可以发现非常高维度的系统，但是仅适用于具有交互作用的核函数的动力系统。</li>
<li><strong>物理信息神经网络（PINN）</strong>：一种新的深度学习方法，用于解决非线性偏微分方程的正问题和反问题。通过在前馈神经网络中嵌入由偏微分方程描述的物理信息，在不需要标签数据的情况下，将 PINN 训练为偏微分方程的近似解的代理模型。受此启发，进一步发展出了多步神经网络LMNets，这本质上是使用给定相点上的流映射提供的信息来反演动力系统种的未知控制函数f的一种逆过程。</li>
</ul>
<p>在数值分析中，发展<strong>高阶方法</strong>是许多应用中的一个重要课题。传统上，在求解动力系统时，高阶离散化技术，如线性多步法和龙格-库塔方法已经得到了发展。近年来，线性多步法也被用于动力系统的发现。随着使用深度学习的发现取得令人满意的进展，对它在动力系统发现的理论理解也在进一步发展。</p>
<p>基于以上所陈述的微分方程反演问题的研究现状，针对<strong>无显式方程的非线性系统</strong>这一在真实物理世界中更为广泛且常见的情形，我们希望基于高阶离散化技术<strong>线性多步法</strong>，结合<strong>多步神经网络LMNets</strong>这一深度学习方法，<strong>拟合和反演出数据背后的动力学物理规律</strong>。</p>
<h2 id="算法框架与原理"><a href="#算法框架与原理" class="headerlink" title="算法框架与原理"></a>算法框架与原理</h2><h3 id="研究对象——常微分方程（组）"><a href="#研究对象——常微分方程（组）" class="headerlink" title="研究对象——常微分方程（组）"></a>研究对象——常微分方程（组）</h3><p>首先明确我们的研究对象——常微分方程：<br>$$<br>\begin{aligned}&amp;\frac{dx(t)}{dt}=f(x(t),t),\&amp;x(t_{0})=x_{0}\end{aligned}<br>$$<br>上述式子是一个最简单的二维常微分方程示例，其中x(t) ∈ R是关于时间t的函数，函数关系由一个微分方程刻画，且给出了一定的初值条件以唯一确定函数x的表达式，从而能够真实地刻画物理过程随时间变化的发展情况。</p>
<p>事实上大多数时候我们需要观测的物理量远远不止x这一个，因此为了后续更容易推广到高维情形（后续无特别说明，均围绕自治常微分方程展开讨论），我们将上述的<strong>非自治常微分方程</strong>转化为向量化的<strong>自治常微分方程</strong>：<br>$$<br>\begin{aligned}&amp;\frac{d\boldsymbol{u}(t)}{dt}=\hat{f}(\boldsymbol{u}(t)),\&amp;\boldsymbol{u}(t_{0})=\boldsymbol{u}<em>{0}\end{aligned}<br>$$<br>其中：<br>$$<br>u=(x,y)\in\mathbb{R}^{D+1},\hat{f}=(f,1)^{T},u</em>{0}=(x_{0},t_{0})^{T}<br>$$<br>在反演问题中，已知在若干时刻节点t_1、t_2、…、t_n的解x的数据，我们一般不会希望直接解出x(t)的表达式，而是通过确定光滑函数f，从而利用给定的数据反演出微分方程。这样的传统是从简单的反演情形推广并沿用的，因为比起得到一个精确的函数表达式，我们更希望探究这个表达式背后的物理规律，而这往往离不开微分方程，因此保留微分方程的形式具有一定的必要性。</p>
<h3 id="多步神经网络"><a href="#多步神经网络" class="headerlink" title="多步神经网络"></a>多步神经网络</h3><p>线性多步法是用于微分方程数值求解的常用方法。传统上，它们用于求解给定常微分方程的解，可以称为微分方程的正问题。但线性多步法也可以用于反演给定解的原微分方程，属于微分方程的反问题，尤其是将线性多步法的经典数值方法与神经网络相结合，例如多步神经网络。</p>
<h4 id="线性多步法"><a href="#线性多步法" class="headerlink" title="线性多步法"></a>线性多步法</h4><p>线性多步法是求解常微分方程数值解的一种常见方法。随着计算机技术的发展，线性多步法得到更加广泛的应用。人们逐渐发现，在某些情况下，线性多步法可以提供更高的数值精度和数值稳定性。此外，线性多步法可以与其他数值方法（如龙格-库塔法）结合使用，互补彼此的优点。</p>
<p>对于上述常微分方程（非自治），设其中结点总数为N，<strong>时间间隔h</strong>为常数:<br>$$<br>h = t_{n + 1} - t_n, n = 1, …, N - 1<br>$$<br>则第n个结点的<strong>步长为M</strong>的线性多步法有以下公式：<br>$$<br>\sum_{m=0}^{M}[\alpha_{m}x_{n-m}+h\beta_{m}f(x_{n-m},t)]=0,n=M、\ldots、N, \alpha_{_M}\neq0<br>$$<br>使用线性多步法求解常微分方程时，必须<strong>选择初始的步长M值</strong>，以及<strong>确定系数α、β的不同方法</strong>，然后可以依次计算n≥M的近似值x_n。</p>
<p>事实上，根据确定系数的方法不同，线性多步法也分为多种类型，本项目中主要使用如下的三种常见的线性多步法：</p>
<ul>
<li><strong>Adams-Moulton 法</strong>：作为一种<strong>隐式</strong>方法，需要通过迭代或其他数值求解技术来解决每个时间步长的方程组。它在求解非刚性和刚性常微分方程时都表现良好，并且具有较高的数值稳定性，其精度随步长的减小而提高，但响应地可能会产生更高的计算代价。对于高阶常微分方程，Adams-Moulton 法需要结合相应的差分格式将高阶常微分方程转化为一阶方程组进行求解。</li>
</ul>
<p>$$<br>\sum_{m=0}^M\alpha_mx_{n-m}=h\sum_{m=0}^M\beta_mf(x_{n-m})<br>$$</p>
<ul>
<li><strong>Adams-Bashforth法</strong>：作为一种<strong>显式</strong>方法，在求解非刚性和刚性常微分方程时都表现良好，计算效率较高，并且容易实现，但可能会收到稳定性条件地限制，其精度随步长的减小而提高。对于高阶常微分方程，Adams-Bashforth法需要结合相应的差分格式将高阶常微分方程转化为一阶方程组进行求解。</li>
</ul>
<p>$$<br>\sum_{m=0}^M\alpha_mx_{n-m}=h\sum_{m=0}^M\beta_mf(x(t_{n-m}))<br>$$</p>
<ul>
<li><strong>后向微分公式法</strong>(Backward Differentiation Formula, <strong>BDF</strong>)：同样是一种<strong>隐式</strong>方法，但与Adams-Moulton法不同，后向微分公式法是通过解一个非线性方程组来得到当前时间步长的数值解。这个方程组可以用牛顿迭代等数值求解技术来解决。该方法具有较好的数值稳定性，其精度依赖于使用的插值多项式的阶数。一般来说，其高阶形式可以提供更高的精度，但也会增加计算的复杂性。它适用于求解非刚性和刚性常微分方程，并且在稳定性和数值精度方面通常有良好的表现。</li>
</ul>
<p>$$<br>\sum_{m=0}^{M}\alpha_{m}x_{n-m}=h\beta f(x(t_{n}))<br>$$</p>
<p>除此之外，即使采用同一种系数确定方式，选择的步长M不同，得到的系数α、β也有所不同。具体而言，本项目中针对以上三种线性多步法，分别选取步长M=2/3/4的三种情况进行实验测试，下面列出各方法对应不同步长时的系数情况：</p>
<p><img src="/images/project3/1.png" alt="线性多步法系数"></p>
<p>线性多步法的<strong>优点</strong>在于<u>其高阶的精度和较小的计算代价，可以有效地逼近微分方程的数值解</u>。但线性多步法可能会受到<u>稳定性和初始条件</u>的<strong>限制</strong>，选择适当的步长和求解方法非常重要。</p>
<h4 id="多步神经网络算法"><a href="#多步神经网络算法" class="headerlink" title="多步神经网络算法"></a>多步神经网络算法</h4><p>从线性多步法的公式中得到启发，可以用神经网络去反演常微分方程右端的f。通过相等时间间隔h的数值解x的数据训练神经网络，该神经网络的参数可以通过使以下均方误差损失函数MSE最小化来训练得到：<br>$$<br>MSE=\frac{1}{N-M+1}\sum_{n=M}^N\parallel y_n\parallel^2<br>$$<br>其中：<br>$$<br>y_n=\sum_{m=0}^M\left[\alpha_mu_{n-m}+h\beta_mf_{NN}(u_{n-m})\right],n=M,\ldots,N<br>$$<br>式中f_NN表示神经网络对函数f的近似，y_n也称为线性多步法（LMM）残差。</p>
<p>在Python中编写函数<code>train()</code>以封装多步神经网络的训练流程：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型训练 </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">u_tensor, model, loss_func, h, M</span>): </span><br><span class="line">    <span class="comment"># 参数说明：训练集u_tensor,待训练模型model,损失函数loss_func,时间步长h,步数M,最大训练次数EPOCH</span></span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=LR)</span><br><span class="line">    loss_history = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCHS):          </span><br><span class="line">        model.train()        </span><br><span class="line">        batch_u = u_tensor.to(device)        </span><br><span class="line">        train_loss = loss_func(batch_u, model, h, M)   </span><br><span class="line">        optimizer.zero_grad()      </span><br><span class="line">        train_loss.backward()    </span><br><span class="line">        optimizer.step()    </span><br><span class="line">        epoch_avg_loss = train_loss.item()         </span><br><span class="line">        loss_history.append(epoch_avg_loss) </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss_history</span><br></pre></td></tr></tbody></table></figure>

<p>其中<code>loss_func</code>即为上述定义的均方误差损失函数MSE：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss_func</span>(<span class="params">u, model, h, M</span>):</span><br><span class="line">	<span class="comment"># 参数说明：模型输入数据u,待训练模型model,时间步长h,步数M</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据选定的方法确定系数α、β（示例：Adams-Moulton法，M=4）</span></span><br><span class="line">    alpha = [<span class="number">1</span>, -<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">    beta = [<span class="number">251</span>/<span class="number">720</span>, <span class="number">646</span>/<span class="number">720</span>, -<span class="number">264</span>/<span class="number">720</span>, <span class="number">106</span>/<span class="number">720</span>, -<span class="number">19</span>/<span class="number">720</span>]</span><br><span class="line">        </span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(M, <span class="built_in">len</span>(u)):</span><br><span class="line">        residual = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> <span class="built_in">range</span>(M+<span class="number">1</span>):</span><br><span class="line">            u_nm = u[n - m].unsqueeze(<span class="number">0</span>)</span><br><span class="line">            f_nm = model(u_nm)</span><br><span class="line">            residual += alpha[m] * u[n - m] + h * beta[m] * f_nm.squeeze()</span><br><span class="line">        loss += torch.norm(residual) ** <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> loss / (<span class="built_in">len</span>(u) - M + <span class="number">1</span>)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="基于多步神经网络改进的常微分方程反演算法"><a href="#基于多步神经网络改进的常微分方程反演算法" class="headerlink" title="基于多步神经网络改进的常微分方程反演算法"></a>基于多步神经网络<strong>改进</strong>的常微分方程反演算法</h3><h4 id="改进的多步神经网络算法"><a href="#改进的多步神经网络算法" class="headerlink" title="改进的多步神经网络算法"></a>改进的多步神经网络算法</h4><p>多步神经网络做出了比较理想的假设，即给出了所有的数值解数据是无噪声的，以及神经网络可以表示对常微分方程产生零残差的反演。这种假设是理想化的，我们可以在实际情况下使用改进的方法尝试发现未知常微分方程。  </p>
<p>对于多步神经网络，除线性多步法残差之外，如果神经网络能够满足一些通用近似性质，尤其是在多步神经网络反演法效果不佳的非自治常微分方程的反演问题中，可以扩展精确和完整数据的收敛结果。考虑到神经网络对函数近似的强大能力（见通用近似定理），这意味着至少在合适的光滑的常微分方程中存在良好的泛化误差。  </p>
<p>假设数据集为给定在若干时刻t_1、t_2、…、t_N的方程的数值解u的值，把每一时刻t_n对应的解u(t_n)记为u^(n)，而t_n对应的导数数据一般无从得知。当时间间隔h较小时，可用<strong>二阶中心差分法</strong>近似求出导数数据，记对于每一时刻t_n求出的导数数据为d^(n)，则：<br>$$<br>d^{(n)}=\begin{cases}(-3\boldsymbol{u}^{(n)}+4\boldsymbol{u}^{(n+1)}-\boldsymbol{u}^{(n+2)})/2h,\quad n=1,\(\boldsymbol{u}^{(n+1)}-\boldsymbol{u}^{(n-1)})/2h,\quad1&lt;n&lt;N,\(\boldsymbol{u}^{(n-2)}-4\boldsymbol{u}^{(n-1)}+3\boldsymbol{u}^{(n)})/2h,\quad n=N.&amp;\end{cases}<br>$$<br>显然二阶中心差分法求导数d^(n)的误差为o(h^2)。</p>
<p>对于常微分方程的反演，我们<strong>改进了多步神经网络的损失函数，其中既包含解数据，又包含导数数据</strong>：<br>$$<br>L=l_1(u,d)+l_2(u,d,f_{NN})<br>$$<br>其中：<br>$$<br>l_{1}=\frac{1}{N-M+1}\sum_{n=M}^{N}\left|\sum_{m=0}^{M}[\alpha_{m}u_{n-m}+h\beta_{m}f_{NN}(u_{n-m})]\right|^{2}<br>$$</p>
<p>$$<br>l_{2}=\frac{1}{N-M+1}\sum_{n=M}^{N}\left|f_{NN}(u_{n})-d_{n}\right|^{2}<br>$$</p>
<p>$$<br>n=M,\ldots,N<br>$$</p>
<p>式中向量范数采用二范数；u为数值解的数据，d为二阶中心差分法得到的导数数据，则有：</p>
<ul>
<li><strong>l_1</strong>：<strong>线性多步法残差</strong>，可以表示常微分方程的近似程度</li>
<li><strong>l_2</strong>：<strong>均方误差函数</strong>，可以表示网络结构的准确度</li>
</ul>
<p>把u^(n)作为输入，d^(n)作为期望输出进行训练，得到的f_NN即为对函数f的近似。训练流程与改进后的损失函数Python实现如下所示：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型训练 </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">u_tensor, model, loss_func, h, M</span>): </span><br><span class="line">    <span class="comment"># 参数说明：训练集u_tensor,待训练模型model,损失函数loss_func,时间步长h,步数M,最大训练次数EPOCH</span></span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=LR)</span><br><span class="line">    loss_history = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCHS):</span><br><span class="line">        model.train()</span><br><span class="line">        batch_u = u_tensor.to(device)</span><br><span class="line">        batch_d = d_tensor.to(device)</span><br><span class="line">        train_loss = loss_func(batch_u, batch_d, model, h, M)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        train_loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        epoch_avg_loss = train_loss.item()</span><br><span class="line">        loss_history.append(epoch_avg_loss)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> loss_history</span><br></pre></td></tr></tbody></table></figure>

<p>可以看到，与改进前相比，在训练过程中增加了对二阶中心差分法得到的导数的相关计算，进一步利用了训练数据中的信息。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss_func</span>(<span class="params">u, d, model, h, M</span>):</span><br><span class="line">    <span class="comment"># 参数说明：模型输入数据u,输入数据对应的真实微分数据d,待训练模型model,时间步长h,步数M</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据选定的方法确定系数α、β（示例：Adams-Moulton法，M=4）</span></span><br><span class="line">    alpha = [<span class="number">1</span>, -<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">    beta = [<span class="number">251</span>/<span class="number">720</span>, <span class="number">646</span>/<span class="number">720</span>, -<span class="number">264</span>/<span class="number">720</span>, <span class="number">106</span>/<span class="number">720</span>, -<span class="number">19</span>/<span class="number">720</span>]</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 多步法残差 l1</span></span><br><span class="line">    l1 = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(M, <span class="built_in">len</span>(u)):</span><br><span class="line">        residual = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> <span class="built_in">range</span>(M+<span class="number">1</span>):</span><br><span class="line">            u_nm = u[n - m].unsqueeze(<span class="number">0</span>)</span><br><span class="line">            f_nm = model(u_nm)</span><br><span class="line">            residual += alpha[m] * u[n - m] + h * beta[m] * f_nm.squeeze()</span><br><span class="line">        l1 += torch.norm(residual) ** <span class="number">2</span></span><br><span class="line">    l1 = l1 / (<span class="built_in">len</span>(u) - M + <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 导数拟合误差 l2</span></span><br><span class="line">    pred_d = model(u)</span><br><span class="line">    l2 = torch.mean(torch.norm(pred_d - d, dim=<span class="number">1</span>) ** <span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> l1 + l2  <span class="comment"># 总损失</span></span><br></pre></td></tr></tbody></table></figure>

<p>值得注意的是，针对非自治常微分方程，由于在将其变换为自治常微分方程时进行了升维操作，这意味着真实的导数向量在y=t这一分量上的值必然是1，因此后续在检验模型训练效果时可利用这一点对模型性能进行先决的把握。</p>
<h4 id="基于误差界理论论证改进算法的优越性"><a href="#基于误差界理论论证改进算法的优越性" class="headerlink" title="基于误差界理论论证改进算法的优越性"></a>基于误差界理论论证改进算法的优越性</h4><p>在上述损失函数中，l_1是对目标函数的良好近似，而l_2是神经网络中常用的损失函数。考虑到神经网络对函数近似的强大能力，这意味着即使对于噪声数据，改进的方法也应有良好的泛化能力和鲁棒性。在这里，我们先从理论上来论证改进方法在泛化能力与鲁棒性上的优越性，后续的数值实验部分将采用三类非自治方程的反演来验证改进的多步神经网络的效果。</p>
<p>对于二分类问题（可推广至函数近似的回归问题），当假设空间是有限个函数的集合F={f_1,f_2,…,f_d}时，对任意一个函数f∈F,至少以概率1-δ，以下不等式成立：<br>$$<br>R(f)\leq\hat{R}(f)+\varepsilon(d,N,\delta)<br>$$<br>其中：<br>$$<br>\begin{gathered}R(f)=E[L(Y,f(X))]\\hat{R}(f)=\frac{1}{N}\sum_{1}^{N}L(y_{i},f(x_{i}))\\varepsilon(d,N,\delta)=\sqrt{\frac{1}{2N}(\log d+\log\frac{1}{\delta})}\end{gathered}<br>$$<br>式中，R(f)为泛化误差（测试集上的测试风险），\hat{R}(f)为训练集上的经验风险，\hat{R}(f) + ε(d,N,δ)即为泛化误差界。观察上式可知，泛化误差界与样本数N成正比，与假设空间包含的函数数量d成反比。因此，当样本数N越大，泛化误差界越小，当假设空间F包含的函数越多，泛化误差界越大。</p>
<p>根据上述定理，针对改进后的神经网络，有如下不等式成立：<br>$$<br>\left|f_{NN}(u^{(n)})-\hat{f}(u^{(n)})\right|\leq\left|f_{NN}(u^{(n)})-d^{(n)}\right|+\left|\hat{f}(u^{(n)})-d^{(n)}\right|\leq w+o(h^{2})<br>$$<br>该不等式表明，改进后的损失函数可以拆解成两部分误差：</p>
<ul>
<li>第一部分为通过神经网络得到的导数近似值与通过二阶中心差分法求出的导数之间的误差，设该误差限为<strong>w</strong>；</li>
<li>第二部分为导数真实值与通过二阶中心差分法求出的导数之间的误差，根据二阶中心差分法的基本原理，其误差限为**o(h^2)**。</li>
</ul>
<p>显然改进后的多步神经网络的泛化误差相较改进前有了进一步的约束，尤其显著减小了<strong>噪声</strong>对模型的影响，具有<strong>更强的泛化能力和鲁棒性</strong>。</p>
<h2 id="数值实验与分析"><a href="#数值实验与分析" class="headerlink" title="数值实验与分析"></a>数值实验与分析</h2><p>为充分验证改进方法的泛化能力与鲁棒性，我们选取了六个实际的物理问题（对应常微分方程）作为测试用例，对应自治/非自治以及三种不同的线性多步法，在每一个测试用例内选取不同的线性多步法步数以及训练数据所含的高斯噪声方差，对改进前后的两种基于多步神经网络的常微分方程反演方法进行测试。下面我们以“受迫振动方程”这一用例详细展示测试流程，其他的测试用例仅作结果展示。</p>
<h3 id="数值实验流程"><a href="#数值实验流程" class="headerlink" title="数值实验流程"></a>数值实验流程</h3><h4 id="数据构建"><a href="#数据构建" class="headerlink" title="数据构建"></a>数据构建</h4><p>基于选定的测试用例对应的常微分方程，可采用如下步骤生成原始的真实数据集：</p>
<ol>
<li>在所给区域内均匀选取间隔为h的N个结点t_n,n=1,2,…,N ,使用Python中的Scipy库求出各结点对应的数值解u^(n)；</li>
<li>将数值解u^(n) 代入二阶中心差分法中得到每个x_n 对应的导数值d^(n) ；</li>
<li>噪声数据构建：在各结点对应的数值解u^(n) 上依次加上<strong>期望为0，方差为0、0.01、0.05的高斯噪声</strong>。</li>
</ol>
<p>在Python中，用函数<code>get_data()</code>函数实现了数据构建的代码封装：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_data</span>(<span class="params">t0, t_end, h, y0, noise_std=<span class="number">0.0</span></span>):     </span><br><span class="line">    t_eval = np.arange(t0, t_end + h, h)     </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 数值解    </span></span><br><span class="line">    sol = solve_ivp(forced_oscillator, [t0, t_end], y0, t_eval=t_eval)    </span><br><span class="line">    x = sol.y.T  <span class="comment"># shape: [N, 2]     </span></span><br><span class="line">    t = sol.t.reshape(-<span class="number">1</span>, <span class="number">1</span>)  <span class="comment"># shape: [N, 1]     </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 添加高斯噪声     </span></span><br><span class="line">    <span class="keyword">if</span> noise_std &gt; <span class="number">0</span>:         </span><br><span class="line">        x += np.random.normal(<span class="number">0</span>, noise_std, size=x.shape)      </span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 构造 u = [x1, x2, t]     </span></span><br><span class="line">    u = np.hstack([x, t])      </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 二阶中心差分求导数 d ≈ du/dt     </span></span><br><span class="line">    d = np.zeros_like(u)     </span><br><span class="line">    <span class="comment"># 边界点使用前向/后向差分     </span></span><br><span class="line">    d[<span class="number">0</span>] = (-<span class="number">3</span> * u[<span class="number">0</span>] + <span class="number">4</span> * u[<span class="number">1</span>] - u[<span class="number">2</span>]) / (<span class="number">2</span> * h)     </span><br><span class="line">    d[-<span class="number">1</span>] = (u[-<span class="number">3</span>] - <span class="number">4</span> * u[-<span class="number">2</span>] + <span class="number">3</span> * u[-<span class="number">1</span>]) / (<span class="number">2</span> * h)     </span><br><span class="line">    <span class="comment"># 内部点使用中心差分     </span></span><br><span class="line">    d[<span class="number">1</span>:-<span class="number">1</span>] = (u[<span class="number">2</span>:] - u[:-<span class="number">2</span>]) / (<span class="number">2</span> * h)      </span><br><span class="line">    </span><br><span class="line">    u_tensor = torch.tensor(u, dtype=torch.float32)     </span><br><span class="line">    d_tensor = torch.tensor(d, dtype=torch.float32)      </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> u_tensor, d_tensor, t<span class="comment"># 模型训练 </span></span><br></pre></td></tr></tbody></table></figure>

<p>以“受迫振动方程”这一测试框架为例，<code>force_oscillator</code>函数中刻画了该场景下的微分方程规律：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forced_oscillator</span>(<span class="params">t, y</span>):</span><br><span class="line">    x1, x2 = y</span><br><span class="line">    dx1dt = x2</span><br><span class="line">    dx2dt = -np.cos(t) * x1 - x2 + t / <span class="number">50</span></span><br><span class="line">    <span class="keyword">return</span> [dx1dt, dx2dt]</span><br></pre></td></tr></tbody></table></figure>

<p>生成的无噪声数据如下图所示：</p>
<p><img src="/images/project3/2.png" alt="受迫振动方程数据构建x1"></p>
<p><img src="/images/project3/3.png" alt="受迫振动方程数据构建x2"></p>
<h4 id="神经网络训练"><a href="#神经网络训练" class="headerlink" title="神经网络训练"></a>神经网络训练</h4><p>根据上述算法理论部分列出的训练流程框架，适用数据（u，d）对神经网络进行训练，训练后得到的函数f_NN即为对函数f的近似。</p>
<p>关于神经网络的结构与参数，本项目中所使用的<strong>神经网络结构</strong>均相同（后续不再赘述），均<strong>包含4个隐藏层，每层128个神经元，激活函数为tanh</strong>，Python中基于pytorch框架对神经网络架构的实现如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ODEApproximator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim=<span class="number">3</span>, hidden_dim=<span class="number">128</span>, output_dim=<span class="number">3</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(ODEApproximator, self).__init__()</span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Linear(input_dim, hidden_dim),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(hidden_dim, hidden_dim),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(hidden_dim, hidden_dim),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(hidden_dim, hidden_dim),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(hidden_dim, output_dim)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.net(x)</span><br></pre></td></tr></tbody></table></figure>

<p>训练过程中实时记录了每一个epoch的损失函数值，下图分别绘制了改进前后的多步神经网络在无噪声数据集上训练过程中损失函数值的变化情况（总Epoch=100），可以看到损失函数均呈收敛态势，且改进后的多步神经网络收敛更快：</p>
<p><img src="/images/project3/4.png" alt="改进前多步神经网络训练过程损失函数值变化"></p>
<p><img src="/images/project3/5.png" alt="改进后多步神经网络训练过程损失函数值变化"></p>
<h4 id="效果检验"><a href="#效果检验" class="headerlink" title="效果检验"></a>效果检验</h4><p>为定量地比较改进前后的多步神经网络的泛化性能与鲁棒性，采取如下的实验步骤进行训练后模型的效果检验：</p>
<ol>
<li>用训练得到的函数f_NN解方程得到数值解u_NN^(n) ,观察近似效果，计算均方误差mse_x 和平均绝对误差mae_x并与多步神经网络作比较；</li>
<li>用数值解u^(n)代入训练得到的函数f_NN得到的导数的近似值d_NN^(n)，将u^(n)代入给定的常微分方程中的 f 得到导数的真实值d^(n),计算均方误差mse_f和平均绝对误差mae_f并与多步神经网络作比较。</li>
</ol>
<p>其中用于比较性能的指标均方误差mse和平均绝对误差mae的定义式如下所示：<br>$$<br>mse_{x}=\frac{1}{ND}\sum_{n=1}^{N}\left|u_{NN}^{(n)}-u^{(n)}\right|_{2}^{2}<br>$$</p>
<p>$$<br>mse_{f}=\frac{1}{ND}\sum_{n=1}^{N}\left|d_{NN}^{(n)}-d^{(n)}\right|_{2}^{2}<br>$$</p>
<p>$$<br>mae_{x}=\frac{1}{ND}\sum_{n=1}^{N}\left|u_{NN}^{(n)}-u^{(n)}\right|_{1}<br>$$</p>
<p>$$<br>mae_{f}=\frac{1}{ND}\sum_{n=1}^{N}\left|d_{NN}^{(n)}-d^{(n)}\right|_{1}<br>$$</p>
<p>Python中将利用训练好的模型进行预测的过程封装成函数<code>predict_u</code>，指标计算直接调用scikit-learn函数库中的相关函数：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">rhs</span>(<span class="params">t, u_np</span>):      </span><br><span class="line">    <span class="string">"""用于 solve_ivp：输入 u = [x1, x2, t] 输出 du/dt"""</span>      </span><br><span class="line">    u_tensor = torch.tensor(u_np, dtype=torch.float32).unsqueeze(<span class="number">0</span>).to(device)      </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():          </span><br><span class="line">        du_dt = model(u_tensor).squeeze().cpu().numpy()      </span><br><span class="line">    <span class="keyword">return</span> du_dt    </span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict_u</span>(<span class="params">u_tensor</span>):      </span><br><span class="line">    <span class="comment"># 初始条件与时间点一致     </span></span><br><span class="line">    u0 = u_tensor[<span class="number">0</span>].cpu().numpy()  <span class="comment"># [x1, x2, t]    </span></span><br><span class="line">    t_span = (<span class="number">0</span>, <span class="number">100</span>)  </span><br><span class="line">    t_eval = np.arange(<span class="number">0</span>, <span class="number">100</span> + <span class="number">0.05</span>, <span class="number">0.05</span>)    </span><br><span class="line">    </span><br><span class="line">    sol_nn = solve_ivp(rhs, t_span, u0, t_eval=t_eval)   </span><br><span class="line">    <span class="keyword">return</span> sol_nn.y.T  <span class="comment"># shape: [N, 3]</span></span><br></pre></td></tr></tbody></table></figure>

<p>在“受迫振动方程”这一测试案例中，基于训练轮数Epoch=50的模型，运行以上检验流程，得到如下检验指标数据：</p>
<p><img src="/images/project3/6.png" alt="Epoch=50时数值解预测误差"></p>
<p>可以看到，随着<strong>噪声强度增加</strong>，两种方法的<strong>误差均增大</strong>，但<strong>改进方法对数值解的预测误差在各种情况下均小于多步神经网络</strong>。 </p>
<p><img src="/images/project3/7.png" alt="Epoch=50时导数值预测误差"></p>
<p>可以看到，当数据无噪声时，本文提出的<strong>改进方法</strong>对于受迫振动方程的<strong>近似效果强</strong>于多步神经网络;随着<strong>噪声强度的增加</strong>，<strong>多步神经网络的误差大幅增加</strong>，而<strong>改进的方法的误差一直保持较小</strong>且始终强于多步神经网络。</p>
<p>除此之外，针对在无噪声数据集上训练轮数分别为Epoch=100和Epoch=500时的模型，将其作为微分方程中的f用于预测数据的生成，效果分别如下图所示：</p>
<p><img src="/images/project3/8.png" alt="Epoch=100时预测效果与真实值对比"></p>
<p><img src="/images/project3/9.png" alt="Epoch=500时预测效果与真实值对比"></p>
<p>可以看到，随着训练轮数的增加，模型对于实际微分方程的拟合效果越来越好，生成的预测数据也越来越接近用于训练的真实数据；事实上，从理论上讲，大概在训练轮数Epoch达到10^4数量级时才会有较为精准的拟合效果，本项目由于用于训练的计算资源有限，仅进行了最大训练轮数为1000的测试。在后续的结果分析部分，将直接采用高训练轮数下的精确结果进行展示。</p>
<h3 id="数值实验结果分析"><a href="#数值实验结果分析" class="headerlink" title="数值实验结果分析"></a>数值实验结果分析</h3><h4 id="非自治常微分方程"><a href="#非自治常微分方程" class="headerlink" title="非自治常微分方程"></a>非自治常微分方程</h4><h5 id="受迫振动方程——Adams-Moulton法"><a href="#受迫振动方程——Adams-Moulton法" class="headerlink" title="受迫振动方程——Adams-Moulton法"></a>受迫振动方程——Adams-Moulton法</h5><p>受迫振动方程(Forced Oscillator)是描述一个振动系统在外界力的作用下进行振动的方程。这个方程可以用来研究各种物理系统中的振动现象，例如弹簧振子、摆锤、电路振荡等。受迫振动方程的核心思想是将外界力引入方程中，以描述振动系统在外界激励下的行为。受迫振动方程的研究对于科学、工程和技术领域具有重要意义。它不仅有助于我们深入理解振动现象的本质和规律，还为我们设计和优化振动控制系统、减振装置等提供了理论基础。同时，它在电子设备、结构工程、交通运输等方面也有广泛的应用。</p>
<p>实验使用的受迫振动方程如下：<br>$$<br>\begin{aligned}&amp;\frac{dx_1}{dt}=x_2,\&amp;\frac{dx_2}{dt}=-cosy\cdot x_1-x_2+\frac{y}{50},\&amp;\frac{dy}{dt}=1\end{aligned}<br>$$<br>使用[0,1,0]T作为初值，生成从t=0到t=100，间隔h=0.05的数据作为训练集，最终得到的多步神经网络对常微分方程的反演效果如下图所示，其中红色曲线是原方程数值解的曲线，蓝色点是用无噪声数据训练的函数f_NN代替f得到的数值解:</p>
<p><img src="/images/project3/10.png" alt="受迫振动方程反演效果"></p>
<h5 id="线性标量方程——Adams-Bashforth法"><a href="#线性标量方程——Adams-Bashforth法" class="headerlink" title="线性标量方程——Adams-Bashforth法"></a>线性标量方程——Adams-Bashforth法</h5><p>线性标量方程(Linear Scalar Equation)是一种描述某个未知函数与其导数之间的关系的方程，其中未知函数的导数的最高次数为1。线性标量方程是微分方程中的一类重要问题，它在数学物理等领域中有广泛应用。线性标量方程的研究对于数学、物理学和工程学等领域具有重要意义。它可以用来描述动力学系统的行为、传热和传质过程、量子力学中的波函数演化等。通过分析和求解线性标量方程，可以深入理解系统的特征、稳定性、渐近行为等，为问题的模拟、控制和优化提供理论基础。在现代科学和工程中，线性标量方程的研究得到了进一步推广和延伸。例如，非线性标量方程、偏微分方程等是线性标量方程的扩展和推广，它们更好地描述了一些复杂的物理现象和现实问题。</p>
<p>实验使用的线性标量方程如下：<br>$$<br>\frac{dx_1}{dt}=-x_1(sin4y+1)+cos\frac{y^2}{1000},<br>$$</p>
<p>$$<br>\frac{dy}{dt}=1<br>$$</p>
<p>使用[2,0]T作为初值，生成从t=0到t=20，间隔h=0.05的数据作为训练集，最终得到的多步神经网络对常微分方程的反演效果如下图所示，其中红色曲线是原方程数值解的曲线，蓝色点是用无噪声数据训练的函数f_NN代替f得到的数值解:</p>
<p><img src="/images/project3/11.png" alt="线性标量方程反演效果"></p>
<h5 id="食饵-捕食者模型方程——后向微分公式法"><a href="#食饵-捕食者模型方程——后向微分公式法" class="headerlink" title="食饵-捕食者模型方程——后向微分公式法"></a>食饵-捕食者模型方程——后向微分公式法</h5><p>食饵-捕食者模型(Predator-prey Model)是生态学中研究食物链和生物群落动力学的重要模型之一，描述了食物链中食饵(被捕食者)和捕食者之间的相互作用关系。这种模型的研究对于我们理解生态系统的稳定性、物种相互作用以及生态系统中物种数量的变化具有重要意义。通过食饵-捕食者模型，我们可以研究食饵与捕食者之间的数量关系以及它们之间的相互作用。一般来说，食饵的数量被认为是捕食者的食物来源，并且捕食者的数量受到食饵的供给和捕食行为的影响。该模型通常描述了食饵数量随时间的变化，以及捕食者数量随时间的变化。食饵-捕食者模型的研究对于生态学、环境保护和资源管理等领域具有重要意义。通过该模型，我们可以深入了解生态系统中物种数量的变化规律，从而预测物种灭绝和生态系统崩溃的风险，以及寻找保护和管理生物多样性的方法。</p>
<p>实验使用的食饵-捕食者模型方程如下：<br>$$<br>\begin{aligned}&amp;\frac{dx_1}{dt}=x_1-x_1\cdot x_2+sin\frac{y}{2}+cosy+2,\&amp;\frac{dx_2}{dt}=x_1\cdot x_2-x_2,\&amp;\frac{dy}{dt}=1\end{aligned}<br>$$<br>使用[3,3,0]T作为初值，生成从t=0到t=50，间隔h=0.05的数据作为训练集，最终得到的多步神经网络对常微分方程的反演效果如下图所示，其中红色曲线是原方程数值解的曲线，蓝色点是用无噪声数据训练的函数f_NN代替f得到的数值解:</p>
<p><img src="/images/project3/12.png" alt="食饵-捕食者模型方程反演效果"></p>
<h4 id="自治常微分方程"><a href="#自治常微分方程" class="headerlink" title="自治常微分方程"></a>自治常微分方程</h4><h5 id="线性常微分方程——Adams-Moulton法"><a href="#线性常微分方程——Adams-Moulton法" class="headerlink" title="线性常微分方程——Adams-Moulton法"></a>线性常微分方程——Adams-Moulton法</h5><p>线性常微分方程(Linear ODEs)是微分方程中的一类重要问题，它描述了未知函数及其导数之间的线性关系。线性常微分方程的研究对于数学、物理学和工程学等领域具有重要意义。它们出现在物理学中许多基础问题的数学描述中，例如弹簧振动、电路分析、传热过程等。在工程学中，线性常微分方程也广泛应用于控制系统的建模和分析。随着科学技术的发展，研究者提出了各种各样的数值方法和近似方法，以求解复杂的线性常微分方程。这些方法包括欧拉方法、龙格-库塔方法、有限差分方法、变分法等，它们为实际问题的求解提供了有效的数值工具。随着对非线性动力学系统的研究和认识的深入，研究者们开始将非线性常微分方程引入到线性常微分方程中，以描述更为复杂的现象和现实问题。</p>
<p>实验使用的线性常微分方程如下：<br>$$<br>\frac{dx_1}{dt}=x_1-4x_2,<br>$$</p>
<p>$$<br>\frac{dx_2}{dt}=4x_1-7x_2<br>$$</p>
<p>使用[0,-1]T作为初值，生成从t=0到t=10，间隔h=0.01的数据作为训练集，最终得到的多步神经网络对常微分方程的反演效果如下图所示，其中红色曲线是原方程数值解的曲线，蓝色点是用无噪声数据训练的函数f_NN代替f得到的数值解:</p>
<p><img src="/images/project3/13.png" alt="线性常微分方程反演效果"></p>
<h5 id="阻尼三次振子方程——Adams-Bashforth法"><a href="#阻尼三次振子方程——Adams-Bashforth法" class="headerlink" title="阻尼三次振子方程——Adams-Bashforth法"></a>阻尼三次振子方程——Adams-Bashforth法</h5><p>阻尼三次振子(Damped Cubic Oscillator)是一种物理系统，它描述了受到阻尼力和弹力作用的振动系统。阻尼指的是系统中存在能量损耗的因素，如摩擦力，它会导致振动的逐渐减弱；三次振子表示系统的势能函数是一个三次函数，它具有非线性的特性。阻尼三次振子最早出现在振动力学和非线性动力学的研究中。对于振动系统的研究是物理学、工程学和应用数学等领域的重要课题之一。阻尼三次振子的研究对于理解和预测复杂动力学系统的行为具有重要意义。通过分析阻尼三次振子的运动规律和稳定性，我们可以深入了解非线性振动系统的动力学特性，例如振动的周期性、混沌行为等。这些研究也为控制系统、电子电路、力学系统等领域的工程应用提供了参考和启示。 </p>
<p>实验使用的阻尼三次振子方程如下：<br>$$<br>\frac{dx_1}{dt}=-0.1x_1^3+2x_2^3,<br>$$</p>
<p>$$<br>\frac{dx_2}{dt}=-2x_1^3-0.1x_2^3<br>$$</p>
<p>使用[1,1]T作为初值，生成从t=0到t=25，间隔h=0.01的数据作为训练集，最终得到的多步神经网络对常微分方程的反演效果如下图所示，其中红色曲线是原方程数值解的曲线，蓝色点是用无噪声数据训练的函数f_NN代替f得到的数值解:</p>
<p><img src="/images/project3/14.png" alt="阻尼三次振子方程反演效果"></p>
<h5 id="阻尼简谐摆方程——后向微分公式法"><a href="#阻尼简谐摆方程——后向微分公式法" class="headerlink" title="阻尼简谐摆方程——后向微分公式法"></a>阻尼简谐摆方程——后向微分公式法</h5><p>阻尼简谐摆(Damped Pendulum)是一个经典力学中的物理系统，它由一个具有质量的物体通过一根轻质绳或杆与一个固定支点相连接组成。阻尼简谐摆在受到重力作用下，沿着一条弧线进行周期性振动。阻尼简谐摆的研究对于理解和预测振动系统的行为具有重要意义。通过分析阻尼简谐摆的运动规律和稳定性，我们可以深入了解振动系统在存在阻尼时的响应特性，例如振动的振幅、频率等。这些研究不仅在理论物理学中具有重要意义，也为工程学中的控制系统、机械振动和结构动力学等领域的应用提供了基础。</p>
<p>实验使用的阻尼简谐摆方程如下：<br>$$<br>\begin{aligned}&amp;\frac{dx_1}{dt}=x_2,\&amp;\frac{dx_2}{dt}=-\alpha x_2-\beta\sin x_1,\&amp;\alpha=0.2,\beta=8.91\end{aligned}<br>$$<br>使用[-1.193,-3.876]T作为初值，生成从t=0到t=20，间隔h=0.01的数据作为训练集，最终得到的多步神经网络对常微分方程的反演效果如下图所示，其中红色曲线是原方程数值解的曲线，蓝色点是用无噪声数据训练的函数f_NN代替f得到的数值解:</p>
<p><img src="/images/project3/15.png" alt="阻尼简谐摆方程反演效果"></p>
<h2 id="研究结论与创新"><a href="#研究结论与创新" class="headerlink" title="研究结论与创新"></a>研究结论与创新</h2><h3 id="研究结论"><a href="#研究结论" class="headerlink" title="研究结论"></a>研究结论</h3><p>本项目<strong>针对常微分方程的反演问题提出了一种基于多步神经网络改进的常微分方程反演算法</strong>，旨在<strong>从数据中提取内含的常微分方程</strong>。通过<strong>将导数数据融入训练数据</strong>，我们改进了多步神经网络的数据集，并<strong>优化了神经网络的损失函数</strong>，以<strong>提高模型的准确性和泛化能力</strong>。经过训练，改进的神经网络成功拟合原方程中的函数，实现了对非自治常微分方程的反演。我们还推导并讨论了改进算法的误差界。我们进一步将改进的算法应用于自治常微分方程反演，通过训练网络拟合原方程函数，拓展了算法在自治常微分方程反演中的适用性。  </p>
<p>本项目通过实验以非自治常微分方程中的受迫振动方程、线性标量方程和食饵-捕食者模型方程，以及自治常微分方程中的线性常微分方程、阻尼三次振子方程和阻尼简谐摆方程为例，展示了算法的有效性。在实验中，我们添加了不同强度的噪声，并利用改进算法对常微分方程进行反演。实验结果表明，<strong>改进方法在大多数情况下优于多步神经网络，在处理有噪声数据时表现更为出色</strong>。这些结果充分验证了本文提出的方法在反演问题中的可行性和优越性。</p>
<h3 id="研究方法创新性"><a href="#研究方法创新性" class="headerlink" title="研究方法创新性"></a>研究方法创新性</h3><p>传统方法通常利用多步神经网络来处理常微分方程的反演问题，主要集中在处理自治常微分方程，对于非自治常微分方程的反演效果不尽理想，特别是在使用带有噪声数据进行训练时表现不佳。本研究首次将研究重点聚焦于非自治常微分方程的反演，通过<strong>将非自治方程转化为自治方程</strong>的形式，提出了一种新的研究思路。针对带有噪声数据的方程反演问题，采用了<strong>改进损失函数</strong>的方法以提升算法的鲁棒性。此外，将这种新方法扩展应用于自治常微分方程的反演问题，结果显示其反演效果明显优于多步神经网络。这一新思路对解决常微分方程反演问题带来了一种新的方法。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] 王开荣,杨大地编著.应用数值分析[M].高等教育出版社,2010.</p>
<p>[2] 付长铠.常微分方程反演的机器学习方法研究[D].长春工业大学,2024.</p>
<p>[3] Raissi M, Perdikaris P, Karniadakis G E. Multistep neural networks for data-driven discovery of nonlinear dynamical systems[J]. arXiv preprint arXiv:1801.01236, 2018.</p>
<p>[4] 李航. 统计学习方法[M]. 第二版. 北京：清华大学出版社, 2019.</p>
<p>[5] 陈新海, 刘杰, 万仟, 等. 一种改进的基于深度神经网络的偏微分方程求解方法[J]. 计算机工程与科学, 2022, 44(11): 1932-1940.</p>
</div></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2025-04-22T18:07:03.000Z" title="2025/4/23 02:07:03">2025-04-23</time>发表</span><span class="level-item"><time datetime="2025-04-29T15:58:08.821Z" title="2025/4/29 23:58:08">2025-04-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/">课程项目</a><span>&nbsp;/&nbsp;</span><a class="link-muted" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E5%B7%A5%E7%A8%8B%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90/">工程数值分析</a></span><span class="level-item">1 小时读完 (大约8883个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2025/04/23/project02/">图像插值算法及其优化</a></p><div class="content"><div id="postchat_postcontent"><h2 id="研究背景及其意义"><a href="#研究背景及其意义" class="headerlink" title="研究背景及其意义"></a>研究背景及其意义</h2><p>图像<strong>放大</strong>与<strong>旋转</strong>是数字图像处理中最基础的几何变换操作，其核心在于如何通过插值算法重建原始图像中不存在的像素信息。当对图像进行放大操作时，输出图像的像素网格会超出原始图像的采样范围，需要通过插值来填补这些新增像素点的颜色值；而在旋转操作中，即使保持图像尺寸不变，原始像素的整数坐标经过旋转变换后也会落在新图像的非整数位置，同样需要通过插值来重新确定每个输出像素的颜色值。</p>
<p>图像插值是利用原图像中的颜色值通过一定的方法计算出待插入像素点的颜色值的过程。对图像进行插值一般有两个步骤：首先定义一个图像插值公式，然后利用该插值公式计算待插入点的颜色值。常见的图像插值算法有双线性法、最近邻法、非均匀法、双三次卷积插值法、双立方法、Lagrange法、 样条插值法、 克里金（Krijing） 插值法等。这些插值方法通常定义一个插值数据点的隐式函数，再提取该函数的等值面作为图像插值方法，常用的插值核包括线性插值核、样条插值核等。</p>
<ul>
<li><strong>最近邻插值</strong>作为最简单的算法，直接将距离待插值点最近的已知像素值作为结果，虽然计算效率极高（时间复杂度O(1)），但会产生明显的块状伪影（“马赛克”）和锯齿形边缘；</li>
<li><strong>双线性插值</strong>通过考虑2×2邻域内四个像素的加权平均，在计算成本（O(n)）和视觉效果之间取得平衡，但仍会导致高频信息丢失和边缘模糊；</li>
<li>更高阶的<strong>双三次插值</strong>（使用4×4邻域）和样条插值虽然能提供更平滑的结果，但计算复杂度显著增加（O(n²)），且可能引入不必要的振铃效应。</li>
</ul>
<p>现有算法的根本<strong>局限</strong>在于<strong>采用统一的插值核函数处理整幅图像，忽视了图像不同区域的特征差异</strong>。例如，在平坦区域使用复杂插值会造成计算资源浪费，而在纹理丰富区域使用简单插值又会导致细节损失。基于此，我们希望通过改良的<strong>四平面插值</strong>算法对图像的放大与旋转效果进行优化，<strong>根据图像局部特征自适应地选择不同的插值策略</strong>，以规避用同一个插值公式对所有像素进行插值存在的不足。</p>
<h2 id="常用图像插值算法"><a href="#常用图像插值算法" class="headerlink" title="常用图像插值算法"></a>常用图像插值算法</h2><p>课本在6.5节中提到，在插值节点数量较多时，为避免Runge振荡现象的发生，并不提倡用高次多项式进行插值，而宁可用低次多项式作分段插值。在图像处理这一特定的应用场景中，需要处理的图像尺寸规模往往较大，且同一行（列）的所有像素颜色值显然并不具有可以用一个多项式函数显式表达的规律，但相邻的像素点颜色值之间又存在一定的关联性，因此分段插值仅考虑局部特征的特性在这里能够良好地契合所需性能。根据对于待插入像素点周围已有的像素点信息的利用情况，这里列举了几种常见的图像插值算法：</p>
<ul>
<li>最近邻法：仅利用待插值像素点转换至原图像坐标后距离其最近的一个像素点的颜色值，将其直接作为待插值像素点的颜色值</li>
<li>双线性法：利用待插值像素点转换至原图像坐标后距离其最近的四个像素点的颜色值，加权平均后作为待插值像素点的颜色值</li>
<li>双立方法：利用待插值像素点转换至原图像坐标后距离其最近的十六个像素点的颜色值，加权平均后作为待插值像素点的颜色值</li>
</ul>
<h3 id="最近邻法"><a href="#最近邻法" class="headerlink" title="最近邻法"></a>最近邻法</h3><p><img src="/images/project2/3.png" alt="一维最近邻插值示意图"></p>
<p>如上图所示，在一维最近邻插值中，坐标轴上各点 xi-1，xi，xi+1 … 两两对半等分间隔 (红色虚线划分)，从而非边界的各坐标点都有一个等宽的邻域，并根据每个坐标点的值构成一个类似分段函数的函数约束，从而使各插值坐标点的值等同于所在邻域原坐标点的值。例如，插值点 x 坐落于 坐标点 xi 的邻域，那么其值 f(x) 就等于 f(xi)。</p>
<p>在二维的图像插值场景中，可以对上述一维最近邻插值进行推广，如下图所示：</p>
<p><img src="/images/project2/4.png" alt="二维最近邻插值示意图"></p>
<p>可以看到，(x0, y0)、(x0, y1)、(x1, y0)、(x1, y1) 都是原图像上的坐标点，颜色值分别对应为 Q11、Q12、Q21、Q22。而颜色值未知的插值点 (x, y)（需转换至原图像坐标），根据最近邻插值方法的约束，其与坐标点 (x0, y0) 位置最接近 (即位于  (x0, y0) 的邻域内)，故插值点 (x, y) 的颜色值 P = Q11。</p>
<p>总而言之，最近邻法的基本思想即：<strong>将待插入点的坐标进行四舍五入，再以该行列坐标都是整数点的颜色值（灰度值）替代待插入点(x, y)处的颜色值。</strong>事实上，这也正是机器学习中KNN（K-Nearest Neighbor）算法在K=1时的情形。</p>
<p>基于以上算法思想，编写python函数代码实现图像放缩与旋转过程中的最近邻法插值：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 最近邻法插值实现图像放缩</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">nearest_neighbor_interpolation</span>(<span class="params">image, scale_factor</span>):</span><br><span class="line">    h, w, channel = image.shape</span><br><span class="line">    new_h, new_w = <span class="built_in">int</span>(h * scale_factor), <span class="built_in">int</span>(w * scale_factor)</span><br><span class="line">    resized_image = np.zeros((new_h, new_w, <span class="built_in">int</span>(channel)), dtype=image.dtype)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(new_h):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(new_w):</span><br><span class="line">            src_i = <span class="built_in">int</span>(<span class="built_in">round</span>((i + <span class="number">1</span>) / scale_factor, <span class="number">0</span>))</span><br><span class="line">            src_j = <span class="built_in">int</span>(<span class="built_in">round</span>((j + <span class="number">1</span>) / scale_factor, <span class="number">0</span>))</span><br><span class="line">            resized_image[i, j] = image[src_i - <span class="number">1</span>, src_j - <span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> resized_image</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最近邻法插值实现图像旋转</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">nearest_neighbor_rotation</span>(<span class="params">image, angle</span>):</span><br><span class="line">    h, w, channel = image.shape</span><br><span class="line">    angle_rad = math.radians(angle)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算旋转后的图像尺寸</span></span><br><span class="line">    cos_theta = <span class="built_in">abs</span>(math.cos(angle_rad))</span><br><span class="line">    sin_theta = <span class="built_in">abs</span>(math.sin(angle_rad))</span><br><span class="line">    new_w = <span class="built_in">int</span>(h * sin_theta + w * cos_theta)</span><br><span class="line">    new_h = <span class="built_in">int</span>(h * cos_theta + w * sin_theta)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 旋转中心</span></span><br><span class="line">    cx, cy = w / <span class="number">2</span>, h / <span class="number">2</span></span><br><span class="line">    new_cx, new_cy = new_w / <span class="number">2</span>, new_h / <span class="number">2</span></span><br><span class="line">    </span><br><span class="line">    rotated_image = np.zeros((new_h, new_w, channel), dtype=image.dtype)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(new_h):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(new_w):</span><br><span class="line">            <span class="comment"># 将新图像坐标转换回原图像坐标</span></span><br><span class="line">            x = (j - new_cx) * math.cos(angle_rad) + (i - new_cy) * math.sin(angle_rad) + cx</span><br><span class="line">            y = -(j - new_cx) * math.sin(angle_rad) + (i - new_cy) * math.cos(angle_rad) + cy</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 最近邻插值</span></span><br><span class="line">            <span class="keyword">if</span> <span class="number">0</span> &lt;= x &lt; w <span class="keyword">and</span> <span class="number">0</span> &lt;= y &lt; h:</span><br><span class="line">                src_x = <span class="built_in">int</span>(<span class="built_in">round</span>(x))</span><br><span class="line">                src_y = <span class="built_in">int</span>(<span class="built_in">round</span>(y))</span><br><span class="line">                rotated_image[i, j] = image[src_y - <span class="number">1</span>, src_x - <span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> rotated_image</span><br></pre></td></tr></tbody></table></figure>

<h3 id="双线性法"><a href="#双线性法" class="headerlink" title="双线性法"></a>双线性法</h3><p><img src="/images/project2/5.png" alt="一维线性插值示意图"></p>
<p>如上图所示，在一维的线性插值中，坐标轴上各点 xi-1，xi，xi+1 … 的值“两两直接相连”为线段，从而构成了一条连续的约束函数。而插值坐标点例如 x，根据约束函数其值应为 f(x)。因为每两个坐标点之间的约束函数曲线是一次线性的线段，对插值结果而言是“线性” 的，所以该方法称为线性插值。基于线性函数的特性，可以便捷地求取原图像上的两个像素点间任一待插值点的颜色值：</p>
<p><img src="/images/project2/6.png" alt="一维线性插值计算示意图"></p>
<p>可以看到，图中 x0 和 x1 都是原有的坐标点，颜色值分别对应为 y0 和 y1，此时根据线性插值法约束，在 (x0, y0) 和 (x1, y1) 构成的一次函数上，颜色值未知的插值点 x的颜色值 y 即为：<br>$$<br>y=y_0+(x-x_0)\frac{y_1-y_0}{x_1-x_0}=y_0+\frac{(x-x_0)y_1-(x-x_0)y_0}{x_1-x_0}<br>$$<br>实际上，即便 x 不在 x0 与 x1 之间，该公式也成立（此时为线性外插），但图像处理中不需涉及此情形。 </p>
<p>从一维的线性插值出发，很容易拓展到二维图像的双线性插值，通过三次一阶线性插值（本质为加权求和）获得最终结果，下图便展示了该过程的定性斜视与定量俯视示意图：</p>
<p><img src="/images/project2/7.png" alt="二维线性插值定性斜视示意图"></p>
<p><img src="/images/project2/8.png" alt="二维线性插值定量俯视示意图"></p>
<p>其中，(x0, y0)、(x0, y1)、(x1, y0)、(x1, y1) 均为原图像上的像素坐标点，颜色值分别对应为 f(x0, y0)、f(x0, y1)、f(x1, y0)、f(x1, y1)。而颜色值未知的插值点 (x, y)，根据双线性插值法的约束，可以先由像素坐标点 (x0, y0) 和 (x0, y1) 在 y 轴向作一维线性插值得到 f(x0, y)、由像素坐标点 (x1, y0) 和 (x1, y1) 在 y 轴向作一维线性插值得到 f(x1, y)，然后再由 (x0, y) 和 (x1, y) 在 x 轴向作一维线性插值得到插值点 (x, y) 的灰度值 f(x, y)。</p>
<p>事实上，一维线性插值先作 x 轴向再作 y 轴向，得到的结果完全相同，仅为顺序先后的区别。这里不妨先由像素坐标点 (x0, y0) 和 (x1, y0) 在 x 轴向作一维线性插值得到 f(x, y0)、由像素坐标点 (x0, y1) 和 (x1, y1) 在 x 轴向作一维线性插值得到 f(x, y1)：<br>$$<br>f(x,y_0)=\frac{x_1-x}{x_1-x_0}f(x_0,y_0)+\frac{x-x_0}{x_1-x_0}f(x_1,y_0)<br>$$</p>
<p>$$<br>f(x,y_1)=\frac{x_1-x}{x_1-x_0}f(x_0,y_1)+\frac{x-x_0}{x_1-x_0}f(x_1,y_1)<br>$$</p>
<p>然后再由 (x, y0) 和 (x, y1) 在 y 轴向作一维线性插值得到插值点 (x, y) 的灰度值 f(x, y)：<br>$$<br>f(x,y)=\frac{y_1-y}{y_1-y_0}f(x,y_0)+\frac{y-y_0}{y_1-y_0}f(x,y_1)<br>$$<br>合并上述式子，得到最终的双线性插值结果：<br>$$<br>f(x,y)=\frac{(y_1-y)(x_1-x)}{(y_1-y_0)(x_1-x_0)}f(x_0,y_0)+\frac{(y_1-y)(x-x_0)}{(y_1-y_0)(x_1-x_0)}f(x_1,y_0)+\frac{(y-y_0)(x_1-x)}{(y_1-y_0)(x_1-x_0)}f(x_0,y_1)+\frac{(y-y_0)(x-x_0)}{(y_1-y_0)(x_1-x_0)}<br>$$<br>值得注意的是，在实际的图像插值处理过程中，为尽量保证插值效果的准确性，往往仅采用距离待插值点（转换至原图像坐标）最近的四个点，即:（[]符号表示待插值点转换至原图像坐标后向下取整）<br>$$<br>x_0=[x]，y_0=[y]<br>$$</p>
<p>$$<br>x_1=x_0+1，y_1=y_0+1<br>$$</p>
<p>从加权求和的角度理解，可以进一步地将双线性插值结果改写为如下形式：<br>$$<br>p=x-[x], q=y-[y]<br>$$</p>
<p>$$<br>\begin{array}{rcl}f(x,y)=(1-q){(1-p)f([x][y])+pf([x]+1,[y])}+q{(1-p)f([x],[y]+1)+pf([x]+1,[y]+1)}\end{array}<br>$$</p>
<p><img src="/images/project2/9.png" alt="二维线性插值加权求和角度示意图"></p>
<p>基于以上算法思想，编写python函数代码实现图像放缩与旋转过程中的双线性法插值：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 双线性法插值实现图像放缩</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bilinear_interpolation</span>(<span class="params">image, scale_factor</span>):</span><br><span class="line">    h, w, channel = image.shape</span><br><span class="line">    new_h, new_w = <span class="built_in">int</span>(h * scale_factor), <span class="built_in">int</span>(w * scale_factor)</span><br><span class="line">    resized_image = np.zeros((new_h, new_w, <span class="built_in">int</span>(channel)), dtype=image.dtype)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(new_h):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(new_w):</span><br><span class="line">            x = (j + <span class="number">1</span>) / scale_factor</span><br><span class="line">            y = (i + <span class="number">1</span>) / scale_factor</span><br><span class="line">            x1 = <span class="built_in">int</span>(x)</span><br><span class="line">            y1 = <span class="built_in">int</span>(y)</span><br><span class="line">            x2 = x1 + <span class="number">1</span></span><br><span class="line">            y2 = y1 + <span class="number">1</span></span><br><span class="line">            p = x - x1</span><br><span class="line">            q = y - y1</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 边界问题处理</span></span><br><span class="line">            <span class="keyword">if</span> x2 == w + <span class="number">1</span>:</span><br><span class="line">                x2 = x1</span><br><span class="line">            <span class="keyword">if</span> y2 == h + <span class="number">1</span>:</span><br><span class="line">                y2 = y1</span><br><span class="line">                </span><br><span class="line">            resized_image[i ,j] = (<span class="number">1</span> - q) * ((<span class="number">1</span> - p) * image[y1 - <span class="number">1</span>, x1 - <span class="number">1</span>] + p * image[y1 - <span class="number">1</span>, x2 - <span class="number">1</span>]) + q * ((<span class="number">1</span> - p) * image[y2 - <span class="number">1</span>, x1 - <span class="number">1</span>] + p * image[y2 - <span class="number">1</span>, x2 - <span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> resized_image</span><br><span class="line"></span><br><span class="line"><span class="comment"># 双线性法插值实现图像旋转</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bilinear_rotation</span>(<span class="params">image, angle</span>):</span><br><span class="line">    h, w, channel = image.shape</span><br><span class="line">    angle_rad = math.radians(angle)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算旋转后的图像尺寸</span></span><br><span class="line">    cos_theta = <span class="built_in">abs</span>(math.cos(angle_rad))</span><br><span class="line">    sin_theta = <span class="built_in">abs</span>(math.sin(angle_rad))</span><br><span class="line">    new_w = <span class="built_in">int</span>(h * sin_theta + w * cos_theta)</span><br><span class="line">    new_h = <span class="built_in">int</span>(h * cos_theta + w * sin_theta)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 旋转中心</span></span><br><span class="line">    cx, cy = w / <span class="number">2</span>, h / <span class="number">2</span></span><br><span class="line">    new_cx, new_cy = new_w / <span class="number">2</span>, new_h / <span class="number">2</span></span><br><span class="line">    </span><br><span class="line">    rotated_image = np.zeros((new_h, new_w, channel), dtype=image.dtype)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(new_h):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(new_w):</span><br><span class="line">            <span class="comment"># 将新图像坐标转换回原图像坐标</span></span><br><span class="line">            x = (j - new_cx) * math.cos(angle_rad) + (i - new_cy) * math.sin(angle_rad) + cx</span><br><span class="line">            y = -(j - new_cx) * math.sin(angle_rad) + (i - new_cy) * math.cos(angle_rad) + cy</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 双线性插值</span></span><br><span class="line">            <span class="keyword">if</span> <span class="number">0</span> &lt;= x &lt; w-<span class="number">1</span> <span class="keyword">and</span> <span class="number">0</span> &lt;= y &lt; h-<span class="number">1</span>:</span><br><span class="line">                x1, y1 = <span class="built_in">int</span>(x), <span class="built_in">int</span>(y)</span><br><span class="line">                x2, y2 = <span class="built_in">min</span>(x1 + <span class="number">1</span>, w - <span class="number">1</span>), <span class="built_in">min</span>(y1 + <span class="number">1</span>, h - <span class="number">1</span>)</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 计算权重</span></span><br><span class="line">                a = x - x1</span><br><span class="line">                b = y - y1</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 边界处理</span></span><br><span class="line">                <span class="keyword">if</span> x2 &gt;= w:</span><br><span class="line">                    x2 = x1</span><br><span class="line">                <span class="keyword">if</span> y2 &gt;= h:</span><br><span class="line">                    y2 = y1</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 插值计算</span></span><br><span class="line">                rotated_image[i, j] = (<span class="number">1</span> - a) * (<span class="number">1</span> - b) * image[y1, x1] + \</span><br><span class="line">                                     a * (<span class="number">1</span> - b) * image[y1, x2] + \</span><br><span class="line">                                     (<span class="number">1</span> - a) * b * image[y2, x1] + \</span><br><span class="line">                                     a * b * image[y2, x2]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> rotated_image    </span><br></pre></td></tr></tbody></table></figure>

<h3 id="双立方法"><a href="#双立方法" class="headerlink" title="双立方法"></a>双立方法</h3><p>双立方法插值又称立方卷积插值/双三次插值，这也是数值分析中最常用的二维插值方法。在这种方法中，插值点 (x, y) 的像素颜色值 f(x, y) 通过矩形网格中最近的十六个采样点的加权平均得到，而各采样点的权重由该点到待求插值点的距离确定，此距离包括水平和竖直两个方向上的距离。相比之下，双线性插值仅由周围的四个采样点加权得到。</p>
<p><img src="/images/project2/10.png" alt="双立方法插值示意图"></p>
<p>如上图所示，设（转换至原图像中）待求插值点坐标为 (i+u, j+v)【i、j为整数部分，u、v为小数部分】，已知其周围的 16 个像素坐标点 (网格) 的颜色值，还需要计算 16 个点各自的权重。以像素坐标点 (i, j) 为例，因为该点在 y 轴和 x 轴方向上与待求插值点 (i+u, j+v) 的距离分别为 u 和 v，所以其权重为 w(u) × w(v)，其中 w(·) 是插值权重核 (可以理解为定义的权重函数)。同理可得其余 15 个像素坐标点各自的权重。那么，待求插值点 (i+u, j+v) 的颜色值 f(i+u, j+v) 将通过如下计算得到：<br>$$<br>f(i+u,j+v)=A\times B\times C<br>$$<br>其中各项由向量或矩阵表示为：<br>$$<br>\mathrm{A}=[w(1+u)w(u)w(1-u)w(2-u)]<br>$$</p>
<p>$$<br>\mathrm{B}=\begin{bmatrix}f(i-1,j-1)&amp;f(i-1,j+0)&amp;f(i-1,j+1)&amp;f(i-1,j+2)\f(i+0,j-1)&amp;f(i+0,j+0)&amp;f(i+0,j+1)&amp;f(i+0,j+2)\f(i+1,j-1)&amp;f(i+1,j+0)&amp;f(i+1,j+1)&amp;f(i+1,j+2)\f(i+2,j-1)&amp;f(i+2,j+0)&amp;f(i+2,j+1)&amp;f(i+2,j+2)\end{bmatrix}<br>$$</p>
<p>$$<br>\mathbb{C}=[w(1+v)w(v)w(1-v)w(2-v)]^T<br>$$</p>
<p>插值权重核 w(·) 为：<br>$$<br>w(x)=\begin{cases}1-2|x|^2+|x|^3&amp;,|x|&lt;1\4-8|x|+5|x|^2-|x|^3&amp;,1\leq|x|&lt;2\0&amp;,|x|\geq2&amp;\end{cases}<br>$$<br>插值权重核 w(·) 的函数图像：</p>
<p><img src="/images/project2/11.png" alt="双立方法插值权重核函数图像"></p>
<p>为方便后续算法实现，将以上加权求和过程各步骤展开，合并后化简得到待插入点的颜色值计算公式：<br>$$<br>f(i+u,j+v)=\sum_{m=0}^{3}\sum_{n=0}^{3}a_{mn}u^{m}v^{n}<br>$$<br>其中多项式的系数a_{mn}计算公式如下：(式中p <em>{qr}与上述矩阵B中元素一一对应，如p <em>00=f(i-1,j-1))<br>$$<br>\begin{aligned}<br>&amp;a</em>{00}=p</em>{11}\&amp;a_{01}=-\frac{1}{2}p_{10}+\frac{1}{2}p_{12}\&amp;a_{02}=p_{10}-\frac{5}{2}p_{11}+2p_{12}-\frac{1}{2}p_{13}\&amp;a_{03}=-\frac{1}{2}p_{10}+\frac{3}{2}p_{11}-\frac{3}{2}p_{12}+\frac{1}{2}p_{13}\&amp;a_{10}=-\frac{1}{2}p_{01}+\frac{1}{2}p_{21}\&amp;a_{11}=\frac{1}{4}p_{00}-\frac{1}{4}p_{02}-\frac{1}{4}p_{20}+\frac{1}{4}p_{22}\&amp;a_{12}=-\frac{1}{2}p_{00}+\frac{1}{4}p_{01}-p_{02}+\frac{1}{4}p_{03}+\frac{1}{2}p_{20}-\frac{5}{4}p_{21}+p_{22}-\frac{1}{4}p_{23}\&amp;a_{13}=\frac{1}{4}p_{00}-\frac{3}{4}p_{01}+\frac{3}{4}p_{02}-\frac{1}{4}p_{03}-\frac{1}{4}p_{20}+\frac{3}{4}p_{21}-\frac{3}{4}p_{22}+\frac{1}{4}p_{23}\<br>&amp;a_{20}=p_{01}-\frac{5}{2}p_{11}+2p_{21}-\frac{1}{2}p_{31}\<br>&amp;a_{21}=-\frac{1}{2}p_{00}+\frac{1}{2}p_{02}+\frac{5}{4}p_{10}-\frac{5}{4}p_{12}-p_{20}+p_{22}+\frac{1}{4}p_{30}-\frac{1}{4}p_{32}\&amp;a_{22}=p_{00}-\frac{5}{2}p_{01}+2p_{02}-\frac{1}{2}p_{03}-\frac{5}{2}p_{10}+\frac{25}{4}p_{11}-5p_{12}+\frac{5}{4}p_{13}+2p_{20}-5p_{21}+4p_{22}-p_{23}-\frac{1}{2}p_{30}+\frac{5}{4}p_{31}-p_{32}+\frac{1}{4}p_{33}\<br>&amp;a_{23}=-\frac{1}{2}p_{00}+\frac{3}{2}p_{01}-\frac{3}{2}p_{02}+\frac{1}{2}p_{03}+\frac{5}{4}p_{10}-\frac{15}{4}p_{11}+\frac{15}{4}p_{12}-\frac{5}{4}p_{13}-p_{20}+3p_{21}-3p_{22}+p_{23}+\frac{1}{4}p_{30}-\frac{3}{4}p_{31}+\frac{3}{4}p_{32}-\frac{1}{4}p_{33}\<br>&amp;a_{30}=-\frac{1}{2}p_{01}+\frac{3}{2}p_{11}-\frac{3}{2}p_{21}+\frac{1}{2}p_{31}\<br>&amp;a_{31}=\frac{1}{4}p_{00}-\frac{1}{4}p_{02}-\frac{3}{4}p_{10}+\frac{3}{4}p_{12}+\frac{3}{4}p_{20}-\frac{3}{4}p_{22}-\frac{1}{4}p_{30}+\frac{1}{4}p_{32}\&amp;a_{32}=-\frac{1}{2}p_{00}+\frac{5}{4}p_{01}-p_{02}+\frac{1}{4}p_{03}+\frac{3}{2}p_{10}-\frac{15}{4}p_{11}+3p_{12}-\frac{3}{4}p_{13}-\frac{3}{2}p_{20}+\frac{15}{4}p_{21}-3p_{22}+\frac{3}{4}p_{23}+\frac{1}{2}p_{30}-\frac{5}{4}p_{31}+p_{32}-\frac{1}{4}p_{33}\&amp;a_{33}=\frac{1}{4}p_{00}-\frac{3}{4}p_{01}+\frac{3}{4}p_{02}-\frac{1}{4}p_{03}-\frac{3}{4}p_{10}+\frac{9}{4}p_{11}-\frac{9}{4}p_{12}+\frac{3}{4}p_{13}+\frac{3}{4}p_{20}-\frac{9}{4}p_{21}+\frac{9}{4}p_{22}-\frac{3}{4}p_{23}-\frac{1}{4}p_{30}+\frac{3}{4}p_{31}-\frac{3}{4}p_{32}+\frac{1}{4}p_{33}<br>\end{aligned}<br>$$<br>基于以上算法思想，编写python函数代码实现图像放缩与旋转过程中的双立方法插值：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 双立方法插值实现图像放缩</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bicubic_interpolation</span>(<span class="params">image, scale_factor</span>):</span><br><span class="line">    h, w, channel = image.shape</span><br><span class="line">    new_h, new_w = <span class="built_in">int</span>(h * scale_factor), <span class="built_in">int</span>(w * scale_factor)</span><br><span class="line">    resized_image = np.zeros((new_h, new_w, channel))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(new_h):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(new_w):</span><br><span class="line">            x = i / scale_factor</span><br><span class="line">            y = j / scale_factor</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 确定16个邻域像素的坐标</span></span><br><span class="line">            x0 = <span class="built_in">max</span>(<span class="number">0</span>, <span class="built_in">int</span>(np.floor(x)) - <span class="number">1</span>)</span><br><span class="line">            x1 = x0 + <span class="number">1</span></span><br><span class="line">            x2 = x0 + <span class="number">2</span></span><br><span class="line">            x3 = <span class="built_in">min</span>(w-<span class="number">1</span>, x0 + <span class="number">3</span>)</span><br><span class="line">            </span><br><span class="line">            y0 = <span class="built_in">max</span>(<span class="number">0</span>, <span class="built_in">int</span>(np.floor(y)) - <span class="number">1</span>)</span><br><span class="line">            y1 = y0 + <span class="number">1</span></span><br><span class="line">            y2 = y0 + <span class="number">2</span></span><br><span class="line">            y3 = <span class="built_in">min</span>(h-<span class="number">1</span>, y0 + <span class="number">3</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 获取16个邻域像素的值</span></span><br><span class="line">            p = np.zeros((<span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>))</span><br><span class="line">            <span class="keyword">for</span> m <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                    xi = x0 + n</span><br><span class="line">                    yi = y0 + m</span><br><span class="line">                    xi = <span class="built_in">min</span>(<span class="built_in">max</span>(xi, <span class="number">0</span>), w-<span class="number">1</span>)  <span class="comment"># 边界处理</span></span><br><span class="line">                    yi = <span class="built_in">min</span>(<span class="built_in">max</span>(yi, <span class="number">0</span>), h-<span class="number">1</span>)</span><br><span class="line">                    p[m, n] = image[yi, xi]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 计算相对位置</span></span><br><span class="line">            dx = x - x1</span><br><span class="line">            dy = y - y1</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 系数</span></span><br><span class="line">            a = np.zeros((<span class="number">4</span>, <span class="number">4</span>, channel))</span><br><span class="line">            a[<span class="number">0</span>, <span class="number">0</span>] = p[<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">            a[<span class="number">0</span>, <span class="number">1</span>] = -<span class="number">0.5</span>*p[<span class="number">1</span>, <span class="number">0</span>] + <span class="number">0.5</span>*p[<span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">            a[<span class="number">0</span>, <span class="number">2</span>] = p[<span class="number">1</span>, <span class="number">0</span>] - <span class="number">2.5</span>*p[<span class="number">1</span>, <span class="number">1</span>] + <span class="number">2</span>*p[<span class="number">1</span>, <span class="number">2</span>] - <span class="number">0.5</span>*p[<span class="number">1</span>, <span class="number">3</span>]</span><br><span class="line">            a[<span class="number">0</span>, <span class="number">3</span>] = -<span class="number">0.5</span>*p[<span class="number">1</span>, <span class="number">0</span>] + <span class="number">1.5</span>*p[<span class="number">1</span>, <span class="number">1</span>] - <span class="number">1.5</span>*p[<span class="number">1</span>, <span class="number">2</span>] + <span class="number">0.5</span>*p[<span class="number">1</span>, <span class="number">3</span>]</span><br><span class="line">            </span><br><span class="line">            a[<span class="number">1</span>, <span class="number">0</span>] = -<span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">1</span>] + <span class="number">0.5</span>*p[<span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">            a[<span class="number">1</span>, <span class="number">1</span>] = <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">0</span>] - <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">2</span>] - <span class="number">0.25</span>*p[<span class="number">2</span>, <span class="number">0</span>] + <span class="number">0.25</span>*p[<span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line">            a[<span class="number">1</span>, <span class="number">2</span>] = -<span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">0</span>] + <span class="number">1.25</span>*p[<span class="number">0</span>, <span class="number">1</span>] - p[<span class="number">0</span>, <span class="number">2</span>] + <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">3</span>] + <span class="number">0.5</span>*p[<span class="number">2</span>, <span class="number">0</span>] - <span class="number">1.25</span>*p[<span class="number">2</span>, <span class="number">1</span>] + p[<span class="number">2</span>, <span class="number">2</span>] - <span class="number">0.25</span>*p[<span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">            a[<span class="number">1</span>, <span class="number">3</span>] = <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">0</span>] - <span class="number">0.75</span>*p[<span class="number">0</span>, <span class="number">1</span>] + <span class="number">0.75</span>*p[<span class="number">0</span>, <span class="number">2</span>] - <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">3</span>] - <span class="number">0.25</span>*p[<span class="number">2</span>, <span class="number">0</span>] + <span class="number">0.75</span>*p[<span class="number">2</span>, <span class="number">1</span>] - <span class="number">0.75</span>*p[<span class="number">2</span>, <span class="number">2</span>] + <span class="number">0.25</span>*p[<span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">            </span><br><span class="line">            a[<span class="number">2</span>, <span class="number">0</span>] = p[<span class="number">0</span>, <span class="number">1</span>] - <span class="number">2.5</span>*p[<span class="number">1</span>, <span class="number">1</span>] + <span class="number">2</span>*p[<span class="number">2</span>, <span class="number">1</span>] - <span class="number">0.5</span>*p[<span class="number">3</span>, <span class="number">1</span>]</span><br><span class="line">            a[<span class="number">2</span>, <span class="number">1</span>] = -<span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">0</span>] + <span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">2</span>] + <span class="number">1.25</span>*p[<span class="number">1</span>, <span class="number">0</span>] - <span class="number">1.25</span>*p[<span class="number">1</span>, <span class="number">2</span>] - p[<span class="number">2</span>, <span class="number">0</span>] + p[<span class="number">2</span>, <span class="number">2</span>] + <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">0</span>] - <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">2</span>]</span><br><span class="line">            a[<span class="number">2</span>, <span class="number">2</span>] = p[<span class="number">0</span>, <span class="number">0</span>] - <span class="number">2.5</span>*p[<span class="number">0</span>, <span class="number">1</span>] + <span class="number">2</span>*p[<span class="number">0</span>, <span class="number">2</span>] - <span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">3</span>] - <span class="number">2.5</span>*p[<span class="number">1</span>, <span class="number">0</span>] + <span class="number">6.25</span>*p[<span class="number">1</span>, <span class="number">1</span>] - <span class="number">5</span>*p[<span class="number">1</span>, <span class="number">2</span>] + <span class="number">1.25</span>*p[<span class="number">1</span>, <span class="number">3</span>] + <span class="number">2</span>*p[<span class="number">2</span>, <span class="number">0</span>] - <span class="number">5</span>*p[<span class="number">2</span>, <span class="number">1</span>] + <span class="number">4</span>*p[<span class="number">2</span>, <span class="number">2</span>] - p[<span class="number">2</span>, <span class="number">3</span>] - <span class="number">0.5</span>*p[<span class="number">3</span>, <span class="number">0</span>] + <span class="number">1.25</span>*p[<span class="number">3</span>, <span class="number">1</span>] - p[<span class="number">3</span>, <span class="number">2</span>] + <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">3</span>]</span><br><span class="line">            a[<span class="number">2</span>, <span class="number">3</span>] = -<span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">0</span>] + <span class="number">1.5</span>*p[<span class="number">0</span>, <span class="number">1</span>] - <span class="number">1.5</span>*p[<span class="number">0</span>, <span class="number">2</span>] + <span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">3</span>] + <span class="number">1.25</span>*p[<span class="number">1</span>, <span class="number">0</span>] - <span class="number">3.75</span>*p[<span class="number">1</span>, <span class="number">1</span>] + <span class="number">3.75</span>*p[<span class="number">1</span>, <span class="number">2</span>] - <span class="number">1.25</span>*p[<span class="number">1</span>, <span class="number">3</span>] - p[<span class="number">2</span>, <span class="number">0</span>] + <span class="number">3</span>*p[<span class="number">2</span>, <span class="number">1</span>] - <span class="number">3</span>*p[<span class="number">2</span>, <span class="number">2</span>] + p[<span class="number">2</span>, <span class="number">3</span>] + <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">0</span>] - <span class="number">0.75</span>*p[<span class="number">3</span>, <span class="number">1</span>] + <span class="number">0.75</span>*p[<span class="number">3</span>, <span class="number">2</span>] - <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">3</span>]</span><br><span class="line">            </span><br><span class="line">            a[<span class="number">3</span>, <span class="number">0</span>] = -<span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">1</span>] + <span class="number">1.5</span>*p[<span class="number">1</span>, <span class="number">1</span>] - <span class="number">1.5</span>*p[<span class="number">2</span>, <span class="number">1</span>] + <span class="number">0.5</span>*p[<span class="number">3</span>, <span class="number">1</span>]</span><br><span class="line">            a[<span class="number">3</span>, <span class="number">1</span>] = <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">0</span>] - <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">2</span>] - <span class="number">0.75</span>*p[<span class="number">1</span>, <span class="number">0</span>] + <span class="number">0.75</span>*p[<span class="number">1</span>, <span class="number">2</span>] + <span class="number">0.75</span>*p[<span class="number">2</span>, <span class="number">0</span>] - <span class="number">0.75</span>*p[<span class="number">2</span>, <span class="number">2</span>] - <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">0</span>] + <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">2</span>]</span><br><span class="line">            a[<span class="number">3</span>, <span class="number">2</span>] = -<span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">0</span>] + <span class="number">1.25</span>*p[<span class="number">0</span>, <span class="number">1</span>] - p[<span class="number">0</span>, <span class="number">2</span>] + <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">3</span>] + <span class="number">1.5</span>*p[<span class="number">1</span>, <span class="number">0</span>] - <span class="number">3.75</span>*p[<span class="number">1</span>, <span class="number">1</span>] + <span class="number">3</span>*p[<span class="number">1</span>, <span class="number">2</span>] - <span class="number">0.75</span>*p[<span class="number">1</span>, <span class="number">3</span>] - <span class="number">1.5</span>*p[<span class="number">2</span>, <span class="number">0</span>] + <span class="number">3.75</span>*p[<span class="number">2</span>, <span class="number">1</span>] - <span class="number">3</span>*p[<span class="number">2</span>, <span class="number">2</span>] + <span class="number">0.75</span>*p[<span class="number">2</span>, <span class="number">3</span>] + <span class="number">0.5</span>*p[<span class="number">3</span>, <span class="number">0</span>] - <span class="number">1.25</span>*p[<span class="number">3</span>, <span class="number">1</span>] + p[<span class="number">3</span>, <span class="number">2</span>] - <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">3</span>]</span><br><span class="line">            a[<span class="number">3</span>, <span class="number">3</span>] = <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">0</span>] - <span class="number">0.75</span>*p[<span class="number">0</span>, <span class="number">1</span>] + <span class="number">0.75</span>*p[<span class="number">0</span>, <span class="number">2</span>] - <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">3</span>] - <span class="number">0.75</span>*p[<span class="number">1</span>, <span class="number">0</span>] + <span class="number">2.25</span>*p[<span class="number">1</span>, <span class="number">1</span>] - <span class="number">2.25</span>*p[<span class="number">1</span>, <span class="number">2</span>] + <span class="number">0.75</span>*p[<span class="number">1</span>, <span class="number">3</span>] + <span class="number">0.75</span>*p[<span class="number">2</span>, <span class="number">0</span>] - <span class="number">2.25</span>*p[<span class="number">2</span>, <span class="number">1</span>] + <span class="number">2.25</span>*p[<span class="number">2</span>, <span class="number">2</span>] - <span class="number">0.75</span>*p[<span class="number">2</span>, <span class="number">3</span>] - <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">0</span>] + <span class="number">0.75</span>*p[<span class="number">3</span>, <span class="number">1</span>] - <span class="number">0.75</span>*p[<span class="number">3</span>, <span class="number">2</span>] + <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">3</span>]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 计算插值结果</span></span><br><span class="line">            value = np.zeros(channel)</span><br><span class="line">            <span class="keyword">for</span> m <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                    value += a[m, n] * (dx**n) * (dy**m)</span><br><span class="line">            </span><br><span class="line">            resized_image[i, j] = np.clip(value, <span class="number">0</span>, <span class="number">255</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> resized_image.astype(np.uint8)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 双立方法插值实现图像旋转</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bicubic_rotation</span>(<span class="params">image, angle</span>):</span><br><span class="line">    h, w, channel = image.shape</span><br><span class="line">    angle_rad = math.radians(angle)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算旋转后的图像尺寸</span></span><br><span class="line">    cos_theta = <span class="built_in">abs</span>(math.cos(angle_rad))</span><br><span class="line">    sin_theta = <span class="built_in">abs</span>(math.sin(angle_rad))</span><br><span class="line">    new_w = <span class="built_in">int</span>(h * sin_theta + w * cos_theta)</span><br><span class="line">    new_h = <span class="built_in">int</span>(h * cos_theta + w * sin_theta)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 旋转中心</span></span><br><span class="line">    cx, cy = w / <span class="number">2</span>, h / <span class="number">2</span></span><br><span class="line">    new_cx, new_cy = new_w / <span class="number">2</span>, new_h / <span class="number">2</span></span><br><span class="line">    </span><br><span class="line">    rotated_image = np.zeros((new_h, new_w, channel))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(new_h):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(new_w):</span><br><span class="line">            <span class="comment"># 将新图像坐标转换回原图像坐标</span></span><br><span class="line">            x = (j - new_cx) * math.cos(angle_rad) + (i - new_cy) * math.sin(angle_rad) + cx</span><br><span class="line">            y = -(j - new_cx) * math.sin(angle_rad) + (i - new_cy) * math.cos(angle_rad) + cy</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> <span class="number">0</span> &lt;= x &lt; w <span class="keyword">and</span> <span class="number">0</span> &lt;= y &lt; h:</span><br><span class="line">                <span class="comment"># 确定16个邻域像素的坐标</span></span><br><span class="line">                x0 = <span class="built_in">max</span>(<span class="number">0</span>, <span class="built_in">int</span>(np.floor(x)) - <span class="number">1</span>)</span><br><span class="line">                x1 = x0 + <span class="number">1</span></span><br><span class="line">                x2 = x0 + <span class="number">2</span></span><br><span class="line">                x3 = <span class="built_in">min</span>(w-<span class="number">1</span>, x0 + <span class="number">3</span>)</span><br><span class="line">                </span><br><span class="line">                y0 = <span class="built_in">max</span>(<span class="number">0</span>, <span class="built_in">int</span>(np.floor(y)) - <span class="number">1</span>)</span><br><span class="line">                y1 = y0 + <span class="number">1</span></span><br><span class="line">                y2 = y0 + <span class="number">2</span></span><br><span class="line">                y3 = <span class="built_in">min</span>(h-<span class="number">1</span>, y0 + <span class="number">3</span>)</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 获取16个邻域像素的值</span></span><br><span class="line">                p = np.zeros((<span class="number">4</span>, <span class="number">4</span>, channel))</span><br><span class="line">                <span class="keyword">for</span> m <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                        xi = x0 + n</span><br><span class="line">                        yi = y0 + m</span><br><span class="line">                        xi = <span class="built_in">min</span>(<span class="built_in">max</span>(xi, <span class="number">0</span>), w-<span class="number">1</span>)  <span class="comment"># 边界处理</span></span><br><span class="line">                        yi = <span class="built_in">min</span>(<span class="built_in">max</span>(yi, <span class="number">0</span>), h-<span class="number">1</span>)</span><br><span class="line">                        p[m, n] = image[yi, xi]</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 计算相对位置</span></span><br><span class="line">                dx = x - x1</span><br><span class="line">                dy = y - y1</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 系数</span></span><br><span class="line">                a = np.zeros((<span class="number">4</span>, <span class="number">4</span>, channel))</span><br><span class="line">                a[<span class="number">0</span>, <span class="number">0</span>] = p[<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">                a[<span class="number">0</span>, <span class="number">1</span>] = -<span class="number">0.5</span>*p[<span class="number">1</span>, <span class="number">0</span>] + <span class="number">0.5</span>*p[<span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">                a[<span class="number">0</span>, <span class="number">2</span>] = p[<span class="number">1</span>, <span class="number">0</span>] - <span class="number">2.5</span>*p[<span class="number">1</span>, <span class="number">1</span>] + <span class="number">2</span>*p[<span class="number">1</span>, <span class="number">2</span>] - <span class="number">0.5</span>*p[<span class="number">1</span>, <span class="number">3</span>]</span><br><span class="line">                a[<span class="number">0</span>, <span class="number">3</span>] = -<span class="number">0.5</span>*p[<span class="number">1</span>, <span class="number">0</span>] + <span class="number">1.5</span>*p[<span class="number">1</span>, <span class="number">1</span>] - <span class="number">1.5</span>*p[<span class="number">1</span>, <span class="number">2</span>] + <span class="number">0.5</span>*p[<span class="number">1</span>, <span class="number">3</span>]</span><br><span class="line">                </span><br><span class="line">                a[<span class="number">1</span>, <span class="number">0</span>] = -<span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">1</span>] + <span class="number">0.5</span>*p[<span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">                a[<span class="number">1</span>, <span class="number">1</span>] = <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">0</span>] - <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">2</span>] - <span class="number">0.25</span>*p[<span class="number">2</span>, <span class="number">0</span>] + <span class="number">0.25</span>*p[<span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line">                a[<span class="number">1</span>, <span class="number">2</span>] = -<span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">0</span>] + <span class="number">1.25</span>*p[<span class="number">0</span>, <span class="number">1</span>] - p[<span class="number">0</span>, <span class="number">2</span>] + <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">3</span>] + <span class="number">0.5</span>*p[<span class="number">2</span>, <span class="number">0</span>] - <span class="number">1.25</span>*p[<span class="number">2</span>, <span class="number">1</span>] + p[<span class="number">2</span>, <span class="number">2</span>] - <span class="number">0.25</span>*p[<span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">                a[<span class="number">1</span>, <span class="number">3</span>] = <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">0</span>] - <span class="number">0.75</span>*p[<span class="number">0</span>, <span class="number">1</span>] + <span class="number">0.75</span>*p[<span class="number">0</span>, <span class="number">2</span>] - <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">3</span>] - <span class="number">0.25</span>*p[<span class="number">2</span>, <span class="number">0</span>] + <span class="number">0.75</span>*p[<span class="number">2</span>, <span class="number">1</span>] - <span class="number">0.75</span>*p[<span class="number">2</span>, <span class="number">2</span>] + <span class="number">0.25</span>*p[<span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">                </span><br><span class="line">                a[<span class="number">2</span>, <span class="number">0</span>] = p[<span class="number">0</span>, <span class="number">1</span>] - <span class="number">2.5</span>*p[<span class="number">1</span>, <span class="number">1</span>] + <span class="number">2</span>*p[<span class="number">2</span>, <span class="number">1</span>] - <span class="number">0.5</span>*p[<span class="number">3</span>, <span class="number">1</span>]</span><br><span class="line">                a[<span class="number">2</span>, <span class="number">1</span>] = -<span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">0</span>] + <span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">2</span>] + <span class="number">1.25</span>*p[<span class="number">1</span>, <span class="number">0</span>] - <span class="number">1.25</span>*p[<span class="number">1</span>, <span class="number">2</span>] - p[<span class="number">2</span>, <span class="number">0</span>] + p[<span class="number">2</span>, <span class="number">2</span>] + <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">0</span>] - <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">2</span>]</span><br><span class="line">                a[<span class="number">2</span>, <span class="number">2</span>] = p[<span class="number">0</span>, <span class="number">0</span>] - <span class="number">2.5</span>*p[<span class="number">0</span>, <span class="number">1</span>] + <span class="number">2</span>*p[<span class="number">0</span>, <span class="number">2</span>] - <span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">3</span>] - <span class="number">2.5</span>*p[<span class="number">1</span>, <span class="number">0</span>] + <span class="number">6.25</span>*p[<span class="number">1</span>, <span class="number">1</span>] - <span class="number">5</span>*p[<span class="number">1</span>, <span class="number">2</span>] + <span class="number">1.25</span>*p[<span class="number">1</span>, <span class="number">3</span>] + <span class="number">2</span>*p[<span class="number">2</span>, <span class="number">0</span>] - <span class="number">5</span>*p[<span class="number">2</span>, <span class="number">1</span>] + <span class="number">4</span>*p[<span class="number">2</span>, <span class="number">2</span>] - p[<span class="number">2</span>, <span class="number">3</span>] - <span class="number">0.5</span>*p[<span class="number">3</span>, <span class="number">0</span>] + <span class="number">1.25</span>*p[<span class="number">3</span>, <span class="number">1</span>] - p[<span class="number">3</span>, <span class="number">2</span>] + <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">3</span>]</span><br><span class="line">                a[<span class="number">2</span>, <span class="number">3</span>] = -<span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">0</span>] + <span class="number">1.5</span>*p[<span class="number">0</span>, <span class="number">1</span>] - <span class="number">1.5</span>*p[<span class="number">0</span>, <span class="number">2</span>] + <span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">3</span>] + <span class="number">1.25</span>*p[<span class="number">1</span>, <span class="number">0</span>] - <span class="number">3.75</span>*p[<span class="number">1</span>, <span class="number">1</span>] + <span class="number">3.75</span>*p[<span class="number">1</span>, <span class="number">2</span>] - <span class="number">1.25</span>*p[<span class="number">1</span>, <span class="number">3</span>] - p[<span class="number">2</span>, <span class="number">0</span>] + <span class="number">3</span>*p[<span class="number">2</span>, <span class="number">1</span>] - <span class="number">3</span>*p[<span class="number">2</span>, <span class="number">2</span>] + p[<span class="number">2</span>, <span class="number">3</span>] + <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">0</span>] - <span class="number">0.75</span>*p[<span class="number">3</span>, <span class="number">1</span>] + <span class="number">0.75</span>*p[<span class="number">3</span>, <span class="number">2</span>] - <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">3</span>]</span><br><span class="line">                </span><br><span class="line">                a[<span class="number">3</span>, <span class="number">0</span>] = -<span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">1</span>] + <span class="number">1.5</span>*p[<span class="number">1</span>, <span class="number">1</span>] - <span class="number">1.5</span>*p[<span class="number">2</span>, <span class="number">1</span>] + <span class="number">0.5</span>*p[<span class="number">3</span>, <span class="number">1</span>]</span><br><span class="line">                a[<span class="number">3</span>, <span class="number">1</span>] = <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">0</span>] - <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">2</span>] - <span class="number">0.75</span>*p[<span class="number">1</span>, <span class="number">0</span>] + <span class="number">0.75</span>*p[<span class="number">1</span>, <span class="number">2</span>] + <span class="number">0.75</span>*p[<span class="number">2</span>, <span class="number">0</span>] - <span class="number">0.75</span>*p[<span class="number">2</span>, <span class="number">2</span>] - <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">0</span>] + <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">2</span>]</span><br><span class="line">                a[<span class="number">3</span>, <span class="number">2</span>] = -<span class="number">0.5</span>*p[<span class="number">0</span>, <span class="number">0</span>] + <span class="number">1.25</span>*p[<span class="number">0</span>, <span class="number">1</span>] - p[<span class="number">0</span>, <span class="number">2</span>] + <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">3</span>] + <span class="number">1.5</span>*p[<span class="number">1</span>, <span class="number">0</span>] - <span class="number">3.75</span>*p[<span class="number">1</span>, <span class="number">1</span>] + <span class="number">3</span>*p[<span class="number">1</span>, <span class="number">2</span>] - <span class="number">0.75</span>*p[<span class="number">1</span>, <span class="number">3</span>] - <span class="number">1.5</span>*p[<span class="number">2</span>, <span class="number">0</span>] + <span class="number">3.75</span>*p[<span class="number">2</span>, <span class="number">1</span>] - <span class="number">3</span>*p[<span class="number">2</span>, <span class="number">2</span>] + <span class="number">0.75</span>*p[<span class="number">2</span>, <span class="number">3</span>] + <span class="number">0.5</span>*p[<span class="number">3</span>, <span class="number">0</span>] - <span class="number">1.25</span>*p[<span class="number">3</span>, <span class="number">1</span>] + p[<span class="number">3</span>, <span class="number">2</span>] - <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">3</span>]</span><br><span class="line">                a[<span class="number">3</span>, <span class="number">3</span>] = <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">0</span>] - <span class="number">0.75</span>*p[<span class="number">0</span>, <span class="number">1</span>] + <span class="number">0.75</span>*p[<span class="number">0</span>, <span class="number">2</span>] - <span class="number">0.25</span>*p[<span class="number">0</span>, <span class="number">3</span>] - <span class="number">0.75</span>*p[<span class="number">1</span>, <span class="number">0</span>] + <span class="number">2.25</span>*p[<span class="number">1</span>, <span class="number">1</span>] - <span class="number">2.25</span>*p[<span class="number">1</span>, <span class="number">2</span>] + <span class="number">0.75</span>*p[<span class="number">1</span>, <span class="number">3</span>] + <span class="number">0.75</span>*p[<span class="number">2</span>, <span class="number">0</span>] - <span class="number">2.25</span>*p[<span class="number">2</span>, <span class="number">1</span>] + <span class="number">2.25</span>*p[<span class="number">2</span>, <span class="number">2</span>] - <span class="number">0.75</span>*p[<span class="number">2</span>, <span class="number">3</span>] - <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">0</span>] + <span class="number">0.75</span>*p[<span class="number">3</span>, <span class="number">1</span>] - <span class="number">0.75</span>*p[<span class="number">3</span>, <span class="number">2</span>] + <span class="number">0.25</span>*p[<span class="number">3</span>, <span class="number">3</span>]</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 计算插值结果</span></span><br><span class="line">                value = np.zeros(channel)</span><br><span class="line">                <span class="keyword">for</span> m <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                        value += a[m, n] * (dx**n) * (dy**m)</span><br><span class="line">                </span><br><span class="line">                rotated_image[i, j] = np.clip(value, <span class="number">0</span>, <span class="number">255</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> rotated_image.astype(np.uint8)</span><br></pre></td></tr></tbody></table></figure>

<h2 id="图像插值算法优化：基于四平面"><a href="#图像插值算法优化：基于四平面" class="headerlink" title="图像插值算法优化：基于四平面"></a>图像插值算法优化：基于四平面</h2><p>在上述的多种基于分段插值的图像插值算法中，均采用f(i, j)来表示图像的像素点坐标处的颜色值，其中ｉ表示行坐标，ｊ表示列坐标。为进一步地体现图像的局部特征差异并将其用于插值过程，我们引入“平面”的概念，并对图像数据进行升维处理，用三维空间点(i, j, f(i, j))来表示一个像素，并将其对应至空间坐标系中的一个点(x, y, z)。</p>
<p>对一个待插入点而言，可以通过坐标平移将其周围4 个像素点转换为：（注意：此处z0<del>z3为像素坐标点s0</del>s3的颜色值，下同）<br>$$<br>s_0(0,0,z_0),s_1(0,1,z_1),s_2(1,0,z_2),s_3(1,1,z_3)<br>$$<br>从上述４个点的坐标可以看出它们任意３个点一定不在同一条直线上， 不在同一直线上的３个点可以确定一个平面， 下面讨论具体的插值方法：</p>
<ol>
<li><p>先求出这４个点可能的４个平面方程</p>
<p>已知空间平面的一般方程为：<br>$$<br>Ax+By+Cz+D=0<br>$$<br>将s0、s1、s2分别带入上式可得：<br>$$<br>\begin{cases}Cz_0+D=0\B+Cz_1+D=0\A+Cz_2+D=0&amp;\end{cases}<br>$$<br>则有：<br>$$<br>D=-Cz_0,B=C(z_0-z_1),A=C(z_0-z_2)<br>$$<br>再将其带回空间平面方程，整理后用f(x, y)代替z得到插值公式：<br>$$<br>f(x,y)=(z_{2}-z_{0})x+(z_{1}-z_{2})y+z_{0}<br>$$<br>同理，将s0、s1、s3带入空间平面方程可得插值公式：<br>$$<br>f(x,y)=(z_{3}-z_{1})x+(z_{1}-z_{0})y+z_{0}<br>$$<br>将s0、s2、s3带入空间平面方程可得插值公式：<br>$$<br>f(x,y)=(z_{2}-z_{0})x+(z_{3}-z_{2})y+z_{0}<br>$$<br>将s1、s2、s3带入空间平面方程可得插值公式：<br>$$<br>f(x,y)=(z_{3}-z_{1})x+(z_{3}-z_{2})y+(z_{2}+z_{1}-z_{3})<br>$$</p>
</li>
<li><p>如果s0、s1、s2、s3这４ 个点在同一平面上， 则使用上述任意一个插值公式进行插值均可。 【平面法】</p>
<blockquote>
<p>判断这４个点是否在同一平面上， 只需要比较z1+z2 与 z0+z3是否相等：</p>
<p>线段s0s3中点坐标为<br>$$<br>(\frac12,\frac12,\frac{z_0+z_3}2)<br>$$<br>线段s1s2中点坐标为<br>$$<br>(\frac12,\frac12,\frac{z_1+z_2}2)<br>$$<br>如果它们的中点坐标相同，则说明两条线段相交，相交的两条直线可以决定一个平面，即如果待插人点周围的四个点满足：<br>$$<br>z_1+z_2=z_0+z_3<br>$$<br>则这它们就是同一平面上的 4 个点，否则就不是同一平面上的 4 个点。</p>
</blockquote>
</li>
<li><p>从４个可能的平面中选择一个平面进行插值【四平面法】</p>
<p>如果它们不是同一平面上的４个点， 情况比较复杂， 需认真讨论，s0、s1、s2、s3４个点的位置关系如下图所示：</p>
<p><img src="/images/project2/12.png" alt="四点不在同一平面"></p>
<p>在插值的过程中如果一半的区域选择由s0、s1、s2 所确定的平面进行插值， 则另一半必须选择由s1、s2、s3所确定的平面进行插值， 以保证对角线的每一边都是在同一个平面上， 避免出现 “锯齿形” 边缘，为了便于描述， 称s0、s1、s2所确定的平面为 “左下平面”，s1、s2、s3 所确定的平面为 “右上平面”，s0、s1、s3 所确定的平面 “左上平面”，s0、s2、s3所确定的平面 “右下平面”。 为此， 需要参考周围其他点的情况以决定选择哪个平面进行插值。 具体情况如下图所示（黑点是待插入点周围的４个点，白点是参考点）：</p>
<p><img src="/images/project2/13.png" alt="待插入点周围的像素点"></p>
<ol>
<li>对于“左下平面”， 只能参考s0、s1、s2三点左面和下面的点， 即s0、s1、s2三点与s4、s5、s6、s8四个点中的任意一点在同一平面上即可。 </li>
<li>对于“右上平面”，只能参考s1、s2、s3三点右面和上面的点， 即s1、s2、s3三点与s7、s9、s10、s11四个点中的任意一点在同一平面上即可。</li>
<li>对于 “左上平面”，只能参考s0、s1、s3三点左面和上面的点， 即s0、s1、s3三点与s6、s8、s10、s11四个点中的任意一点在同一平面上即可。 </li>
<li>对于 “右下平面”，只能参考s0、s2、s3三点右面和下面的点， 即s0、s2、s3三点与s4、s5、s7、s9四 个点中的任意一点在同一平面上即可。</li>
</ol>
<p>针对1、2两种情况， 当y = 1 + x时，用 “左下平面” 进行插值， 否则用 “右上平面” 进行插值；针对3、4两种情况， 当y = x ^ 3时，用 “左上平面” 进行插值， 否则用 “右下平面” 进行插值。</p>
<blockquote>
<p>判断４个点在同一平面上的方法：（以情况1为例）</p>
<ul>
<li><p>对于判断s0、s2、s1、s8 ４ 点是否在同一平面上， 只需要判断z0 + z1与z2 + z8是否相等即可； </p>
</li>
<li><p>对于s0、s1、s2、s5 ４点， 只需要判断z0 + z2与z1 + z5是否相等即可； </p>
</li>
<li><p>对于s0、s1、s2、s6 ４点：如果s0、s2、s6 ３点在同一直线上， 则直线外一点s1与该直线就可以确定一个平面，而要判断这三点是否在同一直线上，只需判断z2 + z6与2 * z0是否相等即可【线段s2(1, 0, z2) s6(-1, 0, z6) 的中点坐标为(0, 0, z2 + z6)，若z2 + z6 = 2 * z0， 则点s0(0, 0, z0)就是它们的中点坐标，当然这３点就在同一条直线上】； </p>
</li>
<li><p>对于s0、s1、s2、s4   ４点， 与s0、s1、s2、s6 ４ 点的情况相同。</p>
</li>
</ul>
</blockquote>
</li>
<li><p>如果2和3两点中的情形均不满足， 说明待插入点周围的情况太复杂（不符合平面插值）， 此时采用<strong>双线性法</strong>进行插值。</p>
</li>
</ol>
<p>基于以上算法思想，编写python函数代码实现图像放缩与旋转过程中的四平面法插值：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 四平面法插值实现图像放缩</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">four_plane_interpolation</span>(<span class="params">img, scale</span>):</span><br><span class="line">    H, W, C = img.shape</span><br><span class="line">    new_H, new_W = <span class="built_in">int</span>(H * scale), <span class="built_in">int</span>(W * scale)</span><br><span class="line">    output = np.zeros((new_H, new_W, C), dtype=img.dtype)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(new_H):</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(new_W):</span><br><span class="line">            <span class="comment"># 计算原图对应坐标（浮点数）</span></span><br><span class="line">            src_x = x / scale</span><br><span class="line">            src_y = y / scale</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 获取周围4个整数坐标点</span></span><br><span class="line">            x0, y0 = <span class="built_in">int</span>(np.floor(src_x)), <span class="built_in">int</span>(np.floor(src_y))</span><br><span class="line">            x1, y1 = <span class="built_in">min</span>(x0 + <span class="number">1</span>, W - <span class="number">1</span>), <span class="built_in">min</span>(y0 + <span class="number">1</span>, H - <span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 获取4个点的颜色值（z坐标）</span></span><br><span class="line">            s0 = img[y0, x0]</span><br><span class="line">            s1 = img[y0, x1]</span><br><span class="line">            s2 = img[y1, x0]</span><br><span class="line">            s3 = img[y1, x1]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 计算相对位置（归一化到[0,1]）</span></span><br><span class="line">            dx = src_x - x0</span><br><span class="line">            dy = src_y - y0</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 判断四点是否共面（z1 + z2 ≈ z0 + z3）</span></span><br><span class="line">            <span class="keyword">if</span> np.allclose(s1 + s2, s0 + s3, atol=<span class="number">1e-6</span>):</span><br><span class="line">                <span class="comment"># 共面时，选择任意平面（此处用左下平面）</span></span><br><span class="line">                interpolated = s0 + (s2 - s0) * dx + (s1 - s0) * dy</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 不共面时，动态选择平面</span></span><br><span class="line">                <span class="comment"># 获取周围12个参考点（简化实现，仅取最近邻）</span></span><br><span class="line">                <span class="comment"># 注：论文中需判断参考点是否共面，此处简化逻辑</span></span><br><span class="line">                <span class="keyword">if</span> dy &gt; <span class="number">1</span> - dx:  <span class="comment"># 对角线 y = 1 - x 上方</span></span><br><span class="line">                    <span class="comment"># 选择右上平面</span></span><br><span class="line">                    interpolated = (s3 - s1) * dx + (s3 - s2) * dy + (s2 + s1 - s3)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="comment"># 选择左下平面</span></span><br><span class="line">                    interpolated = s0 + (s2 - s0) * dx + (s1 - s0) * dy</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 边界检查</span></span><br><span class="line">            interpolated = np.clip(interpolated, <span class="number">0</span>, <span class="number">255</span> <span class="keyword">if</span> img.dtype == np.uint8 <span class="keyword">else</span> <span class="number">1.0</span>)</span><br><span class="line">            output[y, x] = interpolated</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 四平面法插值实现图像旋转</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">four_plane_rotation</span>(<span class="params">image, angle</span>):</span><br><span class="line">    h, w, channel = image.shape</span><br><span class="line">    angle_rad = math.radians(angle)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算旋转后的图像尺寸</span></span><br><span class="line">    cos_theta = <span class="built_in">abs</span>(math.cos(angle_rad))</span><br><span class="line">    sin_theta = <span class="built_in">abs</span>(math.sin(angle_rad))</span><br><span class="line">    new_w = <span class="built_in">int</span>(h * sin_theta + w * cos_theta)</span><br><span class="line">    new_h = <span class="built_in">int</span>(h * cos_theta + w * sin_theta)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 旋转中心</span></span><br><span class="line">    cx, cy = w / <span class="number">2</span>, h / <span class="number">2</span></span><br><span class="line">    new_cx, new_cy = new_w / <span class="number">2</span>, new_h / <span class="number">2</span></span><br><span class="line">    </span><br><span class="line">    rotated_image = np.zeros((new_h, new_w, channel), dtype=image.dtype)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(new_h):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(new_w):</span><br><span class="line">            <span class="comment"># 将新图像坐标转换回原图像坐标</span></span><br><span class="line">            x = (j - new_cx) * math.cos(angle_rad) + (i - new_cy) * math.sin(angle_rad) + cx</span><br><span class="line">            y = -(j - new_cx) * math.sin(angle_rad) + (i - new_cy) * math.cos(angle_rad) + cy</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 边界检查</span></span><br><span class="line">            <span class="keyword">if</span> <span class="number">0</span> &lt;= x &lt; w <span class="keyword">and</span> <span class="number">0</span> &lt;= y &lt; h:</span><br><span class="line">                <span class="comment"># 获取周围4个整数坐标点</span></span><br><span class="line">                x0, y0 = <span class="built_in">int</span>(np.floor(x)), <span class="built_in">int</span>(np.floor(y))</span><br><span class="line">                x1, y1 = <span class="built_in">min</span>(x0 + <span class="number">1</span>, w - <span class="number">1</span>), <span class="built_in">min</span>(y0 + <span class="number">1</span>, h - <span class="number">1</span>)</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 获取4个点的颜色值</span></span><br><span class="line">                s0 = image[y0, x0]</span><br><span class="line">                s1 = image[y0, x1]</span><br><span class="line">                s2 = image[y1, x0]</span><br><span class="line">                s3 = image[y1, x1]</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 计算相对位置</span></span><br><span class="line">                dx = x - x0</span><br><span class="line">                dy = y - y0</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 判断四点是否共面</span></span><br><span class="line">                <span class="keyword">if</span> np.allclose(s1 + s2, s0 + s3, atol=<span class="number">1e-6</span>):</span><br><span class="line">                    <span class="comment"># 共面时，选择任意平面（此处用左下平面）</span></span><br><span class="line">                    interpolated = s0 + (s2 - s0) * dy + (s1 - s0) * dx</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="comment"># 不共面时，动态选择平面</span></span><br><span class="line">                    <span class="keyword">if</span> dy &gt; <span class="number">1</span> - dx:  <span class="comment"># 对角线 y = 1 - x 上方</span></span><br><span class="line">                        <span class="comment"># 选择右上平面</span></span><br><span class="line">                        interpolated = (s3 - s1) * dx + (s3 - s2) * dy + (s2 + s1 - s3)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="comment"># 选择左下平面</span></span><br><span class="line">                        interpolated = s0 + (s2 - s0) * dy + (s1 - s0) * dx</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 边界检查</span></span><br><span class="line">                interpolated = np.clip(interpolated, <span class="number">0</span>, <span class="number">255</span> <span class="keyword">if</span> image.dtype == np.uint8 <span class="keyword">else</span> <span class="number">1.0</span>)</span><br><span class="line">                rotated_image[i, j] = interpolated</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> rotated_image</span><br></pre></td></tr></tbody></table></figure>

<h2 id="实验测试结果分析"><a href="#实验测试结果分析" class="headerlink" title="实验测试结果分析"></a>实验测试结果分析</h2><p>一个理想的插值算法对一幅图像逆时针旋转若干度，再顺时针旋转若干度，应该与原图像相同；同理，对一幅图像放大若干倍，再缩小若干倍，也应该与原图像相同。 基于此，将下面的4幅图像分别用4种算法先逆时针旋转45°，再顺时针旋转45°；先放大４倍，再缩小４倍，然后分别用峰值信噪比（PSNR）验证各算法的优劣。 </p>
<p><img src="/images/project2/5.jpg" alt="1琳娜"></p>
<p><img src="/images/project2/6.jpg" alt="2辣椒"></p>
<p><img src="/images/project2/7.jpg" alt="3狒狒"></p>
<p><img src="/images/project2/8.jpg" alt="4房子"></p>
<p>从定性实验的效果角度，上述四幅图像通过常用的三种分段插值算法完成上述的放大与旋转任务后得到的结果如下图所示：</p>
<p><img src="/images/project2/Figure_1.png" alt="1琳娜传统result"></p>
<p><img src="/images/project2/Figure_2.png" alt="2辣椒传统result"></p>
<p><img src="/images/project2/Figure_3.png" alt="3狒狒传统result"></p>
<p><img src="/images/project2/Figure_4.png" alt="4房子传统result"></p>
<p>从实验结果上来看，最近邻算法的边缘颜色“最醒目”，且出现了较为严重的“锯齿形”边缘现象；双线性算法的边缘颜色“最暗淡”；双线性算法和双三次算法也有“锯齿形”边缘现象， 但视觉效果相比最近邻算法而言并不明显。</p>
<p>通过改进的四平面插值算法，对上述四幅图像完成上述的放大与旋转任务后得到的结果如下图所示：</p>
<p><img src="/images/project2/Figure_5.png" alt="1琳娜四平面result"></p>
<p><img src="/images/project2/Figure_6.png" alt="2辣椒四平面result"></p>
<p><img src="/images/project2/Figure_7.png" alt="3狒狒四平面result"></p>
<p><img src="/images/project2/Figure_8.png" alt="4房子四平面result"></p>
<p>可以看到，四平面插值算法处理后的图像斜线边缘部分是 “光滑连续” 的， 视觉效果比较好，同时有效避免了“锯齿形”边缘现象和“马赛克”现象。</p>
<p>从定量实验的数据角度，我们对于各图像用不同算法完成上述旋转与放缩任务后得到的图像峰值信噪比与算法运行时间进行了计算与统计，结果如下表所示：</p>
<blockquote>
<p>峰值信噪比(PSNR)用于表示信号的最大可能功率与影响其表示的保真度的破坏噪声的功率之间的比率。PSNR在图像处理上主要用于量化受有损压缩影响的图像和视频的重建质量。</p>
<p>PSNR 通过均方误差( MSE ) 定义。</p>
<p>给定一个无噪声的m×n单色图像I及其噪声近似值K，MSE定义为：<br>$$<br>MSE=\frac{1}{mn}\sum_{i=0}^{m-1}\sum_{j=0}^{n-1}[I(i,j)-K(i,j)]^2.<br>$$<br>故PSNR定义为：<br>$$<br>\begin{aligned}\mathrm{PSNR}&amp;=10\cdot\log_{10}\left(\frac{MAX_I^2}{MSE}\right)\&amp;=20\cdot\log_{10}\left(\frac{MAX_I}{\sqrt{MSE}}\right)\&amp;=20\cdot\log_{10}(MAX_I)-10\cdot\log_{10}(MSE).\end{aligned}<br>$$<br>一般而言，通过PSNR来判断处理后图像的失真情况有如下通用结论：</p>
<ul>
<li>PSNR &gt; 30 dB：图像质量较好，失真不明显。</li>
<li>PSNR 20~30 dB：中等质量，存在可察觉失真。</li>
<li>PSNR &lt; 20 dB：质量较差，失真显著。</li>
</ul>
<p>实际计算时，采用opencv自带的PSNR方法cv2.PSNR(img, output)对原始图像与处理后图像的PSNR进行比较计算。</p>
</blockquote>
<table>
<thead>
<tr>
<th>测试图像</th>
<th>最近邻插值PSNR</th>
<th>双线性插值PSNR</th>
<th>双立方插值PSNR</th>
<th>四平面插值PSNR</th>
</tr>
</thead>
<tbody><tr>
<td>琳娜（269*269）</td>
<td>20.74217399</td>
<td>27.11906575</td>
<td>29.36532325</td>
<td>36.70765842</td>
</tr>
<tr>
<td>辣椒（268*268）</td>
<td>22.92424674</td>
<td>27.91435345</td>
<td>31.04713312</td>
<td>39.25866529</td>
</tr>
<tr>
<td>狒狒（268*268）</td>
<td>21.8194312</td>
<td>28.06968286</td>
<td>29.12614241</td>
<td>38.2193871</td>
</tr>
<tr>
<td>房子（256*256）</td>
<td>22.34146151</td>
<td>26.21366716</td>
<td>30.67389681</td>
<td>38.8204405</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>插值算法</th>
<th>最近邻法</th>
<th>双线性法</th>
<th>双立方法</th>
<th>四平面法</th>
</tr>
</thead>
<tbody><tr>
<td>算法运行平均用时</td>
<td>0.678485751</td>
<td>3.293492556</td>
<td>92.66596091</td>
<td>15.02119243</td>
</tr>
</tbody></table>
<p>通过对比上述定量实验结果可以发现，在传统的三种分段插值算法中，随着运算阶数（采样待插值点周围的原图像像素点颜色值信息）的增加，图像经过放缩与旋转处理后的失真程度有明显降低，但仍大致处于存在可察觉失真的区间，且算法运行用时也逐渐增加（事实上双立方法的实现可以在编程层面实现优化，这里只是为更直观地展现O（n^2）时间复杂度在图像大小达到一定规模时的显著影响）；而引入的四平面算法不仅在失真程度上较传统的插值算法均有显著改善，算法运行用时也明显优于传统算法中效果最好的双立方法。</p>
<p>综合以上的定性与定量实验结果及分析，本文提出的基于四平面的图像插值算法在图像处理效果（失真）与运行效率上均较传统算法有明显提升，这充分证明了该算法的有效性。</p>
<p>将上文提到的全部四种算法及旋转与放缩两种功能集成到基于python的gui可视化系统中，并打包成exe可执行文件，制作了一个基于插值的图像处理系统，基本功能演示如下图所示：</p>
<p><img src="/images/project2/1.png" alt="gui演示1"></p>
<p><img src="/images/project2/2.png" alt="gui演示2"></p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] 王开荣,杨大地编著.应用数值分析[M].高等教育出版社,2010.</p>
<p>[2] 毛伟伟,于素萍,石念峰.一种基于四平面的图像插值算法[J].洛阳理工学院学报(自然科学版),2024,34(01):76-81.</p>
<p>[3] 刘显德,李笑.任意大小图像的量子描述及双线性插值方法[J].计算机工程与设计,2024,45(08):2423-2432.</p>
<p>[4] 张喜民,詹海生.基于双三次插值的Canny-Devernay亚像素图像边缘检测算法[J].现代制造工程,2025,(03):107-114.</p>
<p>[5] 陈玲玲,周宁,殷永,等.插值方法在光声图像重建中的应用[J].计算机与数字工程,2013,41(10):1676-1677+1694.</p>
</div></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2025-04-10T09:44:03.000Z" title="2025/4/10 17:44:03">2025-04-10</time>发表</span><span class="level-item"><time datetime="2025-04-29T15:56:14.086Z" title="2025/4/29 23:56:14">2025-04-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/">课程项目</a><span>&nbsp;/&nbsp;</span><a class="link-muted" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E5%B7%A5%E7%A8%8B%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90/">工程数值分析</a></span><span class="level-item">1 小时读完 (大约6923个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2025/04/10/project01/">求矩阵特征值与特征向量：乘幂法及其改进算法</a></p><div class="content"><div id="postchat_postcontent"><h2 id="研究背景意义"><a href="#研究背景意义" class="headerlink" title="研究背景意义"></a>研究背景意义</h2><p>矩阵的特征值计算虽然有比较可靠的理论方法，但是，理论方法只适合于矩阵规模很小或者只是在理论证明中起作用，而实际问题的数据规模都比较大，不太可能采用常规的理论解法。计算机擅长处理大量的数值计算，所以通过适当的数值计算理论，写成程序，让计算机处理，是一种处理大规模矩阵的方法，而且是一种好的方法。乘幂法（又称幂法）是求矩阵按模最大特征值的常用方法，其结构简单、便于使用，在实际工程中应用广泛：</p>
<ul>
<li>在数值分析和优化问题中，乘幂法可通过求解矩阵按模最大特征值来得到其谱半径，从而帮助分析迭代算法的收敛性；</li>
<li>在物理学和工程学中，特征值问题常用于分析系统的稳定性、振动频率和模态分析，乘幂法能够有效求解系统的最大特征值，从而帮助理解系统的动态行为；</li>
<li>在图像处理中，特征值分解用于图像压缩、特征提取和模式识别，乘幂法可用于求解图像矩阵的最大特征值，从而实现高效的图像处理算法；</li>
<li>……</li>
</ul>
<p>但该方法也存在一定的局限性，其只适用于求矩阵按模最大的特征值，而在实际问题中往往需要求矩阵全部特征值，如在主成分分析中需要求相关矩阵的全部特征值以确定各主成分的贡献率，在求解线性微分方程组中通过求系数矩阵的全部特征值以确定其基础解系；同时该方法在求特定矩阵（如具有一对互为相反数的按模最大特征值的矩阵等）的按模最大特征值时是不收敛的。因此，我们希望以乘幂法基本思想为出发点，在此基础上提出一些改进算法以对其进行泛化，使其能够解决更为广泛且通用的应用场景下的实际问题。</p>
<h2 id="算法基本思想"><a href="#算法基本思想" class="headerlink" title="算法基本思想"></a>算法基本思想</h2><p>设n阶矩阵A具有n个线性无关的特征向量<br>$$<br>x_1,x_2,…,x_n<br>$$<br>相应的特征值<br>$$<br>\lambda_1,\lambda_2,…,\lambda_n<br>$$<br>满足：<br>$$<br>\left|\lambda_1\right|&gt;\left|\lambda_2\right|\geq\left|\lambda_3\right|\geq…\geq\left|\lambda_n\right|<br>$$<br>现任取一非零向量u_0，作迭代<br>$$<br>u_k=Au_{k-1}, k=0,1,2,…<br>$$<br>得到向量序列{u_k}(k=0,1,2,…)。因各特征向量线性无关，故n维向量u_0必可由他们线性表示，即：<br>$$<br>u_0=\alpha_{1}x_{1}+\alpha_{n}x_{2}+…+\alpha_{n}x_{n}<br>$$<br>显然有：<br>$$<br>\begin{aligned}u_{k}&amp;=A^{k}v_{0}=\alpha_{1}A^{k}x_{1}+\alpha_{2}A^{k}x_{2}+\cdots+\alpha_{n}A^{k}x_{n}=\alpha_{1}\lambda_{1}^{k}x_{1}+\alpha_{2}\lambda_{2}^{k}x_{2}+\cdots+\alpha_{n}\lambda_{n}^{k}x_{n}\&amp;=\lambda_{1}^{k}\left[\alpha_{1}x_{1}+\alpha_{2}\left(\frac{\lambda_{2}}{\lambda_{1}}\right)^{k}x_{2}+\cdots+\alpha_{n}\left(\frac{\lambda_{n}}{\lambda_{1}}\right)^{k}x_{n}\right]\end{aligned}<br>$$<br>设<br>$$<br>\alpha_{1} \neq 0<br>$$<br>由<br>$$<br>\left|\frac{\lambda_{i}}{\lambda_{1}}\right|&lt;1,i=2,3,\cdots,n<br>$$<br>可得：<br>$$<br>\operatorname*{lim}<em>{k\to\infty}\frac{u</em>{k}}{\lambda_{1}^{k}}=\operatorname*{lim}<em>{k\to\infty}\frac{\lambda</em>{1}^{k}\left[\alpha_{1}x_{1}+\sum_{i=2}^{n}\alpha_{i}\left(\frac{\lambda_{i}}{\lambda_{1}}\right)^{k}x_{i}\right]}{\lambda_{1}^{k}}=\operatorname*{lim}<em>{k\to\infty}\left[\alpha</em>{1}x_{1}+\sum_{i=2}^{n}\alpha_{i}\left(\frac{\lambda_{i}}{\lambda_{1}}\right)^{k}x_{i}\right]=\alpha_{1}x_{1}<br>$$</p>
<p>$$<br>\operatorname*{lim}<em>{k\to\infty}\frac{\left(u</em>{k+1}\right)<em>{m}}{\left(u</em>{k}\right)<em>{m}}=\operatorname*{lim}</em>{k\to\infty}\frac{\left{\lambda_{1}^{k+1}\left[\alpha_{1}x_{1}+\sum_{i=2}^{n}\alpha_{i}\left(\frac{\lambda_{i}}{\lambda_{1}}\right)^{k+1}x_{i}\right]\right}<em>{m}}{\left{\lambda</em>{1}^{k}\left[\alpha_{1}x_{1}+\sum_{i=2}^{n}\alpha_{i}\left(\frac{\lambda_{i}}{\lambda_{i}}\right)^{k}x_{i}\right]\right}<em>{m}}=\lambda</em>{1}\frac{\left(x_{1}\right)<em>{m}}{\left(x</em>{1}\right)<em>{m}}=\lambda</em>{1}<br>$$</p>
<p>这表明向量序列u_k具有收敛性，其收敛速度由比值<br>$$<br>\left|\frac{\lambda_{2}}{\lambda_{1}}\right|<br>$$<br>确定，该比值越小说明收敛速度越快，比值越接近于1收敛速度就越慢。</p>
<p>同时，该结论表明，当k取得足够大时（在实际应用时往往认为某次迭代前后误差小于设定的误差限即满足该条件），有：<br>$$<br>u_{k}≈\lambda_{1}^{k}\alpha_{1}x_{1}, \lambda_{1}≈\frac{\left(u_{k+1}\right)}{\left(u_{k}\right)}<br>$$<br>即u_ k可近似看作特征值λ_ 1对应的特征向量（较原特征向量x_ 1而言仅乘上了有限的常数系数，其结果仍为该特征值对应的特征向量），而对应的按模最大特征值λ_ 1的估计值可由前后两次迭代的特征向量u_ k相除得到。</p>
<p>当矩阵的按模最大特征值是重根时，上述结论仍然成立。设λ_ 1为r重根，即：<br>$$<br>\lambda_{1}=\lambda_{2}=\cdots=\lambda_{r}<br>$$<br>且满足条件<br>$$<br>\mid\lambda_{r}\mid&gt;\mid\lambda_{r+1}\mid\geq\cdots\geq\mid\lambda_{n}\mid<br>$$<br>则有：<br>$$<br>u_{k}=A^{k}u_{0}=\lambda_{1}^{k}\left[\sum_{i=1}^{r}\alpha_{i}x_{i}+\sum_{j=r+1}^{n}\alpha_{j}\left(\frac{\lambda_{j}}{\lambda_{1}}\right)^{k}x_{j}\right]<br>$$<br>易推得（式中(u_ k)_ m表示向量u_ k的第m个分量）：<br>$$<br>\lim_{k\to\infty}\frac{u_{k}}{\lambda_{i}^{k}}=\sum_{i=1}^{^{r}}\alpha_{i}x_{i},\lim_{k\to\infty}\frac{\left(u_{k+1}\right)<em>{m}}{\left(u</em>{k}\right)<em>{m}}=\lambda</em>{1}<br>$$<br>与一般情况略有不同的是，此时<br>$$<br>u_{k}≈\lambda_{i}^{k}\sum_{i=1}^{^{r}}\alpha_{i}x_{i}<br>$$<br>但事实上我们并没有办法在求解之前事先预知一个矩阵是否具有r重按模最大特征值，因此在实际操作的过程中采用与一般情形相同的处理方式（即认为r=1），所得的向量序列u_k仍然收敛，不影响求解结果。</p>
<p>基于以上的算法原理，可以编写如下MATLAB程序，实现一般情况下矩阵按模最大特征值及其对应特征向量的计算：</p>
<figure class="highlight matlab"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">calculatePowerMethod</span><span class="params">(matrixEdit, iterEdit)</span></span></span><br><span class="line">    <span class="comment">% 获取用户输入的矩阵matrixEdit和最大迭代次数iterEdit</span></span><br><span class="line">    A = str2num(matrixEdit.Value); <span class="comment">% 将字符串转换为矩阵</span></span><br><span class="line">    n = <span class="built_in">length</span>(A);</span><br><span class="line">    V = <span class="built_in">rand</span>(n,<span class="number">1</span>);  <span class="comment">% 以随机方式初始化迭代向量u0</span></span><br><span class="line">    max_iter = iterEdit.Value;    <span class="comment">% 获取迭代次数</span></span><br><span class="line"></span><br><span class="line">    Eps = <span class="number">1E-4</span>; <span class="comment">% 迭代精度</span></span><br><span class="line">    k = <span class="number">0</span>; <span class="comment">% 初始迭代次数</span></span><br><span class="line">    lambda0 = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">% 乘幂法迭代，求得矩阵按模最大特征值lambda及其对应特征向量v</span></span><br><span class="line">    <span class="keyword">while</span> k &lt;= max_iter - <span class="number">1</span></span><br><span class="line">        v = A * V;</span><br><span class="line">        [vmax, <span class="built_in">i</span>] = <span class="built_in">max</span>(<span class="built_in">abs</span>(v));</span><br><span class="line">        lambda = v(<span class="built_in">i</span>) / V(<span class="built_in">i</span>);</span><br><span class="line">        V = v;</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">abs</span>(lambda - lambda0)&lt;Eps</span><br><span class="line">            k = k + <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">        lambda0 = lambda;</span><br><span class="line">        k = k + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></tbody></table></figure>

<h2 id="改进的乘幂法"><a href="#改进的乘幂法" class="headerlink" title="改进的乘幂法"></a>改进的乘幂法</h2><p>在实际应用层面，上述的算法逻辑只在一小部分的常规矩阵上对于矩阵的按模最大特征值求解有较好效果。事实上，仅依托以上的代码逻辑进行求解往往会出现以下几大问题：</p>
<ol>
<li>当矩阵A不具有n个线性无关的特征向量时（事先无法判断），乘幂法不适用。</li>
<li>当待求取的按模最大特征值abs(λ_ 1)&gt;1时，迭代向量u_ k的各个分量可能会随着abs(λ_ 1)^k变得很大而使计算机“上溢”；而当待求取的按模最大特征值abs(λ_ 1)&lt;1时，迭代向量u_ k的各个分量可能会随着abs(λ_ 1)^k变得很小使u_k成为零向量。</li>
<li>矩阵的按模最大特征值是一对相反数。</li>
<li>矩阵的按模最大特征值是一对共轭复数。</li>
</ol>
<p>（上述算法中还给出了α_1≠0的假设，事实上如果不满足这一点也不影响乘幂法的成功使用。因为舍入误差的影响，在迭代某一步会产生u_k在x_1方向上的分量不为零，以后的迭代仍会收敛）</p>
<p>针对问题1，我们暂时无法提出具有针对性的解决方案，但后续的实际测试证明，当迭代次数k足够大时，我们针对问题3所提出的改进方案在该种情况下相比基本算法能够有更好的收敛性（收敛更快）；针对问题2，我们可以通过将向量序列u_k进行规范化处理，限制向量的模在特定小范围内，防止计算机运算的上下溢出；针对问题3，我们在原有的乘幂法算法思想的基础上进行了改进，改进后的算法能够有效对原算法无法求解（不收敛）的按模最大特征值互为相反数的矩阵的按模最大特征值进行有效求解；针对问题4，我们目前还没有找到有效的解决方案。</p>
<h3 id="迭代向量的归一化"><a href="#迭代向量的归一化" class="headerlink" title="迭代向量的归一化"></a>迭代向量的归一化</h3><p>设迭代向量u为非零向量，将其归一化得到向量<br>$$<br>y=\frac{u}{max(u)}<br>$$<br>其中max(u)表示向量u的模最大的分量（即向量的无穷范数）。这样的规范化会保证每一次迭代后得到的新的迭代向量的模最大分量均为1，这有效起到了限制迭代向量分量范围的作用，能够很好地解决上述提到的问题2。值得注意的是，对向量规范化的方式并不只有归一化这一种，实际上可以使用任意一种范数对向量进行规范化处理，如2范数等。可以证明，对于基于不同范数的归一化都有相同的向量序列收敛结论，下面以无穷范数为例给出基本的证明。（显然这样的规范化只是在原有的向量基础上除以了某一个常数，这并不会改变向量序列的收敛性）</p>
<p>取初始向量<br>$$<br>u_{0}\neq0<br>$$<br>规范化得<br>$$<br>y_{0}=\frac{u_{0}}{\max(u_{0})}<br>$$<br>构造向量序列：<br>$$<br>\begin{aligned}<br>&amp;u_{1}=Ay_{0}=\frac{Au_{0}}{\max(u_{0})},<br>&amp;y_{1}=\frac{u_{1}}{\max(u_{1})}=\frac{Au_{0}}{\max(Au_{0})}\<br>&amp;u_{2}=Ay_{1}=\frac{A^{2}u_{0}}{\max(Au_{0})},<br>&amp;y_{2}=\frac{u_{2}}{\max(u_{2})}=\frac{A^{2}u_{0}}{\max(A^{2}u_{0})}\<br>&amp;……\<br>&amp;u_{<em>k}=Ay</em>{<em>{k-1}}=\frac{A^{k}u</em>{0}}{\max(A^{k-1}u_{0})},<br>&amp;\quad y_{k}=\frac{u_{k}}{\max(u_{k})}=\frac{A^{k}u_{0}}{\max(A^{k}u_{0})}<br>\end{aligned}<br>$$<br>结合乘幂法基本算法中已经证明的结论<br>$$<br>A^{k}u_{0}=u_{k}≈\lambda_{1}^{k}\alpha_{1}x_{1}<br>$$<br>则有：<br>$$<br>\lim_{k\to\infty}y_{k}=\frac{x_{1}}{\max(x_{1})},<br>\lim_{k\to\infty}\max(u_{k})=\lambda_{1}<br>$$<br>这表明此时y_ k可近似看作特征值λ_ 1对应的特征向量（较原特征向量x_ 1而言仅进行了归一化处理，其结果仍为该特征值对应的特征向量），而对应的按模最大特征值λ_ 1的估计值可近似看作未归一化前的迭代向量最大分量值max(u_k)。</p>
<p>基于以上的算法原理，可以编写如下MATLAB程序，实现归一化处理下矩阵按模最大特征值及其对应特征向量的计算：</p>
<figure class="highlight matlab"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">calculatePowerMethod</span><span class="params">(matrixEdit, iterEdit)</span></span></span><br><span class="line">    <span class="comment">% 获取用户输入的矩阵matrixEdit和最大迭代次数iterEdit</span></span><br><span class="line">    A = str2num(matrixEdit.Value); <span class="comment">% 将字符串转换为矩阵</span></span><br><span class="line">    n = <span class="built_in">length</span>(A);</span><br><span class="line">    V = <span class="built_in">rand</span>(n,<span class="number">1</span>);  <span class="comment">% 以随机方式初始化迭代向量u0</span></span><br><span class="line">    max_iter = iterEdit.Value;    <span class="comment">% 获取迭代次数</span></span><br><span class="line"></span><br><span class="line">    Eps = <span class="number">1E-4</span>; <span class="comment">% 迭代精度</span></span><br><span class="line">    k = <span class="number">0</span>; <span class="comment">% 初始迭代次数</span></span><br><span class="line">    lambda0 = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">% 乘幂法迭代，求得矩阵按模最大特征值lambda及其对应特征向量v</span></span><br><span class="line">    <span class="keyword">while</span> k &lt;= max_iter - <span class="number">1</span></span><br><span class="line">        v = A * V;</span><br><span class="line">        [vmax, <span class="built_in">i</span>] = <span class="built_in">max</span>(<span class="built_in">abs</span>(v));</span><br><span class="line">        lambda = v(<span class="built_in">i</span>);</span><br><span class="line">        V = v / lambda;</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">abs</span>(lambda - lambda0)&lt;Eps</span><br><span class="line">            k = k + <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">        lambda0 = lambda;</span><br><span class="line">        k = k + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="归一化乘幂法的进一步改进"><a href="#归一化乘幂法的进一步改进" class="headerlink" title="归一化乘幂法的进一步改进"></a>归一化乘幂法的进一步改进</h3><p>针对问题3，从其基本情形出发（其他条件与先前保持一致），即有：<br>$$<br>\mid\lambda_1\mid=\mid\lambda_2\mid&gt;\mid\lambda_3\mid\geqslant\cdotp\cdotp\cdotp\geqslant\mid\lambda_n\mid，\lambda_1=-\lambda_2<br>$$<br>在矩阵按模最大特征值为一对相反数的情况下，原迭代向量序列中各相邻向量的比值极限发散：<br>$$<br>\lim_{k\to\infty}\frac{\boldsymbol{u}_{k+1}}{\boldsymbol{u}<em>k}=\lambda_1\lim</em>{k\to\infty}\frac{a_1\boldsymbol{x}_1+(-1)^{k+1}a_2\boldsymbol{x}<em>2+\boldsymbol{\varepsilon}</em>{k+2}}{a_1\boldsymbol{x}_1+(-1)^ka_2\boldsymbol{x}_2+\boldsymbol{\varepsilon}_k}<br>$$<br>说明在该种情况下，采用原有的乘幂法基本算法，得到的迭代向量序列并不收敛，无法得到稳定的矩阵按模最大特征值数值求解结果。</p>
<p>我们在原有算法的基础上进行了一定的改进，构造了新的迭代向量序列，使其对于矩阵按模最大特征值为一对相反数的情况也能保证收敛，从而可以对该情况下的矩阵按模最大特征值进行有效求解。</p>
<p>先从较为简单的非归一化迭代入手，同样取迭代向量序列u_k，其初始非零向量u_0满足：<br>$$<br>u_0=\alpha_{1}x_{1}+\alpha_{n}x_{2}+…+\alpha_{n}x_{n}(\alpha_{1}\neq0)<br>$$<br>则有：<br>$$<br>\boldsymbol{u}<em>{k}=\boldsymbol{A}^{k}\left(\sum</em>{i=1}^{n}a_{i}\boldsymbol{x}<em>{i}\right)=\lambda</em>{1}^{k}\left(a_{1}\boldsymbol{x}<em>{1}+(-1)^{k}a</em>{2}\boldsymbol{x}<em>{2}+\sum</em>{i=3}^{n}\left(\frac{\lambda_{i}}{\lambda_{1}}\right)^{k}a_{i}\boldsymbol{x}<em>{i}\right)=\lambda</em>{1}^{k}(a_{1}\boldsymbol{x}<em>{1}+(-1)^{k}a</em>{2}\boldsymbol{x}<em>{2}+\boldsymbol{\varepsilon}</em>{k}),\lim_{k\to\infty}\varepsilon_{k}=0<br>$$<br>于是可得到：<br>$$<br>\lim_{k\to\infty}\frac{u_{k+2}}{u_{k}}=\lambda_{1}^{2}\lim_{k\to\infty}\frac{\alpha_{1}x_{1}+(-1)^{k+2}\alpha_{2}x_{2}+\varepsilon_{k+2}}{\alpha_{1}x_{1}+(-1)^{k}\alpha_{2}x_{2}+\varepsilon_{k}}=\lambda_{1}^{2}<br>$$<br>且有：<br>$$<br>\lim_{k\to\infty}\frac{u_{k+1}+\lambda_1u_k}{\lambda_1^{k+1}}=2\alpha_1x_1<br>$$<br>以上结果说明，在矩阵按模最大特征值为一对相反数（正值λ_ 1，负值λ_ 2）的情形下，若采用原有的迭代向量序列，对序列中的向量一隔一取出，构成的新序列是收敛的，且新序列中的相邻两项向量的比值极限为矩阵按模最大特征值的平方；同时，u_ {k+1}+λ_ {1}u_ {k}可近似看作λ_ {1}的特征向量，同理u_ {k+1}+λ_ {2}u_ {k}也可近似看作λ_ {2}的特征向量。</p>
<p>易证该方法对于规范化后的向量序列也具有相同的收敛性，在此不额外给出证明。</p>
<p>在实际编程实现算法时，为尽可能地节省算力与内存并提高计算效率，我们不会预先求取最大迭代次数内每一次迭代的结果再将其分为两个向量序列判断其是否收敛（相邻向量插值小于设定误差限），而是需要结合上述改进算法与逐步迭代过程，对于每一次迭代的求解过程进行逻辑优化，优化后的逻辑如下所示（u_k代表规范化后的迭代向量序列，采用2范数方式进行规范化，k=1,2,…）：</p>
<ul>
<li><p>若<br>$$<br>\left|\frac{\left(\boldsymbol{u}_{k+2}^{(1)}\right)_i}{\left(\boldsymbol{u}_k^{(1)}\right)<em>i}-\frac{\left(\boldsymbol{u}</em>{k+1}^{(1)}\right)<em>i}{\left(\boldsymbol{u}</em>{k-1}^{(1)}\right)_i}\right|&lt;\varepsilon:,</p>
<p>\left|\frac{\left(\boldsymbol{u}_{k+2}^{(1)}\right)<em>i}{\left(\boldsymbol{u}</em>{k+1}^{(1)}\right)<em>i}-\frac{\left(\boldsymbol{u}</em>{k+1}^{(1)}\right)_i}{\left(\boldsymbol{u}_k^{(1)}\right)<em>i}\right|&lt;\varepsilon:<br>$$<br>则取矩阵按模最大特征值<br>$$<br>\lambda_1=\frac{(\boldsymbol{u}</em>{k+2}^{(1)})<em>i}{(\boldsymbol{u}</em>{k+1}^{(1)})<em>i}<br>$$<br>对应的特征向量<br>$$<br>x_1=u</em>{k+2}^{(1)}<br>$$</p>
</li>
<li><p>若<br>$$<br>\left|\frac{\left(\boldsymbol{u}_{k+2}^{(1)}\right)_i}{\left(\boldsymbol{u}_k^{(1)}\right)<em>i}-\frac{\left(\boldsymbol{u}</em>{k+1}^{(1)}\right)<em>i}{\left(\boldsymbol{u}</em>{k-1}^{(1)}\right)_i}\right|&lt;\varepsilon:,</p>
<p>\left|\frac{(\boldsymbol{u}<em>{k+2}^{(1)})<em>i}{(\boldsymbol{u}</em>{k+1}^{(1)})<em>i}-\frac{(\boldsymbol{u}</em>{k+1}^{(1)})<em>i}{(\boldsymbol{u}<em>k^{(1)})<em>i}\right|&gt;\varepsilon:<br>$$<br>则取矩阵按模最大特征值<br>$$<br>\lambda_1=\pm\sqrt{\frac{(\boldsymbol{u}</em>{k+2}^{(1)})<em>i}{(\boldsymbol{u}</em>{k}^{(1)})<em>i}}<br>$$<br>对应的特征向量<br>$$<br>x_1=\frac{u</em>{k+2}^{(1)}+\lambda_1u</em>{k+1}^{(1)}}{\parallel u</em>{k+2}^{(1)}+\lambda_1u</em>{k+1}^{(1)}\parallel_2}<br>$$</p>
</li>
</ul>
<p>基于以上的算法原理，可以编写如下MATLAB程序，实现矩阵按模最大特征值及其对应特征向量求解的改进算法，以解决矩阵按模最大特征值可能为一对相反数的特殊情况：</p>
<figure class="highlight matlab"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">calculatePowerMethod1</span><span class="params">(matrixEdit, iterEdit, fig)</span></span></span><br><span class="line">    <span class="comment">% 获取用户输入的矩阵matrixEdit和最大迭代次数iterEdit</span></span><br><span class="line">    A = str2num(matrixEdit.Value); <span class="comment">% 将字符串转换为矩阵</span></span><br><span class="line">    n = <span class="built_in">length</span>(A);</span><br><span class="line">    V = <span class="built_in">rand</span>(n,<span class="number">1</span>);  <span class="comment">% 以随机方式初始化迭代向量u0</span></span><br><span class="line">    max_iter = iterEdit.Value;    <span class="comment">% 获取迭代次数</span></span><br><span class="line"></span><br><span class="line">    Eps = <span class="number">1E-4</span>; <span class="comment">% 迭代精度</span></span><br><span class="line">    k = <span class="number">0</span>; <span class="comment">% 初始迭代次数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">% 乘幂法迭代，求得矩阵按模最大特征值lambda及其对应特征向量v</span></span><br><span class="line">    <span class="keyword">while</span> k &lt;= max_iter - <span class="number">1</span></span><br><span class="line">        v = A * V;</span><br><span class="line">        v1 = A * v;</span><br><span class="line">        v_2 =norm(v, <span class="number">2</span>);</span><br><span class="line">        minus1 = A * v1 ./ v - v1 ./ V;</span><br><span class="line">        minus2 = A * v1 ./ v1 - v1 ./ v;</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">max</span>(<span class="built_in">abs</span>(minus1(:))) &lt; Eps</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">max</span>(<span class="built_in">abs</span>(minus2(:))) &lt; Eps</span><br><span class="line">                lambda0 = A * v1 ./ v1;</span><br><span class="line">                lambda = <span class="built_in">max</span>(lambda0(:));</span><br><span class="line">                v2_2 = norm(A * v1, <span class="number">2</span>);       </span><br><span class="line">                v = A * v1 / v2_2;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                lambda0 = A * v1 ./ v;</span><br><span class="line">                lambda = <span class="built_in">sqrt</span>(<span class="built_in">max</span>(lambda0(:)));</span><br><span class="line">                v3 = A * v1 + lambda * v1;</span><br><span class="line">                v3_2 = norm(v3, <span class="number">2</span>);</span><br><span class="line">                v = v3 / v3_2;</span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line">            k = k + <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">        k = k + <span class="number">1</span>;</span><br><span class="line">        V = v / v_2;</span><br><span class="line">        lambda0 = A * v1 ./ v1;</span><br><span class="line">        lambda = <span class="built_in">max</span>(lambda0(:));</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="求实对称矩阵的全部特征值"><a href="#求实对称矩阵的全部特征值" class="headerlink" title="求实对称矩阵的全部特征值"></a>求实对称矩阵的全部特征值</h3><p>我们知道，实对称矩阵的不同特征值对应的特征向量一定是正交的，因此，可以通过幂法迭代得到矩阵的主特征值λ_ 1和主特征向量x_ 1 ，再重新给出一个新的与v_ 0线性无关的初始迭代向量v_ 1，使v_ 1和x_ 1正交化， 则可以由幂法迭代出矩阵的特征值λ_ 2和特征向量x_ 2。同样使新给出的初始迭代向量v_ 3和x_ 1、x_ 2正交 化，则可以由幂法得到λ_ 3和x_ 3。以此类推，可以计算出矩阵的全部特征值和特征向量。具体证明过程详见参考文献[3]，这里仅给出基于上述改进算法的完整算法逻辑：</p>
<p>令<br>$$<br>A_j=\mathbf{A}<em>{j-1}-\lambda</em>{j-1}\boldsymbol{x}<em>{j-1}\boldsymbol{x}</em>{j-1}^{\mathrm{T}}(j=2,3,\cdots n)<br>$$<br>采取与先前完全相同的迭代向量序列构造方式，有如下判断逻辑（u_k代表规范化后的迭代向量序列，采用2范数方式进行规范化，k=1,2,…）：</p>
<ul>
<li><p>若<br>$$<br>\left|\frac{(u_{k+2}^{(j)})<em>i}{(u_k^{(j)})<em>i}-\frac{(u</em>{k+1}^{(j)})<em>i}{(u</em>{k-1}^{(j)})<em>i}\right|&lt;\varepsilon:,\quad\left|\frac{(u</em>{k+2}^{(j)})<em>i}{(u</em>{k+1}^{(j)})<em>i}-\frac{(u</em>{k+1}^{(j)})<em>i}{(u_k^{(j)})<em>i}\right|&lt;\varepsilon:,<br>$$<br>且满足<br>$$<br>{|x</em>{m}^{\mathrm{T}}v</em>{k+2}^{(j)}|&lt;\varepsilon}\left(m=1,2,\cdots,j-1\right)<br>$$<br>则取矩阵按模最大特征值<br>$$<br>{\lambda</em>{j}}=\frac{\left(\boldsymbol{u}<em>{k+2}^{(j)}\right)</em>{i}}{\left(\boldsymbol{u}<em>{k+1}^{(j)}\right)</em>{i}}<br>$$<br>对应的特征向量<br>$$<br>x_{j}=\boldsymbol{u}_{k+2}^{(j)}<br>$$</p>
</li>
<li><p>若<br>$$<br>\left|\frac{(u_{k+2}^{(j)})<em>i}{(u_k^{(j)})<em>i}-\frac{(u</em>{k+1}^{(j)})<em>i}{(u</em>{k-1}^{(j)})<em>i}\right|&lt;\varepsilon:,\quad\left|\frac{(u_{k+2}^{(j)})_i}{(u_{k+1}^{(j)})_i}-\frac{(u_{k+1}^{(j)})_i}{(u_k^{(j)})_i}\right|&gt;\varepsilon:,<br>$$<br>且满足<br>$$<br>\left|\boldsymbol{x}</em>{m}^{\mathrm{T}}\frac{\boldsymbol{u}</em>{k+2}^{(j)}+\lambda_{j}\boldsymbol{u}<em>{k+1}^{(j)}}{\parallel\boldsymbol{u}</em>{k+2}^{(j)}+\lambda_{j}\boldsymbol{u}<em>{k+1}^{(j)}\parallel</em>{2}}\right|&lt;\varepsilon\quad(m=1,2,\cdots,j-1)<br>$$<br>则取矩阵按模最大特征值<br>$$<br>\lambda_{j}=\pm\sqrt{\frac{(\boldsymbol{u}<em>{k+2}^{(j)})</em>{i}}{(\boldsymbol{u}<em>{k}^{(j)})</em>{i}}}<br>$$<br>对应的特征向量<br>$$<br>\quad x_{j}=\frac{\boldsymbol{u}<em>{k+2}^{(j)}+\lambda</em>{j}\boldsymbol{u}<em>{k+1}^{(j)}}{\parallel\boldsymbol{u}</em>{k+2}^{(j)}+\lambda_{j}\boldsymbol{u}<em>{k+1}^{(j)}\parallel</em>{2}}<br>$$</p>
</li>
</ul>
<p>基于以上的算法原理，可以编写如下MATLAB程序，实现矩阵按模最大特征值及其对应特征向量求解的改进算法，并对于实对称矩阵的全部特征值进行依次求解（每次运行该函数进行一个特征值的求解，代码中省去了存储用于求解下一个特征值的矩阵作为全局变量的部分）：</p>
<figure class="highlight matlab"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">calculatePowerMethod2</span><span class="params">(iterEdit, fig)</span></span></span><br><span class="line">    <span class="comment">% 从存储的全局变量中读取需要求解主特征值的新矩阵data.matrix</span></span><br><span class="line">    A = data.matrix; <span class="comment">% 将字符串转换为矩阵</span></span><br><span class="line">    n = <span class="built_in">length</span>(A);</span><br><span class="line">    V = <span class="built_in">rand</span>(n,<span class="number">1</span>); </span><br><span class="line">    max_iter = iterEdit.Value;    <span class="comment">% 获取迭代次数</span></span><br><span class="line"></span><br><span class="line">    Eps = <span class="number">1E-4</span>; <span class="comment">% 迭代精度</span></span><br><span class="line">    k = <span class="number">0</span>; <span class="comment">% 初始迭代次数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">% 乘幂法迭代，求得矩阵按模最大特征值lambda及其对应特征向量v</span></span><br><span class="line">    <span class="keyword">while</span> k &lt;= max_iter - <span class="number">1</span></span><br><span class="line">        v = A * V;</span><br><span class="line">        v1 = A * v;</span><br><span class="line">        v_2 =norm(v, <span class="number">2</span>);</span><br><span class="line">        v2_2 = norm(A * v1, <span class="number">2</span>);</span><br><span class="line">        v2 = A * v1 / v2_2;</span><br><span class="line">        lambda0 = A * v1 ./ v;</span><br><span class="line">        lambda = <span class="built_in">sqrt</span>(<span class="built_in">max</span>(lambda0(:)));</span><br><span class="line">        v3 = A * v1 + lambda * v1;</span><br><span class="line">        v3_2 = norm(v3, <span class="number">2</span>);</span><br><span class="line">        minus1 = A * v1 ./ v - v1 ./ V;</span><br><span class="line">        minus2 = A * v1 ./ v1 - v1 ./ v;</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">max</span>(<span class="built_in">abs</span>(minus1(:)))&lt;Eps</span><br><span class="line">            num1 = <span class="number">0</span>;</span><br><span class="line">            num2 = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> l = <span class="number">1</span> : <span class="built_in">j</span></span><br><span class="line">                x = full_V(:,l);</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">max</span>(<span class="built_in">abs</span>(minus2(:))) &lt; Eps &amp;&amp; <span class="built_in">abs</span>(x' * v2) &lt; Eps</span><br><span class="line">                    num1 = num1 + <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">elseif</span> <span class="built_in">max</span>(<span class="built_in">abs</span>(minus2(:))) &gt; Eps &amp;&amp; <span class="built_in">abs</span>(x' * v3 / v3_2) &lt; Eps</span><br><span class="line">                    num2 = num2 + <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">end</span></span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line">            <span class="keyword">if</span> num1 == <span class="built_in">j</span></span><br><span class="line">                lambda0 = A * v1 ./ v1;</span><br><span class="line">                lambda = <span class="built_in">max</span>(lambda0(:));</span><br><span class="line">                v = A * v1 / v2_2;</span><br><span class="line">                k = k + <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">elseif</span> num2 == <span class="built_in">j</span></span><br><span class="line">                lambda0 = A * v1 ./ v;</span><br><span class="line">                lambda = <span class="built_in">sqrt</span>(<span class="built_in">max</span>(lambda0(:)));</span><br><span class="line">                v = v3 / v3_2;</span><br><span class="line">                k = k + <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">        k = k + <span class="number">1</span>;</span><br><span class="line">        V = v / v_2;</span><br><span class="line">        lambda0 = A * v1 ./ v1;</span><br><span class="line">        lambda = <span class="built_in">max</span>(lambda0(:));</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></tbody></table></figure>

<h2 id="数据测试实验"><a href="#数据测试实验" class="headerlink" title="数据测试实验"></a>数据测试实验</h2><p>针对以上提到的乘幂法及其改进算法，将其MATLAB求解代码整合进GUI中，并对求解过程中矩阵按模最大特征值随迭代次数增加的收敛情况进行可视化，基本界面如下：</p>
<p><img src="/images/project01/1.png" alt="1"></p>
<h3 id="乘幂法基础算法"><a href="#乘幂法基础算法" class="headerlink" title="乘幂法基础算法"></a>乘幂法基础算法</h3><p>将待求解的矩阵按照格式要求输入框体内，并指定最大迭代次数（默认为100次），按下“计算”控件可以利用基础的乘幂法（归一化版本）对矩阵按模最大特征值及其对应特征向量进行求解，同时会调用MATLAB中自带的矩阵特征值求解函数eig()对求解结果进行验证。</p>
<h4 id="例：矩阵（特征值为-1、3、5）"><a href="#例：矩阵（特征值为-1、3、5）" class="headerlink" title="例：矩阵（特征值为-1、3、5）"></a>例：矩阵（特征值为-1、3、5）</h4><p>$$<br>A=\begin{pmatrix}1&amp;-1&amp;2\-2&amp;0&amp;5\6&amp;-3&amp;6\end{pmatrix}<br>$$</p>
<p>程序求解与可视化结果：</p>
<p><img src="/images/project01/2.png" alt="2"></p>
<p>可以发现，估计的主特征值与实际的主特征值在限定的精度内基本一致，对应的特征向量相差了相同的倍数，也可视为正确求解。</p>
<h3 id="乘幂法改进算法"><a href="#乘幂法改进算法" class="headerlink" title="乘幂法改进算法"></a>乘幂法改进算法</h3><p>将待求解的矩阵按照格式要求输入框体内，并指定最大迭代次数（默认为100次），按下“优化计算”控件可以利用改进的乘幂法（归一化版本）对矩阵按模最大特征值及其对应特征向量进行求解，同时会调用MATLAB中自带的矩阵特征值求解函数eig()对求解结果进行验证。</p>
<h4 id="例1：矩阵（特征值为-1、3、5）"><a href="#例1：矩阵（特征值为-1、3、5）" class="headerlink" title="例1：矩阵（特征值为-1、3、5）"></a>例1：矩阵（特征值为-1、3、5）</h4><p>$$<br>A=\begin{pmatrix}1&amp;-1&amp;2\-2&amp;0&amp;5\6&amp;-3&amp;6\end{pmatrix}<br>$$</p>
<p>程序求解与可视化结果：</p>
<p><img src="/images/project01/3.png" alt="3"></p>
<p>可以发现，估计的主特征值与实际的主特征值在限定的精度内保持一致，通过该算法计算得到的对应特征向量也与MATLAB自带函数（基于 Cholesky 分解/QR分解方法的特征值求解）的求解结果在限定精度内完全一致，但计算需要的迭代次数较基础算法而言有明显增加。</p>
<h4 id="例2：矩阵（特征值为3、-3、1，按模最大特征值为一对相反数）"><a href="#例2：矩阵（特征值为3、-3、1，按模最大特征值为一对相反数）" class="headerlink" title="例2：矩阵（特征值为3、-3、1，按模最大特征值为一对相反数）"></a>例2：矩阵（特征值为3、-3、1，按模最大特征值为一对相反数）</h4><p>$$<br>A=\begin{pmatrix}3&amp;-2&amp;4\0&amp;-3&amp;2\0&amp;0&amp;1\end{pmatrix}<br>$$</p>
<p>程序求解与可视化结果：</p>
<p>调用乘幂法基本算法：</p>
<p><img src="/images/project01/4.png" alt="4"></p>
<p>调用乘幂法改进算法：</p>
<p><img src="/images/project01/5.png" alt="5"></p>
<p>可以发现：</p>
<ul>
<li>调用基础算法时，迭代向量序列在设定的最大迭代次数内并不收敛，最终的求解结果也与实际按模最大特征值有较大误差；</li>
<li>而调用改进算法时，尽管从主特征值收敛过程的可视化结果中无法确定向量序列的收敛性（实际上原序列仍发散），但序列中的向量一隔一选取构成的新序列（图像中对应迭代次数全为奇数/偶数的震荡单边）已由程序判断收敛，且估计的主特征值与实际的主特征值在限定的精度内保持一致，通过该算法计算得到的对应特征向量也与MATLAB自带函数（基于 Cholesky 分解/QR分解方法的特征值求解）的求解结果在限定精度内完全一致。</li>
</ul>
<h4 id="例3：3阶矩阵（特征值为3（重根），但不具有3个线性无关的特征向量）"><a href="#例3：3阶矩阵（特征值为3（重根），但不具有3个线性无关的特征向量）" class="headerlink" title="例3：3阶矩阵（特征值为3（重根），但不具有3个线性无关的特征向量）"></a>例3：3阶矩阵（特征值为3（重根），但不具有3个线性无关的特征向量）</h4><p>$$<br>A=\begin{pmatrix}3&amp;1&amp;0\0&amp;3&amp;1\0&amp;0&amp;3\end{pmatrix}<br>$$</p>
<p>程序求解与可视化结果：</p>
<p>调用乘幂法基本算法（最大迭代次数100）：</p>
<p><img src="/images/project01/6.png" alt="6"></p>
<p>调用乘幂法改进算法（最大迭代次数100）：</p>
<p><img src="/images/project01/7.png" alt="7"></p>
<p>调用乘幂法基本算法（最大迭代次数10000）：</p>
<p><img src="/images/project01/8.png" alt="8"></p>
<p>调用乘幂法改进算法（最大迭代次数10000）：</p>
<p><img src="/images/project01/9.png" alt="9"></p>
<p>可以发现，当矩阵不具有n个线性无关的特征向量时，乘幂法从理论上讲不再适用（不满足假设条件），但实际测试可以发现：</p>
<ul>
<li>当最大迭代次数设置较小（100）时，两种方法均未呈现数值上的收敛（两次迭代间误差不超过设定误差限10^(-4)），所求得的主特征值与对应特征向量与真实值之间均有明显误差；</li>
<li>当最大迭代次数设置足够大（10000）时，两种方法在迭代次数达到10^2数量级时实现了数值上的收敛（两次迭代间误差不超过设定误差限10^(-4)），但在停止迭代后，通过乘幂法基础算法求得的主特征值与对应特征向量与真实值之间仍有明显误差，而通过改进算法求得的主特征值与对应特征向量与真实值已经基本接近，但改进算法所需要的迭代次数也明显更多。</li>
</ul>
<p>但总的来说，从理论角度出发，在这种情况下乘幂法已不再适用，本例的情况仅为收敛较慢，此时改进算法相较于基础算法有更好的精度；但也会有不收敛的情况，此时两种方法均会有比较大的误差，而这是无法事先判断的，因此在使用乘幂法时还是尽量避免该种情况，或者说在优化算法仍然无法快速收敛的情况下需要注意到矩阵不具有n个线性无关的特征向量的特殊情况，此时应选取其他合适的特征值求解算法。</p>
<h4 id="例4：矩阵（特征值为1-2i、1-2i、0-5，按模最大特征值为一对共轭复数）"><a href="#例4：矩阵（特征值为1-2i、1-2i、0-5，按模最大特征值为一对共轭复数）" class="headerlink" title="例4：矩阵（特征值为1+2i、1-2i、0.5，按模最大特征值为一对共轭复数）"></a>例4：矩阵（特征值为1+2i、1-2i、0.5，按模最大特征值为一对共轭复数）</h4><p>$$<br>A=\begin{pmatrix}1&amp;-2&amp;0\2&amp;1&amp;0\0&amp;0&amp;0.5\end{pmatrix}<br>$$</p>
<p>程序求解与可视化结果：</p>
<p>调用乘幂法基本算法：</p>
<p><img src="/images/project01/10.png" alt="10"></p>
<p>调用乘幂法改进算法：</p>
<p><img src="/images/project01/11.png" alt="11"></p>
<p>此情况下两种乘幂法均呈现了极大的不稳定性，无法正确求解。</p>
<h3 id="求实对称矩阵的全部特征值-1"><a href="#求实对称矩阵的全部特征值-1" class="headerlink" title="求实对称矩阵的全部特征值"></a>求实对称矩阵的全部特征值</h3><h4 id="例1：矩阵（特征值为-1、3、5）-1"><a href="#例1：矩阵（特征值为-1、3、5）-1" class="headerlink" title="例1：矩阵（特征值为-1、3、5）"></a>例1：矩阵（特征值为-1、3、5）</h4><p>$$<br>A=\begin{pmatrix}1&amp;-1&amp;2\-2&amp;0&amp;5\6&amp;-3&amp;6\end{pmatrix}<br>$$</p>
<p>程序求解与可视化结果：</p>
<p><img src="/images/project01/12.png" alt="12"></p>
<p>该矩阵不为对称矩阵，无法通过上述算法实现全部特征值的求解，为排错示例。</p>
<h4 id="例2：对称矩阵（特征值为1、2、4）"><a href="#例2：对称矩阵（特征值为1、2、4）" class="headerlink" title="例2：对称矩阵（特征值为1、2、4）"></a>例2：对称矩阵（特征值为1、2、4）</h4><p>$$<br>A=\begin{pmatrix}3&amp;1&amp;1\1&amp;2&amp;0\1&amp;0&amp;2\end{pmatrix}<br>$$</p>
<p>程序求解与可视化结果：</p>
<p><img src="/images/project01/13.png" alt="13"></p>
<p><img src="/images/project01/14.png" alt="14"></p>
<p><img src="/images/project01/15.png" alt="15"></p>
<p><img src="/images/project01/16.png" alt="16"></p>
<p>可以看到，该算法可以用于正确求取实对称矩阵的所有特征值及其相应的特征向量，求解结果与MATLAB自带函数（基于 Cholesky 分解/QR分解方法的特征值求解）的求解结果在限定精度内完全一致。</p>
<h4 id="例3：对称矩阵（特征值为2、1、-1）"><a href="#例3：对称矩阵（特征值为2、1、-1）" class="headerlink" title="例3：对称矩阵（特征值为2、1、-1）"></a>例3：对称矩阵（特征值为2、1、-1）</h4><p>$$<br>A=\begin{pmatrix}1&amp;1&amp;0\1&amp;0&amp;1\0&amp;1&amp;1\end{pmatrix}<br>$$</p>
<p>程序求解与可视化结果：</p>
<p><img src="/images/project01/17.png" alt="17"></p>
<p><img src="/images/project01/18.png" alt="18"></p>
<p><img src="/images/project01/19.png" alt="19"></p>
<p><img src="/images/project01/20.png" alt="20"></p>
<p>可以看到，即使在矩阵的次主特征值为一对相反数的情况下，该算法也可以用于正确求取实对称矩阵的所有特征值及其相应的特征向量，求解结果与MATLAB自带函数（基于 Cholesky 分解/QR分解方法的特征值求解）的求解结果在限定精度内完全一致。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] 王开荣,杨大地编著.应用数值分析[M].高等教育出版社,2010.</p>
<p>[2] 曹连英,曲智林,杨瑞智.基于幂法的求实对称矩阵特征值的注记[J].大学数学,2024,40(04):67-72.</p>
<p>[3] 曾莉,肖明.计算实对称矩阵特征值特征向量的幂法[J].南昌大学学报(理科版),2016,40(04):399-402.</p>
<p>[4] 张青,苟国楷,吕崇德.乘幂法的改进算法[J].应用数学与计算数学学报,1997,(01):51-55.</p>
<p>[5] 马志勇,方珑.矩阵特征值求解及其在图像压缩中的应用[J].上海第二工业大学学报,2012,29(04):315-318.</p>
</div></div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/head2.png" alt="Jinghua Xu"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Jinghua Xu</p><p class="is-size-6 is-block">明月科创实验班人工智能专业 本科大三在读</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>重庆 重庆大学国家卓越工程师学院</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">37</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">24</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">102</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Asgard-Tim" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Asgard-Tim"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Weibo" href="https://www.weibo.com/u/6315188431"><i class="fab fa-weibo"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Bilibili" href="https://space.bilibili.com/171895120"><i class="fab fa-bilibili"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="mailto:20224546@stu.cqu.edu.cn"><i class="fas fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Phone" href="tel:+86 19132050174"><i class="fas fa-phone"></i></a></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><figure class="media-left"><a class="image" href="/2025/06/29/manufac/"><img src="/images/manufac/b99d7042c32e0662693c9051eb8316a5.jpg" alt="小提琴自动演奏机器人中的齿轮系统设计与制造"></a></figure><div class="media-content"><p class="date"><time datetime="2025-06-29T08:40:43.000Z">2025-06-29</time></p><p class="title"><a href="/2025/06/29/manufac/">小提琴自动演奏机器人中的齿轮系统设计与制造</a></p><p class="categories"><a href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/">课程项目</a> / <a href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E4%BA%A7%E5%93%81%E5%88%B6%E9%80%A0/">产品制造</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2025/06/21/sweeping/"><img src="/images/sweeping/0c9ca142c80746ccde051fd86d54a57c.png" alt="SmartRobot扫地机器人"></a></figure><div class="media-content"><p class="date"><time datetime="2025-06-20T20:02:03.000Z">2025-06-21</time></p><p class="title"><a href="/2025/06/21/sweeping/">SmartRobot扫地机器人</a></p><p class="categories"><a href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/">课程项目</a> / <a href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E5%BE%AE%E7%94%B5%E8%B7%AF%E8%AE%BE%E8%AE%A1/">微电路设计</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2025/06/10/project3/"><img src="/images/project3/9.png" alt="常微分方程反演的机器学习方法"></a></figure><div class="media-content"><p class="date"><time datetime="2025-06-09T18:59:03.000Z">2025-06-10</time></p><p class="title"><a href="/2025/06/10/project3/">常微分方程反演的机器学习方法</a></p><p class="categories"><a href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/">课程项目</a> / <a href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E5%B7%A5%E7%A8%8B%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90/">工程数值分析</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2025/05/15/Survey/"><img src="/images/survey/2.png" alt="A Survey on Vision-Language-Action Models for Embodied AI"></a></figure><div class="media-content"><p class="date"><time datetime="2025-05-15T15:32:03.000Z">2025-05-15</time></p><p class="title"><a href="/2025/05/15/Survey/">A Survey on Vision-Language-Action Models for Embodied AI</a></p><p class="categories"><a href="/categories/%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">具身智能论文阅读</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2025/05/09/PaLM-E/"><img src="/images/palm-e/0.png" alt="PaLM-E：An Embodied Multimodal Language Model"></a></figure><div class="media-content"><p class="date"><time datetime="2025-05-09T11:57:03.000Z">2025-05-09</time></p><p class="title"><a href="/2025/05/09/PaLM-E/">PaLM-E：An Embodied Multimodal Language Model</a></p><p class="categories"><a href="/categories/%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">具身智能论文阅读</a></p></div></article></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/About-XJH/"><span class="level-start"><span class="level-item">About XJH</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/About-XJH/%E6%98%8E%E6%85%B5/"><span class="level-start"><span class="level-item">明慵</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/About-XJH/%E6%98%8E%E8%AF%9A/"><span class="level-start"><span class="level-item">明诚</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE/"><span class="level-start"><span class="level-item">个人项目</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"><span class="level-start"><span class="level-item">具身智能论文阅读</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">动手学深度学习</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%E8%AF%BE/"><span class="level-start"><span class="level-item">算法基础课</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/"><span class="level-start"><span class="level-item">课程项目</span></span><span class="level-end"><span class="level-item tag">26</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E4%BA%A7%E5%93%81%E5%88%B6%E9%80%A0/"><span class="level-start"><span class="level-item">产品制造</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E4%BA%A7%E5%93%81%E8%AE%BE%E8%AE%A1/"><span class="level-start"><span class="level-item">产品设计</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E5%AE%9A%E9%87%8F%E5%B7%A5%E7%A8%8B%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%B3%95I/"><span class="level-start"><span class="level-item">定量工程设计方法I</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E5%AE%9A%E9%87%8F%E5%B7%A5%E7%A8%8B%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%B3%95II/"><span class="level-start"><span class="level-item">定量工程设计方法II</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E5%B7%A5%E6%95%88%E5%AD%A6/"><span class="level-start"><span class="level-item">工效学</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E5%B7%A5%E7%A8%8B%E5%8E%9F%E7%90%86/"><span class="level-start"><span class="level-item">工程原理</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E5%B7%A5%E7%A8%8B%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90/"><span class="level-start"><span class="level-item">工程数值分析</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E5%B7%A5%E7%A8%8B%E8%AE%BE%E8%AE%A1/"><span class="level-start"><span class="level-item">工程设计</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E5%BE%AE%E7%94%B5%E8%B7%AF%E8%AE%BE%E8%AE%A1/"><span class="level-start"><span class="level-item">微电路设计</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E6%95%B0%E5%AD%A6%E7%89%A9%E7%90%86%E6%96%B9%E6%B3%95/"><span class="level-start"><span class="level-item">数学物理方法</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E6%99%BA%E8%83%BD%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"><span class="level-start"><span class="level-item">智能图像处理</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%9F%BA%E7%A1%80/"><span class="level-start"><span class="level-item">机器人基础</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/"><span class="level-start"><span class="level-item">概率论与数理统计</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"><span class="level-start"><span class="level-item">线性代数</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E8%87%AA%E5%8A%A8%E6%8E%A7%E5%88%B6%E5%8E%9F%E7%90%86/"><span class="level-start"><span class="level-item">自动控制原理</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BE%E7%A8%8B%E9%A1%B9%E7%9B%AE/%E8%BD%AF%E4%BB%B6%E8%AE%BE%E8%AE%A1/"><span class="level-start"><span class="level-item">软件设计</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2025/06/"><span class="level-start"><span class="level-item">六月 2025</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/05/"><span class="level-start"><span class="level-item">五月 2025</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/04/"><span class="level-start"><span class="level-item">四月 2025</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/03/"><span class="level-start"><span class="level-item">三月 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/02/"><span class="level-start"><span class="level-item">二月 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/01/"><span class="level-start"><span class="level-item">一月 2025</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/12/"><span class="level-start"><span class="level-item">十二月 2024</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/06/"><span class="level-start"><span class="level-item">六月 2024</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/02/"><span class="level-start"><span class="level-item">二月 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/01/"><span class="level-start"><span class="level-item">一月 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/12/"><span class="level-start"><span class="level-item">十二月 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/11/"><span class="level-start"><span class="level-item">十一月 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/10/"><span class="level-start"><span class="level-item">十月 2023</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/09/"><span class="level-start"><span class="level-item">九月 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/07/"><span class="level-start"><span class="level-item">七月 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/06/"><span class="level-start"><span class="level-item">六月 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/05/"><span class="level-start"><span class="level-item">五月 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/03/"><span class="level-start"><span class="level-item">三月 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/02/"><span class="level-start"><span class="level-item">二月 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/%E5%8D%95%E7%89%87%E6%9C%BA/"><span class="tag">单片机</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/STM32/"><span class="tag">STM32</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VLA/"><span class="tag">VLA</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD/"><span class="tag">具身智能</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/"><span class="tag">多模态大模型</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C++</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%AF%E7%89%B9%E6%9E%97%E5%8F%91%E5%8A%A8%E6%9C%BA/"><span class="tag">斯特林发动机</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MATLAB/"><span class="tag">MATLAB</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Matlab/"><span class="tag">Matlab</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="tag">机器学习</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="tag">动手学深度学习</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><span class="tag">学习笔记</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%B0%8F%E8%BD%A6/"><span class="tag">小车</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BF%A1%E5%8F%B7%E4%B8%8E%E7%B3%BB%E7%BB%9F/"><span class="tag">信号与系统</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%A2%91%E8%B0%B1%E5%88%86%E6%9E%90/"><span class="tag">频谱分析</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%AF%E7%89%B9%E6%9E%97%E5%BE%AA%E7%8E%AF/"><span class="tag">斯特林循环</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8A%A8%E5%8A%9B%E5%AD%A6%E4%BB%BF%E7%9C%9F/"><span class="tag">动力学仿真</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%89%E9%99%90%E5%85%83%E4%BB%BF%E7%9C%9F/"><span class="tag">有限元仿真</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"><span class="tag">算法与数据结构</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/3D-VLA/"><span class="tag">3D-VLA</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PaLM-E/"><span class="tag">PaLM-E</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%B7%AF%E5%BE%84%E8%A7%84%E5%88%92/"><span class="tag">路径规划</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RANSAC/"><span class="tag">RANSAC</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%88%86%E6%9E%90/"><span class="tag">数据处理分析</span><span class="tag">1</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/title1.png" alt="Homepage of Jinghua Xu" height="28"></a><p class="is-size-7"><span>© 2025 Tim</span>&nbsp;&nbsp;Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>&nbsp;&amp;&nbsp;<a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© Copyright by Jinghua Xu</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Asgard-Tim"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer=""></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer=""></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer=""></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer=""></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer=""></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" defer=""></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" defer=""></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mhchem.min.js" defer=""></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer=""></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer=""></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer=""></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script>
    <link rel="stylesheet" href="https://ai.tianli0.top/static/public/postChatUser_summary.min.css">
    <script>
        let tianliGPT_key = 'S-TA3IX28M1ZT7TILW';
        let tianliGPT_postSelector = '#postchat_postcontent';
        let tianliGPT_Title = '文章摘要';
        let tianliGPT_postURL = '/^https?://[^/]+/[0-9]{4}/[0-9]{2}/[0-9]{2}/';
        let tianliGPT_blacklist = '';
        let tianliGPT_wordLimit = '1000';
        let tianliGPT_typingAnimate = true;
        let tianliGPT_theme = 'default';
        var postChatConfig = {
          backgroundColor: "#3e86f6",
          bottom: "16px",
          left: "16px",
          fill: "#FFFFFF",
          width: "44px",
          frameWidth: "375px",
          frameHeight: "600px",
          defaultInput: true,
          upLoadWeb: true,
          showInviteLink: true,
          userTitle: "PostChat",
          userDesc: "如果你对网站的内容有任何疑问，可以来问我哦～",
          addButton: true,
          beginningText: "这篇文章介绍了",
          userIcon: "https://ai.tianli0.top/static/img/PostChat.webp",
          userMode: "magic",
          defaultChatQuestions: ["你好","你是谁"],
          defaultSearchQuestions: ["视频压缩","设计"]
        };
    </script>
    <script data-postchat_key="S-TA3IX28M1ZT7TILW" src="https://ai.tianli0.top/static/public/tianli_gpt.min.js"></script>
  <script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/chitose.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body></html>